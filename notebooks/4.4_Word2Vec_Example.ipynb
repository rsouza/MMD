{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mestrado em Modelagem Matematica da Informacao\n",
    "----------------------------------------------\n",
    "Disciplina: Modelagem e Mineracao de Dados\n",
    "------------------------------------------\n",
    "\n",
    "Master Program - Mathematical Modeling of Information\n",
    "-----------------------------------------------------\n",
    "Course: Data Mining and Modeling\n",
    "--------------------------------\n",
    "\n",
    "Professor: Renato Rocha Souza\n",
    "-----------------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import multiprocessing\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "import networkx as nx\n",
    "from random import shuffle\n",
    "\n",
    "import gensim \n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AffinityPropagation, DBSCAN, AgglomerativeClustering, MiniBatchKMeans\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logging\n",
    "importlib.reload(logging)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
    "#logging.root.setLevel(level=logging.INFO)\n",
    "#logger = logging.getLogger()\n",
    "#logger = logging.getLogger(program)\n",
    "#logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n",
    "# https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models\n",
    "# http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "path_io_files = pathlib.Path('../datasets/Word2vec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Evaluation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions = path_io_files / 'questions-words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total evaluation sentences: 19544 \n"
     ]
    }
   ],
   "source": [
    "evals = open(questions).readlines()\n",
    "num_sections = len([l for l in evals if l.startswith(':')])\n",
    "print('total evaluation sentences: {} '.format(len(evals) - num_sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def w2v_model_accuracy(model):\n",
    "    accuracy = model.accuracy(questions)\n",
    "    sum_corr = len(accuracy[-1]['correct'])\n",
    "    sum_incorr = len(accuracy[-1]['incorrect'])\n",
    "    total = sum_corr + sum_incorr\n",
    "    percent = lambda a: a / total * 100\n",
    "    print('Total sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, \n",
    "                                                                             percent(sum_corr), \n",
    "                                                                             percent(sum_incorr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    print('queen')\n",
    "    for result in model.wv.most_similar(\"queen\"):\n",
    "        print(result)\n",
    "    print()\n",
    "    print('man')\n",
    "    for result in model.wv.most_similar(\"man\"):\n",
    "        print(result)\n",
    "    print()\n",
    "    print('woman')    \n",
    "    for result in model.wv.most_similar(\"woman\"):\n",
    "        print(result)\n",
    "    print()\n",
    "    print('frog')\n",
    "    for result in model.wv.most_similar(\"frog\"):\n",
    "        print(result)\n",
    "    print()\n",
    "    print('awful')\n",
    "    for result in model.wv.most_similar(\"awful\"):\n",
    "        print(result)\n",
    "    print()\n",
    "    print(\"breakfast cereal dinner lunch:\")\n",
    "    print(model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "    print(\"captain onion starship alien:\")\n",
    "    print(model.wv.doesnt_match(\"captain onion starship alien\".split()))\n",
    "    print(\"father mother son daughter film:\")\n",
    "    print(model.wv.doesnt_match(\"father mother son daughter film\".split()))\n",
    "    print(\"france england germany berlin:\")\n",
    "    print(model.wv.doesnt_match(\"france england germany berlin\".split()))\n",
    "    print(\"woman\", \"girl\")\n",
    "    print(model.wv.similarity(\"woman\", \"girl\"))\n",
    "    print(\"woman\", \"man\")\n",
    "    print(model.wv.similarity(\"woman\", \"man\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TED Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#download the data\n",
    "\n",
    "#url = \"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\"\n",
    "#urllib.request.urlretrieve(url, filename=\"ted_en-20160408.zip\")\n",
    "\n",
    "# extract subtitles\n",
    "with zipfile.ZipFile('../datasets/Word2vec/ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "input_text = '\\n'.join(doc.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove parenthesis \n",
    "input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "\n",
    "# store as list of sentences\n",
    "sentences_strings_ted = []\n",
    "for line in input_text_noparens.split('\\n'):\n",
    "    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "    \n",
    "# store as list of lists of words\n",
    "sentences_ted = []\n",
    "for sent_str in sentences_strings_ted:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "    sentences_ted.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ted = Word2Vec(sentences=sentences_ted, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n",
      "('chief', 0.7341812252998352)\n",
      "('jones', 0.7341130971908569)\n",
      "('king', 0.7312071919441223)\n",
      "('president', 0.7141561508178711)\n",
      "('dalai', 0.7064103484153748)\n",
      "('maria', 0.6999576091766357)\n",
      "('church', 0.6935442090034485)\n",
      "('lord', 0.6926425099372864)\n",
      "('christ', 0.6920947432518005)\n",
      "('mary', 0.6916918754577637)\n",
      "\n",
      "man\n",
      "('woman', 0.8481717109680176)\n",
      "('guy', 0.8205757141113281)\n",
      "('lady', 0.7724880576133728)\n",
      "('soldier', 0.7338567972183228)\n",
      "('gentleman', 0.7279619574546814)\n",
      "('boy', 0.7160205841064453)\n",
      "('girl', 0.7013405561447144)\n",
      "('poet', 0.6984959840774536)\n",
      "('kid', 0.6874792575836182)\n",
      "('king', 0.683418869972229)\n",
      "\n",
      "woman\n",
      "('man', 0.848171591758728)\n",
      "('girl', 0.827579140663147)\n",
      "('lady', 0.7960957288742065)\n",
      "('boy', 0.7917556762695312)\n",
      "('kid', 0.7614094614982605)\n",
      "('child', 0.7317502498626709)\n",
      "('soldier', 0.7312405109405518)\n",
      "('guy', 0.7175164222717285)\n",
      "('gentleman', 0.7001538872718811)\n",
      "('person', 0.6926642656326294)\n",
      "\n",
      "frog\n",
      "('compound', 0.7530810832977295)\n",
      "('dish', 0.7402394413948059)\n",
      "('shoe', 0.7327032089233398)\n",
      "('dragon', 0.7244929075241089)\n",
      "('submersible', 0.724308967590332)\n",
      "('snake', 0.722419023513794)\n",
      "('tumor', 0.7211047410964966)\n",
      "('bacterium', 0.7209346890449524)\n",
      "('coat', 0.7175017595291138)\n",
      "('fragment', 0.7144671678543091)\n",
      "\n",
      "awful\n",
      "('outsider', 0.6663205623626709)\n",
      "('unbelievable', 0.6471660137176514)\n",
      "('impostor', 0.6437609195709229)\n",
      "('entrepreneur', 0.6247069835662842)\n",
      "('authentic', 0.6196577548980713)\n",
      "('atheist', 0.6189941167831421)\n",
      "('underutilized', 0.6157602071762085)\n",
      "('unprecedented', 0.6071993112564087)\n",
      "('unfair', 0.6030739545822144)\n",
      "('expert', 0.6029096841812134)\n",
      "\n",
      "breakfast cereal dinner lunch:\n",
      "cereal\n",
      "captain onion starship alien:\n",
      "captain\n",
      "father mother son daughter film:\n",
      "film\n",
      "france england germany berlin:\n",
      "berlin\n",
      "woman girl\n",
      "0.8275790904043095\n",
      "woman man\n",
      "0.8481716454267726\n"
     ]
    }
   ],
   "source": [
    "print_results(model_ted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Google News Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "googlenews = os.path.join(path_io_files,'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_googlenews = gensim.models.KeyedVectors.load_word2vec_format(googlenews, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model_accuracy(model_googlenews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_results(model_googlenews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Media Cloud Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#mediacloud = os.path.join(path_io_files, 'MediaCloud_w2v')\n",
    "mediacloud = os.path.join(path_io_files, 'MediaCloud_w2v_trigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_mediacloud = gensim.models.Word2Vec.load(mediacloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_mediacloud.most_similar('fgv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_neighbors(word, model, nviz=15):\n",
    "    g = nx.Graph()\n",
    "    g.add_node(word, {'color':'blue'})\n",
    "    viz1 = model.most_similar(word, topn=nviz)\n",
    "    g.add_weighted_edges_from([(word, v, w) for v,w in viz1 if w> 0.5] )\n",
    "    for v in viz1:\n",
    "        g.add_weighted_edges_from([(v[0], v2, w2) for v2,w2 in model.most_similar(v[0])])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word = 'andr√©_braz'\n",
    "g = build_neighbors(word, model_mediacloud)\n",
    "cols = ['r']*len(g.nodes()); cols[g.nodes().index(word)]='b'\n",
    "pos = nx.spring_layout(g, iterations=100)\n",
    "nx.draw_networkx(g,pos=pos, node_color=cols, node_size=1000, alpha=0.5, font_size=16)\n",
    "#nx.draw_networkx_labels(g, pos,dict(zip(g.nodes(),g.nodes())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_results(model_mediacloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wikipedia Model\n",
    "\n",
    "(You'll need at least 36GB RAM to process this file)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim\n",
    "# Download the raw xml file from wikimedia\n",
    "# https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "wikipedia = os.path.join(path_io_files,'enwiki-latest-pages-articles.xml.bz2')\n",
    "\n",
    "# Use this tool to open the wikimedia dump\n",
    "wiki = WikiCorpus(wikipedia, lemmatize=False, dictionary={})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create a new file and save the processed dump\n",
    "with open(os.path.join(path_io_files,'wikimedia_processed_w2v'), 'w') as f:\n",
    "    for text in wiki.get_texts():\n",
    "        text = [token.decode('utf8') for token in text]\n",
    "        f.write(' '.join(text) + \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Create a model based in the processed dump \n",
    "with open(os.path.join(path_io_files,'wikimedia_processed_w2v'), 'r') as f:\n",
    "    model_wikipedia = Word2Vec(LineSentence(f), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Trim unneeded model memory = use (much) less RAM\n",
    "model_wikipedia.init_sims(replace=True)\n",
    "# Save as a model\n",
    "model_wikipedia.save(os.path.join(path_io_files,'model_wikimedia_w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now you can load only the trimmed model and forget the other files\n",
    "model_wikipedia = gensim.models.Word2Vec.load(os.path.join(path_io_files,'model_wikimedia_w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_wikipedia.wv.vocab[\"tee\"].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model_accuracy(model_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_results(model_wikipedia)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
