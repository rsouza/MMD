{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google's Word2Vec for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial  \n",
    "https://github.com/wendykan/DeepLearningMovies  \n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/  \n",
    "\n",
    "\n",
    "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.\n",
    "\n",
    "Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's charge the batteries for our analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "#import seaborn as sns\n",
    "#import seaborn.apionly as sns\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "%matplotlib inline\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set  \n",
    "--\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "sampleSubmission - A comma-delimited sample submission file in the correct format.\n",
    "Data fields\n",
    "\n",
    "id - Unique ID of each review\n",
    "sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "review - Text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = \"../datasets/Kaggle/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(datapath, 'BOW_labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(os.path.join(datapath, 'BOW_testData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(os.path.join(datapath, \"BOW_unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} labeled train reviews, \\\n",
    "{} labeled test reviews, and \\\n",
    "{} unlabeled reviews\\n\".format(train[\"review\"].size,\n",
    "                               test[\"review\"].size,\n",
    "                               unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: For Beginners - Bag of Words\n",
    "--\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words  \n",
    "\n",
    "What is NLP?\n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. This page will help you get started with loading and cleaning the IMDB movie reviews, then applying a simple Bag of Words model to get surprisingly accurate predictions of whether a review is thumbs-up or thumbs-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist(review, remove_stopwords=False):\n",
    "        # Function to convert a document to a sequence of words, optionally removing stop words.  \n",
    "        # Returns a list of words.\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        #raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append(KaggleWord2VecUtility.review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all the datasets and getting word lists\n",
    "--\n",
    "first set is without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may critic think movie well watched movie cinemax first say much hate storyline mean come snowman scare besides little kids secondly pretty gory bet since movie low budget probably used ketchup critical vote bomb nice try sequel suck twice much',\n",
       " 'uk edition show rather less extravagant us version person concerned get new kitchen perhaps bedroom bathroom wonderfully grateful got us version show everything reality tv instead making improvements house occupants could afford entire house gets rebuilt know show trying show lousy welfare system exists us beg hard enough receive rather vulgar product placement takes place particularly sears also uncalled rsther turning one family deprived area potential millionaires would far better help community whole instead spending hundreds thousands dollars one home build something whole community perhaps place diy power tools borrowed returned along building materials everyone benefit want giving one person cause enormous resentment among rest local community still live run houses']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for i in range(0, len(train[\"review\"])):\n",
    "    clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
    "clean_train_reviews[99:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['young theater actress reluctantly accepts first major part staged play lady macbeth thanks mishap production diva falling front moving car success winning accolades betty christina marsillach finds instead horror one responsible getting point terrorizes cruel ways first stage manager boyfriend stefano william mcnamara viciously stabbed knife watch rope tied pillar uncle vast mansion keeping eyes closing thanks needles scotch taped eyes pricking result blinks allowing escape betty finds unfortunate position specific gold chain found lady macbeth wardrobe torn shreds madman killing series crows escaped cage equipment room clothing designer first knocked floor iron subsequently stabbed heinously pair scissors cap nasty scene chain falls throat killer cut open throat find specifically elaborating act dario uses ripping sound scissors optimum effect camera often retreating back victim dead face nowhere turn inspector alan santini urbano barberini playing cold bland dario request promises catch psycho betty relies remaining friends comfort theater director marco ian charleson ghandi chariots fire would later die aids sad footnote dario revealed interview charleson informed end shoot hiv positive attempting stage successful horror movie career despite rejected critics pal mira daria nicolodi dario former squeeze agent confident serial killer quite driven showdown course occur theater marco added interesting change production using crows disposal think dario savage nihilistic although certainly made later gruesome films masters horror entries sleepless would suggest film really ups ante pure violence towards victims psychopath method forcing betty watch admittedly gag dario regarding type audience like look away horrific parts horror movies found rubbing eyes every time blink brilliantly dario shoots pricking point view achieving tormenting effect perspective ones scotch taped needles holding eyes open like dario show widened eyes betty horrified forced watching little blood tears needles choice blink photographic work ronnie taylor impeccable crow point view shot theater end searches killer camera travels rooms theater following killer wishes see muse box seat dream sequence taken betty memory event regarding mother death certain killer donning mask gloves one causing trauma present highlight opinion peephole bullet fire sequence masterly staged involved camera follows bullet shoots eye victim exploding back head going telephone betty planned use call help even get crow pecking eyeball killer add vicious scene crow shown eyeball rolling around beak aspect care often pointed naysayers film final ending personally felt bit unnecessary guess dario wanted point betty indeed like mother woman sadomasochistic tendencies twist relate killer torments heroine rock music used violent scenes bother felt moments wicked graphic attacks needed jarring thud heavy metal often provide',\n",
       " 'one true thing quiet film opened fall glowing reviews mild box office tells crippled story ellen renee zellweger workaholic forced move back home take care terminally ill mother meryl streep aloof father william hurt run academic department terms general strength one true thing lies way actors elevate characters hollywood clich territory streep kate perfect homemaker whose ability light room charm evident opening scenes costume party celebrating hurt birthday ellen never close mother since graduated harvard university certain destain ellen almost thinks mother simplistic air head hand admires father shares special passion writing ellen writes aggressive new york firm almost heartbroken latest piece torn hurt seems lonely figure get point kate gets sicker ellen perspectives change grows closer mother distant father hurt keeps making excuses family needs ellen assumes affair meanwhile given desk work spend time craft activities mother cult group minnies also learning mother weak first assumed without giving much away one true thing masterpiece character study streep turns beautiful performance time working subtle level starts slow ends brilliant speech vows marriage streep earned eleventh oscar nomination performance hurt also convincing father carries secret revealed closing moments renee zellweger steals movie forget chicago cold mountain bridget jones diary whatever else seen rent movie remarkable working within character bitter resentment understanding parents zellweger manages realistic portrayal young woman fighting keep lip screaming inside']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for i in range(0,len(test[\"review\"])):\n",
    "    clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
    "clean_test_reviews[99:101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Pickles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'wb') as f:\n",
    "    pickle.dump((clean_train_reviews, clean_test_reviews),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    clean_train_reviews, clean_test_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second set mantains stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i may not be a critic but here is what i think of this movie well just watched the movie on cinemax and first of all i just have to say how much i hate the storyline i mean come on what does a snowman scare besides little kids secondly it is pretty gory but i bet since the movie is so low budget they probably used ketchup so my critical vote is bomb nice try and the sequel will suck twice as much',\n",
       " 'there is a uk edition to this show which is rather less extravagant than the us version the person concerned will get a new kitchen or perhaps bedroom and bathroom and is wonderfully grateful for what they have got the us version of this show is everything that reality tv shouldn t be instead of making a few improvements to a house which the occupants could not afford or do themselves the entire house gets rebuilt i do not know if this show is trying to show what a lousy welfare system exists in the us or if you beg hard enough you will receive the rather vulgar product placement that takes place particularly by sears is also uncalled for rsther than turning one family in a deprived area into potential millionaires it would be far better to help the community as a whole where instead of spending the hundreds of thousands of dollars on one home build something for the whole community perhaps a place where diy and power tools can be borrowed and returned along with building materials so that everyone can benefit should they want to giving it all to one person can cause enormous resentment among the rest of the local community who still live in the same run down houses']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews_sw = []\n",
    "for i in range(0, len(train[\"review\"])):\n",
    "    clean_train_reviews_sw.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], False)))\n",
    "    \n",
    "clean_train_reviews_sw[99:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a young theater actress reluctantly accepts her first major part in a staged play as lady macbeth thanks to the mishap of the production s diva falling in front of a moving car a success winning her accolades betty christina marsillach finds instead horror as the one responsible for getting her to this point terrorizes her in cruel ways first her stage manager boyfriend stefano william mcnamara is viciously stabbed with a knife while she has to watch rope tied to a pillar in his uncle s vast mansion keeping her eyes from closing thanks to needles scotch taped under her eyes with pricking a result if she blinks allowing her to escape betty again finds herself in this unfortunate position when a specific gold chain is found on her lady macbeth wardrobe that had been torn to shreds by the madman after killing a series of crows which escaped from their cage in the equipment room by the clothing designer who is first knocked to the floor by an iron and subsequently stabbed heinously by a pair of scissors to cap off this nasty scene the chain falls into her throat with the killer having to cut open her throat to find it while not specifically elaborating this act dario uses the ripping sound of the scissors for optimum effect with the camera often retreating back to the victim s dead face having nowhere to turn inspector alan santini urbano barberini playing him cold and bland at dario s request promises to catch the psycho as betty relies on her few remaining friends for comfort the theater director marco ian charleson of ghandi chariots of fire who would later die of aids a sad footnote dario revealed in an interview that charleson informed him at the end of the shoot that he was hiv positive attempting the stage after a successful horror movie career despite being rejected by critics and pal mira daria nicolodi dario s former squeeze her agent and confident but the serial killer is quite driven and a showdown between them will of course occur in the theater as marco has added an interesting change to the production using the crows at his disposal i think this is dario at his most savage nihilistic although he has certainly made later gruesome films such as his masters of horror entries and sleepless would suggest this film really ups the ante in pure violence towards the victims of the psychopath his method of forcing betty to watch was admittedly a gag by dario regarding the type of audience who like to look away from the more horrific parts of horror movies i found myself rubbing my eyes every time she just has to blink brilliantly dario shoots the pricking from point of view achieving a tormenting effect from our perspective as if we were the ones with the scotch taped needles holding our eyes open i like how dario will show the widened eyes of betty horrified at what she s being forced into watching as little blood tears down the needles when she has no choice but blink the photographic work of ronnie taylor is impeccable such as the crow s point of view shot in the theater at the end as it searches for the killer or when the camera travels through rooms in the theater following the killer who wishes to see his muse from a box seat or the dream sequence where we are taken into betty s memory of an event regarding her mother s death by a certain killer donning the same mask and gloves as the one causing her trauma at present the highlight in my opinion is the peephole bullet fire sequence masterly staged by all involved as the camera follows a bullet which shoots through the eye of a victim exploding from the back of her head going through a telephone betty planned to use to call for help we even get a crow pecking the eyeball from the killer to add to this vicious scene the crow is shown with the eyeball rolling around in it s beak only aspect i didn t care for often pointed out by naysayers of the film is the final ending which i personally felt was a bit unnecessary but i guess dario wanted to point out that betty was indeed not like her mother a woman with sadomasochistic tendencies which in a twist relate to why the killer torments our heroine the rock music used during the violent scenes didn t bother me because i felt that those moments of wicked graphic attacks needed a jarring thud which heavy metal can often provide',\n",
       " 'one true thing is a very quiet film that opened in the fall of to glowing reviews but mild box office it tells the crippled story of ellen renee zellweger a workaholic who is forced to move back home to take care of her terminally ill mother meryl streep so that her aloof father william hurt can run his academic department these terms are only general the strength of one true thing lies in the way the actors elevate their characters above hollywood clich territory streep is kate the perfect homemaker whose ability to light up a room with her charm is evident in her opening scenes at a costume party celebrating hurt s birthday but ellen has never been close to her mother and since she graduated from harvard university has a certain destain about her ellen almost thinks her mother is a simplistic air head while on the other hand she admires her father who shares a special passion writing ellen writes for an aggressive new york firm and is almost heartbroken when her latest piece is torn down by hurt who seems to be a very lonely figure to get to the point as kate gets sicker ellen s perspectives change and she grows closer to her mother and more distant to her father hurt keeps making excuses not to be there when the family needs him most and ellen assumes he s having an affair meanwhile she s given up her desk at work to spend time doing craft activities with her mother s cult group the minnies and also learning that her mother isn t as weak as she first assumed without giving too much away one true thing is a masterpiece in character study streep once again turns in a beautiful performance this time working on a subtle level that starts slow but ends with a brilliant speech on the vows of marriage streep earned her eleventh oscar nomination for this performance hurt is also convincing as the father who carries a secret that isn t revealed until the closing moments but it is renee zellweger who steals this movie forget chicago cold mountain bridget jones s diary or whatever else you ve seen her do and rent this movie she is remarkable in it working within her character s bitter resentment at understanding her parents zellweger manages a realistic portrayal of a young woman fighting to keep her lip up while she s screaming inside']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews_sw = []\n",
    "for i in range(0,len(test[\"review\"])):\n",
    "    clean_test_reviews_sw.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], False)))\n",
    "    \n",
    "clean_test_reviews_sw[99:101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews_sw.pkl'),'wb') as f:\n",
    "    pickle.dump((clean_train_reviews_sw, \n",
    "                 clean_test_reviews_sw),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews_sw.pkl'),'rb') as f:\n",
    "    clean_train_reviews_sw, clean_test_reviews_sw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Features from a Bag of Words (Using scikit-learn)\n",
    "--\n",
    "\n",
    "Now that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "Sentence 1: \"The cat sat on the hat\"  \n",
    "Sentence 2: \"The dog ate the cat and the hat\"  \n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Similarly, the features for Sentence 2 are: { 3, 1, 0, 0, 1, 1, 1, 1}\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
    "\n",
    "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test two strategies: CountVectorizer and Tfidf Vectorizer:  \n",
    "First we'll start with plain word counts:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer1 = CountVectorizer(input='content', \n",
    "                              encoding='utf-8', \n",
    "                              decode_error='strict', \n",
    "                              strip_accents=None, \n",
    "                              lowercase=True, \n",
    "                              preprocessor=None, \n",
    "                              tokenizer=None, \n",
    "                              stop_words=None, \n",
    "                              #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                              ngram_range=(1, 1),\n",
    "                              analyzer='word', \n",
    "                              max_df=1.0, \n",
    "                              min_df=1, \n",
    "                              max_features=5000, \n",
    "                              vocabulary=None, \n",
    "                              binary=False, \n",
    "                              dtype=np.int64,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "second, it transforms our training data into feature vectors. \n",
    "The input to fit_transform should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features1 = vectorizer1.fit_transform(clean_train_reviews)\n",
    "train_data_features1 = train_data_features1.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features1 = vectorizer1.fit_transform(clean_test_reviews)\n",
    "test_data_features1 = test_data_features1.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_data_features1.pkl'),'wb') as f:\n",
    "    pickle.dump(train_data_features1,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'test_data_features1.pkl'),'wb') as f:\n",
    "    pickle.dump(test_data_features1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_data_features1.pkl'),'rb') as f:\n",
    "    train_data_features1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'test_data_features1.pkl'),'rb') as f:\n",
    "    test_data_features1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use TfIDf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer2 = TfidfVectorizer(input='content',\n",
    "                              #encoding='utf-8',\n",
    "                              decode_error='strict',\n",
    "                              strip_accents=None,\n",
    "                              lowercase=True,\n",
    "                              preprocessor=None,\n",
    "                              tokenizer=None,\n",
    "                              analyzer='word',\n",
    "                              stop_words=None,\n",
    "                              #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                              ngram_range=(1, 3),\n",
    "                              max_df=1.0,\n",
    "                              min_df=1,\n",
    "                              max_features=6000,\n",
    "                              vocabulary=None, \n",
    "                              binary=False, \n",
    "                              dtype=np.int64,\n",
    "                              norm='l2',\n",
    "                              use_idf=True,\n",
    "                              smooth_idf=True,\n",
    "                              sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 6000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features2 = vectorizer2.fit_transform(clean_train_reviews_sw)\n",
    "train_data_features2 = train_data_features2.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 6000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features2 = vectorizer2.fit_transform(clean_test_reviews_sw)\n",
    "test_data_features2 = test_data_features2.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_data_features2.pkl'),'wb') as f:\n",
    "    pickle.dump(train_data_features2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'test_data_features2.pkl'),'wb') as f:\n",
    "    pickle.dump(test_data_features2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_data_features2.pkl'),'rb') as f:\n",
    "    train_data_features2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'test_data_features2.pkl'),'rb') as f:\n",
    "    test_data_features2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing Train set for Cross Validation  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://github.com/zygmuntz/classifying-text/blob/master/bow_validate.py  \n",
    "Alternatively, we can use the indexes to divide the train samples  \n",
    "\n",
    "train_i, test_i = train_test_split(np.arange(len(train)), train_size = 0.8, random_state = 44)  \n",
    "\n",
    "After generating indexes, we can divide ou datasets:  \n",
    "traincv = train_data_features1[train_i]  \n",
    "testcv = train_data_features1[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plain Word Counts\n",
    "X_traincv1, X_testcv1, y_traincv1, y_testcv1 = train_test_split(train_data_features1,\n",
    "                                                                train[\"sentiment\"],\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "X_traincv2, X_testcv2, y_traincv2, y_testcv2 = train_test_split(train_data_features2,\n",
    "                                                                train[\"sentiment\"],\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function to evaluate the classifiers:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_scores_cv(clf, cv, X, y):\n",
    "    scores = cross_validation.cross_val_score(clf, X, y, \n",
    "                                              scoring=None, \n",
    "                                              cv=cv, \n",
    "                                              n_jobs=-1,\n",
    "                                              verbose=0,\n",
    "                                              fit_params=None,\n",
    "                                              pre_dispatch='2*n_jobs')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training some Classifiers  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "--\n",
    "\n",
    "At this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning! Here, we'll use the Random Forest classifier.  The Random Forest algorithm is included in scikit-learn (Random Forest uses many tree-based classifiers to make predictions, hence the \"forest\"). Below, we set the number of trees to 100 as a reasonable default value. More trees may (or may not) perform better, but will certainly take longer to run. Likewise, the more features you include for each review, the longer this will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8586\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_1 = RandomForestClassifier(n_estimators=300, \n",
    "                                  criterion='gini', \n",
    "                                  max_depth=None, \n",
    "                                  min_samples_split=2, \n",
    "                                  min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, \n",
    "                                  max_features='auto', \n",
    "                                  max_leaf_nodes=None, \n",
    "                                  bootstrap=False, \n",
    "                                  oob_score=False, \n",
    "                                  n_jobs=-1, \n",
    "                                  random_state=0, \n",
    "                                  verbose=0, \n",
    "                                  warm_start=False, \n",
    "                                  class_weight=None).fit(X_traincv1, y_traincv1)\n",
    "\n",
    "eval_RF_1_tts = clf_RF_1.score(X_testcv1, y_testcv1)\n",
    "print(eval_RF_1_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44666667,  0.55333333],\n",
       "       [ 0.38666667,  0.61333333],\n",
       "       [ 0.35666667,  0.64333333],\n",
       "       ..., \n",
       "       [ 0.37666667,  0.62333333],\n",
       "       [ 0.80666667,  0.19333333],\n",
       "       [ 0.17666667,  0.82333333]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_1.predict_proba(X_testcv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train on the TfIdf samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8532\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_2 = RandomForestClassifier(n_estimators=300, \n",
    "                                  criterion='gini', \n",
    "                                  max_depth=None, \n",
    "                                  min_samples_split=2, \n",
    "                                  min_samples_leaf=1, \n",
    "                                  min_weight_fraction_leaf=0.0, \n",
    "                                  max_features='auto', \n",
    "                                  max_leaf_nodes=None, \n",
    "                                  bootstrap=False, \n",
    "                                  oob_score=False, \n",
    "                                  n_jobs=-1, \n",
    "                                  random_state=0, \n",
    "                                  verbose=0, \n",
    "                                  warm_start=False, \n",
    "                                  class_weight=None).fit(X_traincv2, y_traincv2)\n",
    "\n",
    "eval_RF_2_tts = clf_RF_2.score(X_testcv2, y_testcv2)\n",
    "print(eval_RF_2_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46666667,  0.53333333],\n",
       "       [ 0.29666667,  0.70333333],\n",
       "       [ 0.22333333,  0.77666667],\n",
       "       ..., \n",
       "       [ 0.25333333,  0.74666667],\n",
       "       [ 0.75666667,  0.24333333],\n",
       "       [ 0.28333333,  0.71666667]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_2.predict_proba(X_testcv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8586\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_1 = LR(penalty='l2',\n",
    "              dual=False,\n",
    "              tol=0.0001,\n",
    "              C=1.0,\n",
    "              fit_intercept=True,\n",
    "              intercept_scaling=1,\n",
    "              class_weight=None,\n",
    "              random_state=None,\n",
    "              solver='liblinear',\n",
    "              max_iter=100,\n",
    "              multi_class='ovr',\n",
    "              verbose=0).fit(X_traincv1, y_traincv1)\n",
    "\n",
    "eval_LR_1_tts = clf_LR_1.score(X_testcv1, y_testcv1)\n",
    "print(eval_LR_1_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8926\n"
     ]
    }
   ],
   "source": [
    "clf_LR_2 = LR(penalty='l2',\n",
    "              dual=False,\n",
    "              tol=0.0001,\n",
    "              C=1.0,\n",
    "              fit_intercept=True,\n",
    "              intercept_scaling=1,\n",
    "              class_weight=None,\n",
    "              random_state=None,\n",
    "              solver='liblinear',\n",
    "              max_iter=100,\n",
    "              multi_class='ovr',\n",
    "              verbose=0).fit(X_traincv2, y_traincv2)\n",
    "\n",
    "eval_LR_2_tts = clf_LR_2.score(X_testcv2, y_testcv2)\n",
    "print(eval_LR_2_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost Classifier  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8066\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GBC_1 = GradientBoostingClassifier(loss='deviance',\n",
    "                                       learning_rate=0.1,\n",
    "                                       n_estimators=100,\n",
    "                                       subsample=1.0,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf=1,\n",
    "                                       min_weight_fraction_leaf=0.0,\n",
    "                                       max_depth=3,\n",
    "                                       init=None,\n",
    "                                       random_state=0,\n",
    "                                       max_features=None,\n",
    "                                       verbose=0,\n",
    "                                       max_leaf_nodes=None,\n",
    "                                       warm_start=False,\n",
    "                                       presort='auto').fit(X_traincv1, y_traincv1)\n",
    "\n",
    "eval_GBC_1_tts = clf_GBC_1.score(X_testcv1, y_testcv1)\n",
    "print(eval_GBC_1_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8142\n"
     ]
    }
   ],
   "source": [
    "clf_GBC_2 = GradientBoostingClassifier(loss='deviance',\n",
    "                                       learning_rate=0.1,\n",
    "                                       n_estimators=100,\n",
    "                                       subsample=1.0,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf=1,\n",
    "                                       min_weight_fraction_leaf=0.0,\n",
    "                                       max_depth=3,\n",
    "                                       init=None,\n",
    "                                       random_state=0,\n",
    "                                       max_features=None,\n",
    "                                       verbose=0,\n",
    "                                       max_leaf_nodes=None,\n",
    "                                       warm_start=False,\n",
    "                                       presort='auto').fit(X_traincv2, y_traincv2)\n",
    "\n",
    "eval_GBC_2_tts = clf_GBC_2.score(X_testcv2, y_testcv2)\n",
    "print(eval_GBC_2_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do some voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8598\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf_vot_1 = VotingClassifier(estimators=[('rf', clf_RF_1),\n",
    "                                         ('lr', clf_LR_1),\n",
    "                                         ('gbc', clf_GBC_1)], voting='hard').fit(X_traincv1, y_traincv1)\n",
    "\n",
    "eval_vot_1_tts = clf_vot_1.score(X_testcv1, y_testcv1)\n",
    "print(eval_vot_1_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8662\n"
     ]
    }
   ],
   "source": [
    "clf_vot_2 = VotingClassifier(estimators=[('rf', clf_RF_2),\n",
    "                                         ('lr', clf_LR_2),\n",
    "                                         ('gbc', clf_GBC_2)], voting='hard').fit(X_traincv2, y_traincv2)\n",
    "\n",
    "eval_vot_2_tts = clf_vot_2.score(X_testcv2, y_testcv2)\n",
    "print(eval_vot_2_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAIuCAYAAABw5vSxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X2cVXW9//3XZwQ1Kw4qFcKMgzcgiCiOjtGNlwje0ohe\nlYhmdYAL6WGUp5MpdXka6Ghqmp5feOzGH5qpQXpZ4S0qGNqNhoLmpY4iR4SZ0UpURFNu5/v7Y2+m\nmXHQ0bU3s2f7ej4e++Fea33X2t/1GR3f893ftVaklJAkSZL03lV0dwckSZKkns5QLUmSJGVkqJYk\nSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJZSci6iPiuiIe//GI+L/aLF8TES9HxIMR8emIaCjW\nZ/dEEXF+RLwYEc93d18kqVgM1ZJ6pIg4LSIeiojXIqI5Im6PiE+2aVK0m/CnlA5IKd2f78engbHA\ngJTSqJTSH1JKw7J+RkSsjIgxWY+zjWPXR8R3i3HsTj6rEvh3YGhKacD2+ExJ6g6Gakk9TkT8O3AZ\ncD7wUWBP4ErgxG7oziDguZTS+m747E5FxA7d3Qdo7ccgYE1K6aX3uL8k9QiGakk9SkT0AWYBZ6aU\n5qeU3kwpbUkp3Z5SOncb+9wYES9ExCsRsTgi9m+zbVxEPBER6yKiMR/YiYjdI+LW/D4vRcR9bfZZ\nGRFjImIycBXwifz+9RFxREQ0tmlbGRE3R8Tf81MgfpRfv3dELIqINflt1+fPjYj4Bbk/FG7NH/fs\n/Prx+aknL0fEvRExtEOfzomIvwCvR0RFRJwbEU35YzRExJFdqO+XI+IPEfGjiFgbEU+2HTGPiD4R\n8b8j4vl8vf4zIqLDvpdFxEvA74C7gYH5Plz9Hs5jh/y6syPiL/lvJq6KiI9GxB35494dEf/SxZ/3\nNRFxRUTclt/3gYjYq8324fnjvZQ/xoz8+oiIGRGxIv9znBcRfd+pnpLePwzVknqaTwA7Ab99F/vc\nAexDblR7GXBDm23/G5iaUuoDHADcm1//TaAR2D2/33c6HjSldDXwFeCBlFKflNKsrZsAIqICuA1Y\nSS4kDwTm5dsE8H2gPzAMqARm5o/7JWA1UJc/7qURMQT4JfB14CPAneRCd682XZoIHA/0BfYFvgoc\nkj+3Y4Hn8seflVL63tvU6+PAivy5zwR+3SZA/gLYCOwNHAwcDfw/nez7kfy244Hm/HlMfrfnkVLa\nkl/3WXLTbIYA48n9TGfk+7hD/nhbvd3Pe+vx6/N1+h/gAoCI+BBwT37/PfI1XJTf56z85x4ODABe\nIfftiCQBhmpJPc/u5KYTtHR1h5TSz1NKb6SUNgHfAw6KiA/nN28EhkfEh1NKr6aUHs2v30QuWO2V\nHwn/43vo68fzxzgnpbQ+pbQxpfSnfJ/+J6W0KKW0OT814nLgiA77R5v3E4DbUkr35oPmpcAHgLbz\nyP9XSun5lNIGYAuwI3BARPRKKa1OKa3sYr//llL6Uf68bwSeBj4TER8FjgO+kT+fNcB/Aae22bc5\npXRlSqkl34+O3u15bDU7pbQmpfQC8Hvgzymlx/I/09+QC/jAO/68AX6dUlqa/3foBmBkfv0JwAsp\npf/K/6z+kVJ6KL/tDOD/TSm90Oa4n8//4SRJhmpJPc5LQL+uhpn8NIiL8l/bryU3apyAfvkmnwM+\nA6yKiN9FxKj8+h+QG8W8O79vp1NL3kElsKqzPwAi4iMRMTc/PWMtcH2bPnVmALBq60JKKZEbSR/Y\npk1Tm+3/A/wbuZHmv0XELyNijy72u7nD8qr851cDvYEX8lM3XgF+0qHfjby9d3Uebfytzfs3O1n+\nEHTp5w3w1zbv39i6L7mf1/9so9/VwG/y5/0y8CS5P7w+to32kt5nDNWSepoHgPXASV1s/wVyI5Bj\nUkp9yV04F/kX+RHLk8hNRZgP3Jhf/4+U0tkppX3y+/97V+Ykd9AI7LmNPwAuBFqAA/L9Op32I9Md\n717yPLlg11YV7QNou31SSvNSSoe32e+iLvZ7YIflPfOf30iu9runlHZLKe2aUuqbUjrwbfrd0bs+\nj3fpbX/e76CR3JSPzqwGjs+f99Zz/2B+5FySDNWSepaU0jpy82H/OyJOjIgPRESviDg+IjoLjR8C\nNgCvRMQHyYXZrXOee0fu1nx98lMRXgM257d9JiL2yR/j9fz6ze+yu0uAF4CLImKXiNgp/nnbvw/n\nj7suIgYC3+qw71/JzVve6kZyUzCOzJ/v2eQC7gOdfXBEDMm33ZHcFJc3yU0J6YqPRsTX8p9zMjAU\nuCOl9FdyFx5eHhEfzl+8t3e0uWd3F7yr83gPtvnz7oLbgI9FxNcjYseI+FBEHJbf9lPg+xGxJ7R+\n0zC+QH2WVAYM1ZJ6nJTS5eTufXwe8Hdyo4hn0vnFi7/Ib28GHgf+1GH7F4GV+akCZ5Ab6QQYDCyM\niNeAPwL/nVL6/dYudLGfLeRGTQfn+9BIbk4x5O5gcgiwFrgVuLnD7hcB/5GfbvDvKaXl5EazrwBe\nJDdl5YSU0tag37FPO+WP8SK50eGP0MnFltvw53yf1wD/CXwupfRKftuXyM3VfhJ4GbiJ3MWWXfIe\nzqOzdW9X/3f6eb9d314nd3HleHJ/1CwHRuc3/y9y32TcHRGv5o97WCeHkfQ+FbnpbEX8gIjjyF3I\nUgHMSSld3GH7nsDV5H7hvwScnlJ6PiIOAn5MbjRnC/D9/AUzkqQiiYgvA1NSSu9m9FmS3veKOlKd\nn0d4BblbOQ0HTo029yPNuxT4eUrpIHJXU2/9+vYN4IsppRHkbq30X5G/h6skSZJUSoo9/eMw4JmU\n0qr8LYjm8dYnnu1P/r6wKaXFW7enlJ7JX71O/kKQv5MbzZYkSZJKSrFD9UDa316pibdeVf4ouVta\nERGfBT4UEbu2bZC/UKT31pAtSSqOlNK1Tv2QpHev2KG6s1sYdZzE/S1gdEQsJfekqmbaXGGfv6/q\nL4B/LVIfJUmSpEx6vXOTTJrI3d90q0pyV6G3yk/t2DpS/UFyV5m/ll/+MLlbHH2nzVOt2omI4l5p\nKUmSJOWllDq9732xR6ofAvaNiOr8vVInAre0bRARu0fE1s59m9ydQIiI3uRuj3VtSunXb/chKaVM\nr/r6+szH8GU9rWfpv6yl9Szll/W0lqX6sp7/fL2doobqlHuYwnRyDwt4ApiXUmqIiFkRUZdvNhp4\nOiKeAj4KXJBfPwH4NPCvEfFIRCyLiAORJEmSSkyxp3+QUloA7NdhXX2b9zfz1ocekFK6Abih2P2T\nJEmSsvKJisDo0aO7uwtlxXoWlvUsHGtZWNazsKxn4VjLwrKeXVP0JyoWW0Sknn4OkiRJKn0RQeqm\nCxUlSZKksmeoliRJkjIyVEuSJEkZGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQ\nLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1J\nkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnKyFAtSZIk\nZWSoliRJkjIyVEuSJEkZGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVk\nqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlZKiW\nJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnKyFAtSZIkZWSoliRJ\nkjIyVEuSJEkZGaolSZKkjAzVkiRJUkZFD9URcVxEPBURyyPi3E627xkRCyPiLxFxb0QMaLPty/n9\nno6ILxW7r5IkSdJ7ESml4h08ogJYDowFngceAiamlJ5q0+ZG4JaU0vURMRqYnFL6UkTsCjwM1AAB\nLAVqUkqvdviMVMxzkCRJkgAigpRSdLat2CPVhwHPpJRWpZQ2AfOAEzu02R+4FyCltLjN9mOBu1NK\nr6aU1gJ3A8cVub+SJEnSu1bsUD0QaGyz3JRf19ajwOcAIuKzwIfyo9Qd923uZF9JkiSp2xU7VHc2\nPN5xrsa3gNERsRQ4nFx43tzFfSVJkvQ2FixYwNChQxkyZAgXX3zxW7Y3NjYyZswYampqGDlyJHfe\neScAmzZtYvLkyRx44IEcfPDB3HfffQC8+eab1NXVMWzYMEaMGMF3vvOddse78cYbGT58OCNGjOD0\n008v/gmWiF5FPn4TsGeb5Upyc6tbpZRe4J8j1R8EPpdSei0imoDRHfb9XWcfMnPmzNb3o0ePZvTo\n0Z01kyRJel9paWlh+vTpLFq0iAEDBlBbW8uJJ57I0KFDW9ucf/75nHLKKUybNo2GhgbGjRvHypUr\nueqqq4gIHnvsMV588UWOP/54Hn74YQC+9a1vccQRR7B582bGjBnDXXfdxbHHHsuKFSu4+OKLeeCB\nB+jTpw9r1qzprlMviMWLF7N48eIutS12qH4I2DciqoEXgInAqW0bRMTuwMv5qw2/DVyd33QXcEFE\n/Au5EfWjgRmdfUjbUC1JkqScJUuWMHjwYKqrqwGYOHEi8+fPbxeqKyoqWLduHQBr165l4MDcbNsn\nn3ySsWPHAvCRj3yEvn378vDDD3PooYdyxBFHANCrVy9qampoamoC4KqrruKrX/0qffr0AaBfv37b\n50SLpONg7axZs7bZtqjTP1JKW4Dp5C4yfAKYl1JqiIhZEVGXbzYaeDoingI+ClyQ3/cV4D/J3QHk\nz8Cs/AWLkiRJ6oLm5maqqqpalysrK2lubm7Xpr6+nuuuu46qqirq6uqYPXs2AAcddBDz589ny5Yt\nrFy5kqVLl9LY2Nhu37Vr13Lrrbdy1FFHAbB8+XKefvppPv3pT/PJT36Su+66q8hnWDqKPVJNSmkB\nsF+HdfVt3t8M3LyNfX8O/LyI3ZMkSSpbnd12OKL9ZWtz585l0qRJfOMb3+DBBx/k9NNP54knnmDy\n5Mk0NDRQW1tLdXU1n/rUp+jV65/RccuWLZx22mn827/9W+tI+ObNm1mxYgX3338/q1ev5vDDD+eJ\nJ55oHbkuZ0UP1ZIkSeoelZWVrF69unW5qamJAQMGtGszZ86c1hHlUaNGsX79etasWUO/fv247LLL\nWtt96lOfYvDgwa3LZ5xxBvvttx9f+9rX2n3eJz7xCSoqKhg0aBD77bcfzzzzDIccckixTrFk+Jhy\nSZKkMlVbW8uKFStYtWoVGzduZN68eYwfP75dm+rqahYuXAhAQ0MDGzZsoF+/frz55pu88cYbANxz\nzz307t27dS72eeedx7p167j88svbHeukk07i3nvvBWDNmjU888wz7L333sU+zZJQ1Ccqbg8+UVGS\nJGnbFixYwFlnnUVLSwtTpkxhxowZ1NfXU1tbS11dHQ0NDUydOpXXX3+diooKLrnkEsaOHcuqVas4\n9thj2WGHHRg4cCBz5syhqqqqdZ72sGHD2HHHHYkIpk+fzuTJkwH45je/yYIFC+jVqxfnnXceJ598\ncjdXoHDe7omKhmpJkiSpC7rzMeWSJElS2TNUS5IkSRkZqiVJkqSMDNWSJElSRoZqSZIkKSNDtSRJ\nkpSRoVqSJEnKyFAtSZIkZWSoliRJkjIyVEuSJEkZGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKU\nkaFakiRJyshQLUmSVKb69x9ERHT7q3//Qd1diqKLlFJ39yGTiEg9/RwkSZKKISKAUshJQTnktYgg\npRSdbXOkWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRoZqSZIkKSNDtSRJ\nkpSRoVqSJEnKyFAtSZIkZWSoliRJkjIyVEuSJEkZGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKU\nkaFakiRJyshQLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGh\nWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqS\nJEnKyFAtSZIkZVT0UB0Rx0XEUxGxPCLO7WR7VUTcGxHLIuLRiDg+v75XRPw8Ih6LiCciYkax+ypJ\nkiS9F0UN1RFRAVwBHAsMB06NiKEdmp0H/CqlVAOcClyZX38ysGNK6UDgUGBaROxZzP5KkiRJ70Wx\nR6oPA55JKa1KKW0C5gEndmjTAvTJv+8LNOffJ+CDEbEDsAuwAVhX5P5KkiRJ71qxQ/VAoLHNclN+\nXVuzgC9GRCNwG/C1/Pr/D3gDeAF4Drg0pbS2qL2VJEmS3oNih+roZF3qsHwqcE1KqQr4DHB9fv3H\ngc1Af2Bv4OyIGFScbkqSJEnvXa8iH78JaDsPuhJ4vkObKeTmXJNSejAidoqIfuTC9oKUUgvwYkT8\nkdzc6uc6fsjMmTNb348ePZrRo0cX7gwkSZL0vrR48WIWL17cpbaRUseB48LJz4d+GhhLbhrHEuDU\nlFJDmza3AzemlK6NiGHAPSmlyog4B9gvpTQlIj6Y3/eUlNLjHT4jFfMcJEmSeqqI4K2TBLpDUA55\nLSJIKXU2E6O40z9SSluA6cDdwBPAvJRSQ0TMioi6fLOzgakR8ShwA/Dl/Pr/Bj4cEY8DfwbmdAzU\nkiRJUiko6kj19uBItSRJUuccqS6sbhupliRJkt4PDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnK\nyFAtSZIkZWSoliRJkjIyVEuSJEkZGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQ\nLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUlSyVmwYAFDhw5lyJAhXHzxxW/Z3tjYyJgxY6ipqWHkyJHc\neeedrdsee+wxPvnJT3LAAQdw0EEHsXHjRgA2bdrEtGnT2G+//dh///35zW9+A8Dll1/O8OHDGTly\nJEcffTSNjY3b5yRVViKl1N19yCQiUk8/B0mS9E8tLS0MGTKERYsWMWDAAGpra5k3bx5Dhw5tbTNt\n2jRqamqYNm0aDQ0NjBs3jpUrV7JlyxZqamq44YYbOOCAA3jllVfo27cvEcHMmTNpaWnhe9/7HgAv\nv/wyu+22G/fddx8f//jH2XnnnfnJT37C4sWLmTdvXnedfkFFBFAKOSkoh7wWEaSUorNtjlRLkqSS\nsmTJEgYPHkx1dTW9e/dm4sSJzJ8/v12biooK1q1bB8DatWsZOHAgAHfffTcHHXQQBxxwAAC77rpr\nPljC1Vdfzbe//e3WY+y2224AHHHEEey8884AjBo1iubm5uKeoMqSoVqSJJWU5uZmqqqqWpcrKyvf\nEnTr6+u57rrrqKqqoq6ujtmzZwOwfPlyAI477jgOPfRQLrnkEgBeffVVAM477zwOOeQQTjnlFF58\n8cW3fPacOXM4/vjji3JeKm+GakmSVFI6myawdbR5q7lz5zJp0iQaGxu5/fbbOf300wHYvHkzf/zj\nH5k7dy6///3v+c1vfsPvfvc7Nm/eTFNTE4cffjhLly5l1KhRfPOb32x3zOuvv56lS5fyrW99q3gn\np7JlqJYkSSWlsrKS1atXty43NTUxYMCAdm3mzJnDhAkTgNyUjfXr17NmzRoqKys54ogj2HXXXfnA\nBz7AuHHjWLZsGbvvvjsf/OAHOemkkwA4+eSTeeSRR1qPt3DhQi688EJuvfVWevfuvR3OUuXGUC1J\nkkpKbW0tK1asYNWqVWzcuJF58+Yxfvz4dm2qq6tZuHAhAA0NDWzYsIF+/fpx7LHH8thjj7F+/Xo2\nb97Mfffdx/777w/ACSecwO9+9zsgF6K3rn/kkUf4yle+wi233MLuu+++Hc9U5cS7f0iSpJKzYMEC\nzjrrLFpaWpgyZQozZsygvr6e2tpa6urqaGhoYOrUqbz++utUVFRwySWXMHbsWAB++ctf8v3vf5+K\nigo+85nPcOGFFwKwevVqvvjFL/Lqq6/ykY98hGuuuYbKykqOPvpoHn/8cfbYYw9SSlRXV/Pb3/62\nO0+/YLz7R2G93d0/DNWSJEllylBdWN5ST5IkSSoiQ7UkSZKUkaFakiRJyshQLUmSJGVkqJYkSZIy\nMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JUgEsWLCAoUOHMmTI\nEC6++OK3bG9sbGTMmDHU1NQwcuRI7rzzTgBWrVrFLrvsQk1NDTU1NZx55pmt+/zqV7/ioIMOYsSI\nEcyYMeMdjyWp+0RKqbv7kElEpJ5+DpKknq2lpYUhQ4awaNEiBgwYQG1tLfPmzWPo0KGtbaZNm0ZN\nTQ3Tpk2joaGBcePGsXLlSlatWsUJJ5zAY4891u6YL7/8MgcffDCPPPIIu+22G5MmTeJLX/oSRx55\n5DaPJXUUEUAp5KSgHPJaRJBSis62OVItSVJGS5YsYfDgwVRXV9O7d28mTpzI/Pnz27WpqKhg3bp1\nAKxdu5aBAwe2bussbDz77LPst99+7LbbbgCMHTuWm2++Gcj9j31bx5LUPQzVkiRl1NzcTFVVVety\nZWUlzc3N7drU19dz3XXXUVVVRV1dHbNnz27d9txzz3HIIYdw5JFH8oc//AGAfffdl6eeeorVq1ez\nefNmfvvb39LY2AjAzJkzt3ksSd3DUC2pRynGvNW5c+dy4IEHMnLkSMaNG8fLL7/cum327NkMHTr0\nLXNay4G1LJzORppzX7v/09y5c5k0aRKNjY3cfvvtnH766QDssccerF69mqVLl/LDH/6Q0047jddf\nf52+ffvy4x//mAkTJnDEEUew11570atXr7c9Vrno338QEdGtr/79B3V3GdTTpJR69Ct3CpLeD7Zs\n2ZL22Wef9Nxzz6WNGzemgw46KDU0NLRrc8YZZ6Sf/OQnKaWUnnzyyTRo0KCUUkrPPfdcGjFixFuO\nuXnz5vTRj340vfzyyymllM4555w0a9aslFJK9957bzr66KPTpk2bUkopvfjii0U7t+3NWhbWAw88\nkI499tjW5QsvvDBddNFF7doMHz48NTU1tS7vvffendZh9OjRaenSpW9Z/7Of/Syde+657+pYPRWQ\nIHXzqzzyRWnUsrzqmbaRSR2pltRjFGPe6tZ1r732Gikl1q1b17rPT37yE2bMmNE6OtivX7+inFd3\nsJaFVVtby4oVK1i1ahUbN25k3rx5jB8/vl2b6upqFi5cCEBDQwMbNmygX79+rFmzhpaWFiA3j3rF\nihXsvffeALz44osAvPLKK1x55ZVMnTr1bY8lqfsYqiX1GMWYt9qrVy+uvPJKRowYQWVlJQ0NDUye\nPBmA5cuXc//99zNq1CiOPPJIHn744e1wltuHtSysHXbYgSuuuIJjjjmG4cOHM3HiRIYNG0Z9fT23\n3XYbAJdeeilXXXUVI0eO5Atf+ALXXnstAPfffz8HHnggBx98MBMmTOCnP/0pffv2BeCss85i+PDh\nHH744XznO99hn332edtjSepG2xrC7ikvyuTrBEnv7KabbkpTp05tXb7uuuvS17/+9XZtLrvssnTZ\nZZellHJfye+///4ppZQ2bNjQOi1h6dKlqaqqKr322mtp06ZNaezYsWnlypUppZSmT5+eLrjggpRS\nSgcccEA666yzUkopLVmyJO21115FPb/tyVqqlFESUxbKI1+URi3Lq57J6R+SerrKykpWr17dutzU\n1MSAAQPatZkzZw4TJkwAYNSoUaxfv541a9aw4447suuuuwJQU1PDPvvsw/Lly3n00UeJCAYNGgTA\nhAkT+NOf/tT6eZ/97GeB3Nf7FRUVvPTSS8U+ze3CWkpSYRmqJfUYxZi3OnDgQJ588snWgHfPPfcw\nbNgwAE466SQWLVoE5KYvbNq0id133317nW5RWUtJKqxe3d0BSeqqtvNWW1pamDJlSuu81draWurq\n6rj00kuZOnUql19+ORUVFe3mrX73u9+ld+/e7LDDDq3zVvv27Ut9fT2HH344O+64I9XV1fz85z8H\nYPLkyUyePJkRI0aw00478Ytf/KIbz76wrKUkFZaPKZckSSWlNB6tXT6P1e7+WkI51TP5mHJJkiSp\nOAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVU9FAdEcdFxFMRsTwizu1ke1VE3BsR\nyyLi0Yg4vs22AyPiTxHxeET8JSJ2LHZ/JUmSpHerqPepjogKYDkwFngeeAiYmFJ6qk2bnwLLUko/\njYhhwB0ppb0iYgdgGfCFlNLjEbErsLbjTam9T7UkSeWlNO6tXD73Ve7+WkI51bO77lN9GPBMSmlV\nSmkTMA84sUObFqBP/n1foDn//hjgLymlxwFSSq+YniVJklSKih2qBwKNbZab8uvamgV8MSIagduA\nr+XXDwGIiAUR8XBEfKvIfZWKYsGCBQwdOpQhQ4Zw8cUXv2V7Y2MjY8aMoaamhpEjR3LnnXcCsGrV\nKnbZZRdqamqoqanhzDPPbN3n+OOP5+CDD2bEiBGceeaZrX/9n3POOQwbNoyRI0fyuc99jnXr1m2f\nk5Qk6X2uV5GP39nweMfR5lOBa1JKl0fEKOB6YHi+b58CDgXWA4si4uGU0u86HnDmzJmt70ePHs3o\n0aML0nkpq5aWFqZPn86iRYsYMGAAtbW1nHjiiQwdOrS1zfnnn88pp5zCtGnTaGhoYNy4caxcuRKA\nfffdl2XLlr3luDfddBMf+tCHAPj85z/PTTfdxIQJEzjmmGO46KKLqKioYMaMGVx44YVceOGF2+dk\nJUkqM4sXL2bx4sVdalvsUN0E7NlmuZLc3Oq2pgDHAqSUHoyInSOiX37f+1JKrwBExB1ADfC2oVoq\nJUuWLGHw4MFUV1cDMHHiRObPn98uVFdUVLSOKK9du5aBA//5Zc62ZjxtDdSbNm1i48aN+TlzcNRR\nR7W2GTVqFDfffHNhT0hSp/r3H8Tf/raqW/vwsY9V89e/PtetfZDKTcfB2lmzZm2zbbGnfzwE7BsR\n1fk7d0yRO9FNAAAgAElEQVQEbunQZhVwFED+QsWdUkprgLuAA/MhuxdwBPBkkfsrFVRzczNVVVWt\ny5WVlTQ3N7drU19fz3XXXUdVVRV1dXXMnj27ddtzzz3HIYccwpFHHskf/vCHdvsdd9xx9O/fnz59\n+vD5z3/+LZ999dVXc/zxx79lvaTCywXq1K2v7g710vtdUUN1SmkLMB24G3gCmJdSaoiIWRFRl292\nNjA1Ih4FbgC+nN93LXAZ8DC5u4A8nFK6s5j9lQqts5HmraPKW82dO5dJkybR2NjI7bffzumnnw7A\nHnvswerVq1m6dCk//OEPOe2003j99ddb91uwYAEvvPACGzZs4N577213zAsuuIDevXtz2mmnFeGs\nJElSR8We/kFKaQGwX4d19W3eNwCf3sa+vwR+WdQOSkVUWVnJ6tWrW5ebmpoYMGBAuzZz5szhrrvu\nAnJTNtavX8+aNWvo168fO+6YuzV7TU0N++yzD8uXL6empqZ13x133JETTjiB+fPnM3bsWACuvfZa\n7rjjjrcEbUmSVDw+UVEqotraWlasWMGqVavYuHEj8+bNY/z48e3aVFdXs3DhQgAaGhrYsGED/fr1\nY82aNbS0tADw7LPPsmLFCvbee2/+8Y9/8Ne//hWAzZs3c8cdd7TO0V6wYAE/+MEPuOWWW9hpp522\n45luH/37DyIiuvXVv/+g7i5DQZRCLcupnpJU1Ie/bA8+/EWlbsGCBZx11lm0tLQwZcoUZsyYQX19\nPbW1tdTV1dHQ0MDUqVN5/fXXqaio4JJLLmHs2LH8+te/5rvf/S69e/dmhx124Hvf+x7jxo3j73//\nO3V1dWzcuJEtW7YwZswYLr/8cioqKhg8eDAbN25k9913B3Ij31deeWU3V6BwSuMhBuXzAIPuryVY\nz4L2oixqCdazkEqjllBO9dzWw18M1ZJ6jNL4n0P5/I+h+2sJ1rOgvSiLWoL1LKTSqCWUUz2764mK\nkiRJUtkzVEuSJEkZGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVkqJYk\nSZIyMlRLkiRJGXUpVEfEyRHx4fz78yLi1xFRU9yuSZIkST1DV0eq/yOl9FpEfBo4CpgD/Lh43ZIk\nSZJ6jq6G6i35f34G+FlK6XZgx+J0SZIkSepZuhqqmyPip8AE4I6I2Old7CtJkiSVta4G4wnAXcBx\nKaW1wG7At4rWK6lM9O8/iIjo1lf//oO6uwySJJW9Xl1plFJ6IyL+DnwaeAbYnP+npLfxt7+tAlI3\n9yG69fMlSXo/6OrdP+qBc4Fv51f1Bq4vVqckSZKknqSr0z/+b2A88A+AlNLzwIeL1SlJkiSpJ+lq\nqN6YUkrkv8eOiA8Wr0uSJElSz9LVUH1j/u4ffSNiKrAQuKp43ZIkSZJ6jsgNQHehYcTRwDFAAHel\nlO4pZse6KiJSV89B2t4igu6+UBGCcvlvxHoWTmnUEqxnQXtRFrUE61lIpVFLKKd6ppQ6vQPAO4bq\niNgBWJhSOrIYncvKUK1SVhq/zMrjFxlYz0IqjVqC9SxoL8qilmA9C6k0agnlVM9thep3nP6RUtoC\ntETEvxS8Z5IkSVIZ6NJ9qoHXgf8/Iu4hfwcQgJTS14vSK0mSJKkH6Wqo/nX+JUmSJKmDd3Oh4o7A\nkPzi0ymlTUXr1bvgnGqVstKYy1Ye89jAehZSadQSrGdBe1EWtQTrWUilUUsop3pua051l0aqI2I0\ncC3wHLm7f1RFxJdTSvcXqpOSJElST9XV6R8/BI5JKT0NEBFDgLnAIcXqmCRJktRTdPXhL723BmqA\nlNJyoHdxuiRJkiT1LF0dqX44IuYA1+WXvwAsLU6XJEmSpJ6lSxcqRsROwFeBT5ObU30/cGVKaUNx\nu/fOvFBRpaw0LhApj4tDwHoWUmnUEqxnQXtRFrUE61lIpVFLKKd6vucnKuYP8EFgff5BMFufsrhT\nSumNgvb0PTBUq5SVxi+z8vhFBtazkEqjlmA9C9qLsqglWM9CKo1aQjnV8z0/UTFvEfCBNssfABZm\n7ZgkSZJUDroaqndOKb2+dSH/fpfidEmSJEnqWboaqv8RETVbFyLiUODN4nRJkiRJ6lm6evePfwNu\niojnyU3MGQCcUrReSZIkST3I245UR0RtRPRPKT0EDAV+BWwGFgArt0P/JEmSpJL3TtM/fgpszL//\nBPAd4L+BV4CfFbFfkiRJUo/xTtM/dkgpvZx/fwrws5TSzcDNEfFocbsmSZIk9QzvNFK9Q0RsDd5j\ngXvbbOvqfGxJkiSprL1TMJ4L3BcRa8jd7eP3ABGxL/BqkfsmSZIk9Qjv+ETFiBgF7AHcnVL6R37d\nEOBDKaVlxe/i2/OJiiplpfEkq/J4ihVYz0IqjVqC9SxoL8qilmA9C6k0agnlVM9MjykvZYZqlbLS\n+GVWHr/IwHoWUmnUEqxnQXtRFrUE61lIpVFLKKd6Zn1MuSRJkqRtMFRLkiRJGRmqJUmSpIwM1ZIk\nSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlVPRQHRHHRcRTEbE8Is7tZHtVRNwbEcsi4tGI\nOL7D9j0j4rWI+Pdi91WSJEl6L4oaqiOiArgCOBYYDpwaEUM7NDsP+FVKqQY4Fbiyw/bLgDuK2U9J\nkiQpi2KPVB8GPJNSWpVS2gTMA07s0KYF6JN/3xdo3rohIk4E/gd4osj9lCRJkt6zYofqgUBjm+Wm\n/Lq2ZgFfjIhG4DbgawARsQtwTn57p89YlyRJkkpBsUN1Z2E4dVg+FbgmpVQFfAa4Pr9+FnB5SumN\ntzmWJEmS1O16Ffn4TcCebZYrgec7tJlCbs41KaUHI2KniOgHfBz4XET8ANgV2BIRb6aUOs65ZubM\nma3vR48ezejRowt5DpIkSXofWrx4MYsXL+5S20ip48Bx4UTEDsDTwFjgBWAJcGpKqaFNm9uBG1NK\n10bEMOCelFJlh+PUA6+llC7r5DNSMc9ByiIieOuXM9u9F5TLfyPWs3BKo5ZgPQvai7KoJVjPQiqN\nWkI51TOl1OnsiaJO/0gpbQGmA3eTu9hwXkqpISJmRURdvtnZwNSIeBS4AfhyMfskSZIkFVpRR6q3\nB0eqVcpKY4SgPEYHwHoWUmnUEqxnQXtRFrUE61lIpVFLKKd6dstItSRJkvR+YKiWJEmSMjJUS5Ik\nSRkZqiVJkqSMDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnKyFAtSZIkZWSoliRJkjIyVEuSJEkZ\nGaolSZKkjAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVkqJYkSZIyMlRLkiRJGRmq\nJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJ\nkqSMDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnKyFAtSZIkZWSoliRJkjIyVEuSJEkZGaolSZKk\njAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM\n1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWS\nJElSRoZqSZIkKSNDtSRJkpRR0UN1RBwXEU9FxPKIOLeT7VURcW9ELIuIRyPi+Pz6oyLi4Yj4S0Q8\nFBFHFruvkiRJ0ntR1FAdERXAFcCxwHDg1IgY2qHZecCvUko1wKnAlfn1LwJ1KaWDgH8FritmX/VP\nCxYsYOjQoQwZMoSLL774LdsbGxsZM2YMNTU1jBw5kjvvvBOAl19+mTFjxvDhD3+Yr3/9650ee/z4\n8Rx44IGty+eccw7Dhg1j5MiRfO5zn2PdunXFOSlJkqQiKvZI9WHAMymlVSmlTcA84MQObVqAPvn3\nfYFmgJTSX1JKf82/fwLYKSJ6F7m/73stLS1Mnz6du+66iyeeeIK5c+fy1FNPtWtz/vnnc8opp7Bs\n2TLmzp3LmWeeCcDOO+/M+eefzw9/+MNOj/2b3/yGPn36tFt3zDHH8MQTT/Doo48yePBgLrzwwuKc\nmCRJUhEVO1QPBBrbLDfl17U1C/hiRDQCtwFf63iQiPg88Eg+mKuIlixZwuDBg6murqZ3795MnDiR\n+fPnt2tTUVHROqK8du1aBg7M/Uh32WUXPvnJT7LTTju95bj/+Mc/uPzyyznvvPParT/qqKOoqMj9\nazhq1CiampqKcVqSJElFVexQHZ2sSx2WTwWuSSlVAZ8Brm93gIjhwIXAGUXpodppbm6mqqqqdbmy\nspLm5uZ2berr67nuuuuoqqqirq6O2bNnv+Nx/+M//oOzzz6bD3zgA9tsc/XVV3P88ce/985LkiR1\nk15FPn4TsGeb5Urg+Q5tppCbc01K6cGI2Dki+qWU1kREJfBr4Isppee29SEzZ85sfT969GhGjx5d\nkM6/H6XU8W8eiGj/t9HcuXOZNGkS3/jGN3jwwQc5/fTTeeKJJ7Z5zL/85S+sWLGCyy67jOeee67T\nz7jgggvo3bs3p512WvaTkCRJKoDFixezePHiLrUtdqh+CNg3IqqBF4CJ5Eam21oFHAVcGxHDgJ3y\ngbovuekgM1JKD77dh7QN1cqmsrKS1atXty43NTUxYMCAdm3mzJnDXXfdBeSmbKxfv541a9bQr1+/\nTo/5wAMPsGzZMvbee282bdrE3//+d8aMGcO9994LwLXXXssdd9zRuixJklQKOg7Wzpo1a5ttizr9\nI6W0BZgO3A08AcxLKTVExKyIqMs3OxuYGhGPAjcAX86v/yqwD/AfEfFI/pZ7nac2FUxtbS0rVqxg\n1apVbNy4kXnz5jF+/Ph2baqrq1m4cCEADQ0NbNiw4S2Buu1o9Fe+8hWampp49tln+cMf/sB+++3X\nGqAXLFjAD37wA2655ZZO52JLkiT1BNHZV/E9SUSknn4OpWbBggWcddZZtLS0MGXKFGbMmEF9fT21\ntbXU1dXR0NDA1KlTef3116moqOCSSy5h7NixAOy111689tprbNy4kb59+3L33XczdOg/76K4atUq\nTjjhBB577DEABg8ezMaNG9l9992B3Mj3lVde+dZO9VC5qTPd/e9ndDrlpieynoVTGrUE61nQXpRF\nLcF6FlJp1BLKqZ4ppc6uGTRUS8VUGr/MyuMXGVjPQiqNWoL1LGgvyqKWYD0LqTRqCeVUz22Fah9T\nLkmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1J\nkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRoZqtdO//yAiottf/fsP6u5SSJIkdVmklLq7\nD5lEROrp51BKIgIohXoG5fBzLY16lkctwXoWUmnUEqxnQXtRFrUE61lIpVFLKKd6ppSis21lMVK9\nYMEChg4dypAhQ7j44ovfsr2xsZExY8ZQU1PDyJEjufPOO1u3XXjhhQwePJhhw4Zx9913t65/9dVX\nOfnkkxk2bBjDhw/nz3/+c+u22bNnM3ToUEaMGMGMGTOKe3KSJEkqeb26uwOFMH36dBYtWsSAAQOo\nra3lxBNPZOjQoa3bzz//fE455RSmTZtGQ0MD48aNY+XKlTz55JPceOONNDQ00NTUxFFHHcUzzzxD\nRHDWWWcxbtw4brrpJjZv3swbb7wBwOLFi7n11lt5/PHH6dWrF2vWrOmu05YkSVKJKIuR6sGDB1Nd\nXU3v3r2ZOHEi8+fPb7e9oqKCdevWAbB27VoGDhwIwC233MLEiRPp1asXgwYNYvDgwSxZsoTXXnuN\n3//+90yaNAmAXr160adPHwB+/OMfM2PGDHr1yv090q9fv+11mpIkSSpRZRGqq6qqWt9XVlbS3Nzc\nbnt9fT3XXXcdVVVV1NXVMXv2bACam5vb7Ttw4ECam5t59tln6devH5MmTaKmpoYzzjiDN998E4Dl\ny5dz//33M2rUKI488kgefvjh7XCGkiRJKmVlEao7yk3K/6e5c+cyadIkGhsbuf322zn99NMBOp0w\nHxFs3ryZZcuW8dWvfpVly5axyy67cNFFFwGwefNm1q5dy4MPPsgPfvADJkyYUPwTkiRJUkkri1C9\nevXq1vdNTU0MGDCg3fY5c+a0ht9Ro0axfv161qxZQ2VlZaf7VlZWUlVVxaGHHgrA5z//eZYtWwbk\nRsI/+9nPAlBbW0tFRQUvvfRSUc9PkiRJpa0sQvWKFStYtWoVGzduZN68eYwfP77d9urqahYuXAhA\nQ0MDGzZsoF+/fowfP55f/epXbNy4kZUrV7JixQoOO+wwPvaxj1FVVcXy5csBWLRoEfvvvz8AJ510\nEosWLQJyU0E2bdrE7rvvvh3PVpIkSaWmLO7+ccUVV3DMMcfQ0tLClClTGDZsGPX19dTW1lJXV8el\nl17K1KlTufzyy6moqODaa68FYP/992fChAnsv//+9O7dmyuvvLJ16siPfvQjvvCFL7Bp0yb23ntv\nrrnmGgAmT57M5MmTGTFiBDvttBO/+MUvuu28JUmSVBp8+Iva8SbxhVUa9SyPWoL1LKTSqCVYz4L2\noixqCdazkEqjllBO9Szrh79IkiRJ3clQLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJG\nhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRr26\nuwOFEBHd3QU+9rFq/vrX57q7G5IkSeoGZRGqIXV3B/jb37o/2EuSJKl7OP1DkiRJyshQLUmSJGVk\nqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlZKiW\nJEmSMjJUS5IkSRkZqiVJkqSMDNWSJElSRkUP1RFxXEQ8FRHLI+LcTrZXRcS9EbEsIh6NiOPbbPt2\nRDwTEQ0RcUyx+ypJkiS9F5FSKt7BIyqA5cBY4HngIWBiSumpNm1+CixLKf00IoYBd6SU9oqI/YEb\ngFqgElgIDE4dOhwRCYp3Dl0XFLOW20tEYD0LpzTqWR61BOtZSKVRS7CeBe1FWdQSrGchlUYtoZzq\nmVKKzrYVe6T6MOCZlNKqlNImYB5wYoc2LUCf/Pu+QHP+/XhgXkppc0rpOeCZ/PEkSZKkklLsUD0Q\naGyz3JRf19Ys4IsR0QjcBnxtG/s2d7KvJEmS1O2KHao7Gx7vOPZ/KnBNSqkK+Axw/bvYV5IkSep2\nvYp8/CZgzzbLleTmVrc1BTgWIKX0YETsHBH9urhv3sw270fnX5IkSdJ7t3jxYhYvXtyltsW+UHEH\n4GlyFyq+ACwBTk0pNbRpcztwY0rp2vyFiveklCrbXKj4cXLTPu7BCxWLzgsaCqs06lketQTrWUil\nUUuwngXtRVnUEqxnIZVGLaGc6rmtCxWLOlKdUtoSEdOBu8lNNZmTUmqIiFnAQyml24Czgasi4hvk\nLlr8cn7fJyPiRuBJYBNwZsdALUmSJJWCoo5Ubw+OVBeWf9EWVmnUszxqCdazkEqjlmA9C9qLsqgl\nWM9CKo1aQjnVs7tuqSdJkiSVPUO1JEmSlJGhWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJ\nkqSMDNWSJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnKyFAtSZIkZWSoliRJkjIyVEuSJEkZGaolSZKk\njAzVkiRJUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM\n1ZIkSVJGhmpJkiQpI0O1JEmSlJGhWpIkScrIUC1JkiRlZKiWJEmSMjJUS5IkSRkZqiVJkqSMDNWS\nJElSRoZqSZIkKSNDtSRJkpSRoVqSJEnKyFAtSZIkZWSoliRJkjIyVEuSJEkZGaolSZKkjAzVkiRJ\nUkaGakmSJCkjQ7UkSZKUkaFakiRJyshQLUmSJGVkqJYkSZIyMlRLkiRJGRmqJUmSpIwM1ZIkSVJG\nhmpJkiQpI0O1JEmSlJGhWpL+T3vnGW5XWbTh+yGhhN47IQaQqihFehFEAkgVFBBRAUFpBlGaiCAg\nijRpIiIoJWBAwNBCCURQegdpgohUlf5J0Ag83495N1kcUk5y9j7rnL3nvq5z5ay9V3BcWetd8848\nM5MkSZIkPSSd6iRJkiRJkiTpIelUJ0mSJEmSJEkPSac6SZIkSZIkSXpIOtVJkiRJkiRJ0kNa7lRL\nGibpMUlPSDpwIt+fIOk+SfdKelzSq5XvfiLpYUl/lnRS66wc27r/dEcytm4D2oyxdRvQRoyt24A2\nY2zdBrQZY+s2oI0YW7cBbcbYug3oF7TUqZY0HXAqsDGwPLCDpGWq59j+tu1P2l4JOAW4tPzdNYA1\nba8ArAB8StK6rbF0bGv+sx3L2LoNaDPG1m1AGzG2bgPajLF1G9BmjK3bgDZibN0GtBlj6zagX9Dq\nSPWngL/Yfsb2/4CLgC0nc/4OwIXldwMzSZoJGAQMBP7RSmOTJEmSJEmSZFpotVO9CPBs5fi58tmH\nkDQYGALcCGD7dmJr9CLwPHCt7cdbaGuSJEmSJEmSTBOy3br/uLQt8Fnbu5fjnYBVbX9rIuceACzS\n+E7SEsBJwBcAATcAB9j+Y5e/17r/A0mSJEmSJElSwbYm9vnAFv/vPgcMrhwvCrwwiXO3B/asHG8N\n3G77bQBJ1wCrAx9wqif1fyxJkiRJkiRJeotWyz/uApaUtLikGQjHeVTXkyQtDcxZJB8N/g6sJ2mA\npOmB9YBHW2xvkiRJkiRJkkw1LXWqbb8L7A1cB/wZuMj2o5KOkPS5yqnbE0WMVS4B/go8BNwH3Gf7\nqlbamyRJkiRJkiTTQks11UmSJEmSJEnSCeRExSRJJomkrFlIkqRjyTWwNUiasW4bWkE61S1C0oC6\nbWhHJC0raa267egEJK0AHCBpUDnOlwsgafbSPz+pAUmDSo1OQj6XrULSUEnzOdP5TUfS8sDZkuao\n25Zmk051C5D0UWC4pPXqtqWdkLQg0VpxjnyRtBZJswPnET3i3wXIl8v7z/ZFwDaSPlK3PZ1Gmch7\nKrBzY7PXyUgaCnxB0rJ129JOSFocuBNYom5b2o0SkDgcGAEMkLRQvRY1l3Sqm4yk5Qhn5G3g1ZrN\naTdmAS4G5gDOlzRTZgRag+03gUuBFYF7JM3b6de6OC6XlJ/LbT9ds0kdRVlbzyXaql7VaLfaqZQN\nxm+B+Yk1MWkeg4n3+OySjlaQ/lJzEOEfbQeMBGau15zmkjdJE5E0L/Ab4ETbp9t+qHyeUdUeIGku\nSYNtPwUsDvwCuMn2f0qHmaRJSJpP0pbl8G2ie8/1tl/u1GtdXqjTA4cCZ9k+2/a4xnf1WtcZSJoZ\nOAY4xfY5tl8sn3fkO0zSwsTm7gTbp3RpR5tMI5JmK1m6h4FPEo71/Q7eq9e6/k2R02xeNsOXA58H\nXizv9bahIxekFjIP8LztiyRN13jhZtp82impoqOA3Uqq83bgamAhSavWalybURyUrYGtJG0DPAgc\nCPxX0l6S5i7ndZQjWV6o/wPeoQyfKk72+892cXKS1mEiwvUATLgHG45OuxY9TYalgYdsX1g2fR31\nTLaC4kz/BNgWmJFoA/wAMFt59yQ946PABZI2Ax4DvgisIOnIes1qLulUNwFJi5bU+JvA7PD+Yq/K\nOUMkfbomE/sttv9DaCgXATYmMgE7AfMB20n6ZI3mtQ2SZPs922cCfwHWAQbYPhm4A1iWuN5zd9Im\nUdICkvYthwOJnvrY/l8ZTKXy7K8uadbaDG1TJC0lafUS3RoHNApEB5TvpyvFTl/qBMda0nzl1+q1\n6HrO8qUQLJkKiuTtZmJy87rAIcA+wAbAlkVnnUwDkgbYHg0cBJwMLG37amAbIohzWK0GNpF0qntI\niRB8DzijpCTfk3QmhGNdqVJfBlgzq9a7TyW1Oych+/gBMByYrfw+EPiapJXrsbB9qERcNwI+QzjV\n+0rawvblxACnVYEdJQ2sz9JeZ3VgtVJMcyqRIfkixHCrct3WBfYida2tYB3gwpIluR0YKWl+2++U\nF/V7wMeAjYC23tSU7Mi5kvYhIn2rSNqlZFLcyJ4AS5bv8v3eTSr1In8DlgJ+BGxh+3HgRODjwPaS\nhtRhX3+mPKfvKgb+rUkM87tI0o6lLuVzxHv8qFoNbRL50PWQ8lI9H3hX0seBXYkX72nl+/ElmvpT\n4B7b4+uztn9RNiWfBM4mIgZfB4YAewD/JWQh0wFv1WVjf6fyIkbSIsTL5JuEMzMG2FjSRrZHAVcS\nWvZ3ajG2F6lEPccSm7f9bN9GOHbDJB0u6SOSNgHOILStz9djbVtzLlHM9BXbJxHFs7eWotFFJX0K\nOB240PYrNdrZG7wD/JpIo09HvGt2kfQVeD97sgrxDP89NcDdpzh9awHnAAcTHX4+Vxy/e4hnfMU6\nbexvlBqzxrVdCDgS+I3tbQk99Unl+j4DfBq4tj5rm4jt/JmGH2JhW6/8PhNwAnB8OV6CePmOIV4I\nDwFb1W1zf/gBFgMuqBxvClxTOV4deBQ4ntCwD6zb5v76Q0hoxgIrVK793cBHy/EcwIWEjnjLuu3t\nxeuyMNFlZvVyvADwBPBVQtK1IfB74DJgFLB5OU91294OP2VtPR5YuBxvUtbRxvExxAbv2rLGbtXO\n1x/4CDBH+X0x4ALgq+V4C+AlIrBzRrlPt6jb5v7wU97Te1eO9wTOrhx/rby7dynHs9dtc3/5AQYB\nxwE/rXx2NvAJQlYI8G3gvapv1A7PcCelcZtGSRXtCXyj6C3vIfou3i7pBdvHExrLtYgOCm/bfrTo\nVjtGjzq1SBpMRGBWkvR721sSvUJfl7QB8Cfbt0u6ElgBmNXtH51qCZKWJtpGPUi0J/yS7T9LGk1o\n3PLAYasAACAASURBVEba/pukSwkd8WN12tvLrExIOuaWdApxjXYjXrL32R4DjClR/hlsv5XPdlMZ\nSnSdmV7SM8BJwGcJx3ED2wcXbfH/gOlt/6tdr3+RC/4CGCppf+BW4BTgckmP2h4l6UFgJaKf/Fm2\n727X69Esyv0zD3CypEG2f0q8x9eXtKLtB2yfI2lbYBNJ17h0nEkmj2Jo2DeIwMQuko6yfSjwb2Id\nHV5OvZ3YGL/fergd7lm1wf+HWpC0PtFi61xgOeBZ4CbgWODHtv9Yn3X9j+JQ3wd8wvazxbkbb3sL\nSd8mIjQvEpGDQ4EDbP+pPov7L5LmIa719rZvLRrNPYiikTmJ6vfliPt5N2BP2zfVZW9vIWlmT2iV\ndwjRCWUE0R99fuBl4G+2z5U0nUOelM5Lkylp40OBR4guDOsRL+kLgdElaNERlADOl4gN3dnA2sBZ\nhAxuZ2Av23+vzcB+iKSlgD8QdTqLAX8CjrN9fNlEv0Tce38joq2H2L6jJnP7FaVY+woia3IDUeD+\nTeAuoibld8D/Af8kstBfs31bO62jqameCiQtJulbALbHArcRIvtjCA3qcKIqe8MsEuk+JeI3N5FO\nX1fS920PAxaUNML2CYSDtzjxcj0mHeppQ9JswHiiT+jWkm62fQqhVf0t4TgeW36fjUiPdoJDPQ9w\npKThALZ/RPQBfp1It89CyD+OkbSMi161XV4EdSNpYUnXlCLElwl5zc5E0GIMcATwCjHJcqkaTe0V\nJA2WtJ6jN/zviGjekkTU/hSipd44wslOukl5Ly8HjCY2a0uVPw+RtBvRneI/wFeAnwE/S4e6eyi6\n8MxASJCGEvr/F4luH2sSzvVWRKDicSJYcxu01zqajt/UsQhwgKRfSVrC9veJm2OY7e2JnpbzEt1A\n5q3Rzn5DkSHsB7xBpOOOJxw7bH8KWKI41qNs7wV82faVpetKMhUoJrD9nNjA/B8Rnb4fwPZhhD54\nJLCA7fOAHxapQycwiJB5bC/p16Ww5llC3vG07V2BMwnnJntSNxnbLxDp4V9L+joh+zqBWA9+RVz7\n+4E1aLMJbF0pa9ungWsl7UEUKO5KOCozAF8gNryfAg5XDMZJpoCkJYEdgL8SdRLnEDrpJwi510+A\nnUomZGtguyKvyXfNFCiFw9cTG72/A4cBdzkGu9xNPMtrAIfZvtr2L2zfWJvBLSTlH1OJpEHAacRC\n9yyx0C8IjLT9hqTFgLltP1Cjmf0CSR8lqqxPtH1eSb3NSzyEN9u+q5z3IPCC7WGNtHt9VvdPyrU+\nj4i8jJD0BaLN4xzAU7ZPL+f9iJCBrAK81U4RhO5QIvkXAk8BrxERljNtX1K+n8f2K+2Urqyb6rWU\ntBNRzLQCIXOYA3jO9rXl+/lt/7M2Y3sRSesQnSgeJ/S+/wIG2/5luU+XAQbZvrlGM/sFJXhzMZHx\nuJLIiv6HyIiMsf2cpOUIWdwRJVOVdINybS8kJDQjJG1NFIF+Bvglca0hnOpvAYeWjUxbkk71VCBp\noKM/6gxE4cwGRBHX48DFDcekcn6+eCdBeRDHANfZ3qV8NgMRjdmVaJP3e9v3le/WtH1rXfb2Z8q1\nvgEYZ3vp8pmIVnHbEenPexyDX5C0pO0n67K3LjShn+oMhI51MWB3YprfDlkn0Tqqm2VJCxJ9p48n\nNjev2P5cl/Pbem2taPaHElHrTQipwkPEqPY7Kue29bXoKZX173Tbx5TA2IxE3/3NiXtspO0XFUNz\nFrZ9fX0W9x/KtR1FDL7b2ZWGDJK2J7KhPyvnAMxm+42azO0V0qmeSrouYEVjfSCRklwW+EdGUidP\niZqOIApEVgNOK3KDxverEuk3AZc4+oQm04CkJQhJx0nEZuVV29tUvp8dGAZsBtxh+/ROzgZUCxCJ\nFPvuwHeATRobvKQ1TGRtXQnYl0jZr2b7/tqMqxHFsKVZiH7cOxCa1U8C/+3U57S7FKfvAkLbOyfw\ndduPVb7fhtClvwCMKDKk3Kh0g3JtRxLX92XiXT7C9h8q52wLfBc41vbvajG0l0mnehJMzUMlaW0i\nVZ4v3SkgaSZi53qL7fMlbUwUxh1r+4LKeasRUdRf2P5LPdb2b4pj+HXgDdu/LZ/dCTxr+/OV82Yn\nCm4ftP1wLcb2YSQt4hzs0jQaGYGuv3c5pxHtmhuYxfazvW5oH6GLPOYrwJNZqD1lyrp2NjDK0bHn\nQOKdsmNVflCkcOsQ8oVn6rG2/1HkSUOKdHMJIhC2FHBeNatXItZPd0rBZzrVE6Gkhxa1/ZeSDvqH\noyK963kfiujlDnfKSJq3ej0rjvVPbZ9f+Xx222/WYWO7IGl6x6S1gS6TECXdQehUq471RJ2bdqYU\nI46bVDpSXdrm5bPdcxSTKlcGnqQMNSE0rVO899o9g6JKS8eJfPeB5zPvxe5RlbKVIMMBhGP9JccI\n8sZ5C9j+R01mtgXFsd6S6Exzbqdu/LL7x8RZgugCcAxwFbHwf4jywh3Q5bNc6KZA1w1KKUL6LjBc\n0i6Vz9Oh7iG2/1f+fKekkbG9GtGu8JrKeR3hUJcXayMTcgVxz80wqdOrf+az3RRmBZYn+theDLw0\nqXuvsbaqtCdtR4e6cj8uDewvacjEzita/wGV47wXJ0PjulYdagc/Ie67Xys6VlDOS4d6Gqlc66eI\nws9Hgd0lrVurYTWRTvVEKClwEVrKs8rN8iEqhU2zSdqyq4OddB/b1xFtePZX9KzNNkZNpotjvRYw\nf9Gtdgwl4rwJ8ENi/Po3gW9KmqV6XuXZngs4sev3ybThmID6BDHqfSzRB/xDVK7/HMChiqESbUe5\nHzcmWo7tCuysifThrlyPWSSt3uuG9jO6bjqqx8WxvgYYIWnmfNf0jC7X9ikiEPkQlUmJnUTKPypU\n0rxDgbmIqushxPSlMbZflTSD7fGVRW5O4GpgX9t312d932Rq05RdpSFJ9+mOVrV8974UpJMoL8/Z\niWEaP7N9haRPEUWcVxO6/vGa0OVnTmIAzI/cpj1Ve4vK2rom0dlnELA+UQw6yvYdZQPzNjFJ9b3i\nUF8JHNSuqWRJKxLFXtsRxYcrEdPmzm/oe7vcj1cQQzMeqsvmdkHSUNt/rduOdkQx+v3tuu2og4xU\nVyiL/ubEgIzxto8jBrpsCaym6L94RNG+NRzqi4ED06H+MEWbvmT5fXnF+OHJ0nCoM3owdRSt6mqS\n5i/Shg0nlTkpL2hVUs8dsQ6U9O8bxAji2STNaPtO4MdM0Fo2rs9chLNzRDrUPaesrVsSo4rntX07\n4SAOADaTtD8xPXGe4lDPSUz5PLhdHerCEOAR2w/a/g3Remxz4GuSBsP79+OcxP34vXSoJ4+6OQwn\nHeqpR9JCZbM7WTrVoYZ0qj+ApE8SL9j9bD9UnMKziZHNWwBHE1OCxpV05MXAkbZvqc3ovk23tOnw\nYccuNYNTzVRpVcv1bWjh2k6r2qCycVhC0grl4yeIoS5DyvFTwO3AsZJWL3/ncuDofLabg2Io1qHA\n1rbHlKImAScSQ00+A5xt+3lFh6BfEhM926o3eOV+HFg+uhMYKGkLAMcE07uItXOVcm5jg3GUc9DL\nROmuNr2ck37PVFC5tlOsQ1GXOohOZOCUT2l/NKGqfAlCZD9e0X7nM8DixOI2lmhc/kL5ax8FvusO\n7Z3aHWw/rOgD+h0i4jdFbToxUOfKTimcaxaOKX9PAMcRxSLd0ap+S9IJtv/dm7b2JiVC+jngR8Bd\nRa+6G7Ac8L3i3KxEtBT8MjGW3JK2c4dM7uslBhFBnLUk7UdsaDYFtrB9iqSzKtGtQcABtp+ux9TW\nUe6tzwLrS3re9mmSbgDWVhTO/ZEYP349EZC4jBihfaDLhNnkw1S06fsSwQVJutBd2rFWtenAx0rG\nJJkMlTqU4UyoQ3m9PLNvNc7TB+tQDpd0SPX7TqFjdxPwAYnBbOXPK4B3idHZrwI7EYvbOrb/r+JQ\nY/vedKgnTmVnO5SIUB9MdJvYTtF3tjE9sfogzglcCzyfDnX3qVzrNQlHemNC3rBHiSwgaS5JM5XN\nY8OhvpKoE2hLh7pyXT4KfJuYgPp7YCFiLPuehJb6d4RDPRj4IvA8QDrUPaNy/ZeRND/wV+J6bwfc\nZHsrYtraamUNeLucP53t19rNoa5E8NYgovN/B74r6SBCt38zMZ79QMJpuZaI5A+wPSod6smj0Kaf\nTLxrfgDMA3xB0uKVcwZW3jWjCW1/MhmKSnAOYH/gVNvDiaz9F4H9Ku/x6rW9mJiG3JnX13ZH/xBO\nyCjgNGAfYGDlu5WIyPXKddvZ334IXeC1RDQAYgjJ+cS43a2BY4CZy3dzMmHzUrvt/e2H0PzfC2xY\njpcnZEw/JBbDK4BFKtd6DLB23Xa36FrM3OV4XiJ6tS9wBzC0fL5B5ZxlgD837tX8adq/xablvjwM\nuKX8WzSK49cr13zDuu1s8TVYGJi1/L40cA6wWzleFLgVOLRy/kzEdNP7gBXrtr+//JQ18LLK8YaE\npOtwYHDl8zmB64B167a5P/0QG5YdgRnL8RbEaPIvVc6Zq1zbjn6Pd3qkeiWiKPE04DaiqO5USbMq\n2haNJNKQOSZ7Kkhteu+RWtUJlPT5CEm/l7RNyYrMQmycdwG2sv3XEtU/sZyPY2zxus4CsKYh6SPE\npm4rIoMyPfAeMF357lSiq8eY+qxsLZKmB75AZEEA5gbmAzaQNMT2c8DngW0lHQdg+z+ENGZH2w/0\nvtX9g9Smt46sQ+kZHdlST3q/vdMGwGa29y9pjMGEg3I68BiwpO17G+fXaXN/QBMm0G0LbA8cRLw0\nqtp0U9Gml43Ne04pzTRR5A0XAscTWswhTNCqjlaltVHRus3pNkutw/sO9TmExGAI0Z7sx7bvK5rq\no4nuEjMR9+bBtq/MZ7t5VNbVuYmi5C8B9xNr6k62n5S0IXATsKDtF9r5+hdHY0aijeOxwJ5EVmQX\nYqLkpbb/LmlhYHHbt9VmbD+koU0nJIOnSfoWsAjwCqH9PYHIgH6UkCtsDrzolNJMkWodCjF6fDdC\nUz0rUYtXrUMZY/tmSfM7ZXOdFalu7MAqi/grhO5qPdvjHdOXBhIL/pu27+1yfjIRUpvee6RW9cOU\nTMjpwOu2L7L9Y+Bh4ItF63clIT8aT7Rw2ycd6uZScajXJ/4txhEb6nOAzxSHeh3gEGCxxhrQjtdf\nMVBk8fL/bVFikzeOyB49CowgRrTvKGmw7RfSoe4eqU1vHVmH0hw6JlJdWfTXIzRrTxK7sMWAvYFf\nAE8DvwJ2z4dv6lBUXu8FPEtE+X/uMmCkRKMvIKJVKaXpIZI2BY4i0m0bERr1Vyr39+nEMKK2Ta03\nkDS77TdL1OorwAO2jy0v2c8TmZGLgAdt31Cnre1OyfxtAVxj+1pJawFHEKn5h4HvAofb/n2NZrac\nkjLfnZBfrUxE+AYQ75nZyp+rAjsAP23HjW6zKdH8N23/W9E27yDgT7bPkrQoIdW82vZR5fyZCF31\nUcBXU0ozaRRzN8ZVjucl9NMQ2aYdimxuA5ee/ZKWIZzr7VM290E6IlJdcag3As4kItRLEgU0cxE7\n3m8RlcM/TId66khteu+RWtUJKIY8nFleAjcQevFVJV1BXJ+dgfOIFPy5kpaszdg2ppKp2pooCG2M\nJ76byJrMDwwl7svfV85vVx4j3q37A7fY/hvwN2KNfIMI3NwNHJIO9ZRJbXrryDqU5tPWkWp1GdUs\n6TuE/urCoi9dl6ga3pXojfqOY0xxpoW7QWrTe4fUqk6a8hzPB6xm+7wiPzgOuN72wZXzZrf9Zk1m\ntiWV+/L9ayvpJGJNXbY4Nh1BV2mhpC8DHyc2E2favrZ8vgIhhxth+8GazO1XpDa9NWQdSmto20h1\nSRH9WNIBmjD9Zz7gqwC2XyNGkM8DLGB7nO3x5bu8YSZDatN7j9SqTpzKPfga4bgcKWlH22OJYUND\nJB1W+Stt2Y+7LopG35KGAWdJ+nW5/sOJDgt3l0xCNZLdtrggaeUiRbqT6Jc8Cti7fL4YsDbhuKRD\nPQVSm946sg6ldbSlU10c6t8CLxFC+p+Ur34APCfp5HI8K5GanLXXjeynVLXpko6VtDtxH+1FTKj7\nfJGDLA+8WKuxbUAlE7AVcI7tfxCazAeAQyTtSPQQPdn2MzWa2qs0rouk1WyPJjSsB0naqTjWZwEr\nF0kMbuNR7L1J0ari6PLzCULS8HNKX2VJP7C9P9Gf+gG1+bhiSQtLWrv8Powoltuq/NnIKF1GODB/\nBP5ie6LTTpMPMZQYOf59Ivv0T6LL0TjgFOIeu4TIjA6oy8j+RsksvU3MinhF0gHlq/8ROvRbJX0b\nmN32KbZ/WNbUtg/WNIO2k39IGgxcDfzK9omKvr0/JqInDxIO4GGELmseonDm8rrs7U900aafSvSe\nnodouXM5segdDPyDSG9eVpuxbUDlep9CbFpWs32XpBmJyM3BhFbzLkdhWNtHESrXZAXiuf4sMcjm\nTkWx7I+IyV/nSJrb9quT/Q8m3aYEK74GnGD7n5I2I2RfexbneXWiNuUA289I+ng7R2QVnSiGE3U5\nRxC1I6fYvrFshHcErrJ9maTlgEHOupJuo+hBfRIhlznS9vHlmi9BaPUXJFq9zZgble5RskdnE4GZ\nVwkJ7F6ExGM+4vn+DBFo3IfQTT9Zj7X9k4FTPqXfMZSour5T0gJExOpVImW+DlGZvp2kIcD4TtKf\nTisNbXrlGq1IbEYmpk3/A6lN7xGV6zYbUfG+j6R3gZGSGlrVp4gXyvt0wrUuDvXGRAr4EKJjz3WS\nNisbi4HATyVd60rrxqRnKKr9LyA2LI3WWc8A60ja0FEYe6uk/YiOF88Q6eS2xTGW+Qmiw8SJwOPA\nxyX9oTjWCxOjnK+1/UitxvYTqtJC2+9IugN4G1hT0sMObfoTks4hnO2l2nnj1mwcA9e+STjQmzjq\nUGBCHcqjhLQGSac561CmmrZxqhU9ezcsjt68RDHD0sDttvcuL9sjiDHZVzgqsoHOcEamlRKd2k3S\nv4CTHLrz+YhWbhfafk3SA0TRyAK2X2r83byuU48mDNAZBuwiaRxwne3hxbG+W9KnyuLYqZuWVYlM\n1OXA5ZIeBK6Q9FnbV0m6y9kztWkULfDFxGTOc0q0cGNi3P1pwA7FgbyfWHOfhs6Q3Ni+QlEkfBTw\nEDF8ZHXgT0Rm9CWirWPSDRrrmaSViSzonUTrtu0IbfrLREa0oU3PCHU3abwvyjt7NaIO5V3bIxRN\nHPaQdJjtH5a/knUo00A76d0+DQyTtLPtS4DziYj1dZLmcPRM/iOwoKLVW9sXz/SU1Kb3HqlVnTQT\neVbfIqr/G9+dA9xDRPI/lg5101maWAPuLcGLa4D1bP8XuLIcfx34PpHBuq82S1uMpMGSNpG0VOXj\nSwjH+XpCl7q3pJHEO+iiol9NJkNq01tP1qH0Dm2lqS6px+WBG8vua1tClnABUchwNPA921fVaGa/\nILXpvUdqVSdNRUO9BjAvkQq+mXCir7F9gKR1CXkXwLO2j6vJ3LZF0g7ApsCywJ2O6WrV72cCpmv3\nDEqJoB5MrHkjgfMcA0kuAx6x/T3FRLqlgecc7cna9no0g9Smt5asQ+ld2kn+MYwYPjAe+Kak6W3/\npgS59gGWA/Yu6eFc5KZMatN7gdSqTp6Khvo4IiK6DjE4YxXgaknnAWsB25TvFqzL1naj+jwXWd0r\nxObuBkmz2v535bz3e1K38xpg+x5JOwFrAkcSWaQnibHOJ0lawvYTwBOVv9O216MZpDa9tWQdSu/S\nFk61pPmI6MGeth+WtBuwuqTx5WXwb+AN27el4zd5Upvee6RWdcoopql9HTjMpZuMpNuAA4iIy1yA\niEEbuzJhvG7SQyov43WJ7MBNRMbvS8DMkkbbfrnTnvuygbhR0iNEsOY7xMS/5YANiCLiZCpIbXrL\nyTqUXqJddJnjiXHNC5fjs4H/En18d7E92qUpfKe9AKaB1Kb3HqlVnQhlcwGA7f8R919Vl7oLkUmx\n7ZcJR29T4CsZyWoeJdV+NDG2eCviPryJmLK2DbBp2WR3JLZfsn2j7U2JFPolQEb6ukFq01tL1qHU\nR79cECsaoUGEju+N8vCtKemftu+XNIqYwHRrrcb2M2z/tkRH15X0TtGmz0JUX48vDk9Dm57VwT3A\n9g0ly7IbE7SqB5bvniOKQa+iA7SqAKVA5tXyPA8sGziAR4AzJK1l+3kiijWEKIx90/ZLkg4pm5Gk\nB1TW1oWBlYCf2L5YMbVzC+B7hAM5AHix8m/Ukah067F9uaQripShrZ/TJjEfkVmap7y7z7P9R0n7\nA19Nbfq0M4k6lNOAeyQdW+pQ1iTW1UeI7OhD9VncXvQ7p7pyw2wJ7EykIY8CriPm0x+raPG2JbCX\n7cdqNLffkdr01pNa1UmyBBG1/4jt1yXN4Bh7f4qkeYhsybWE9Og7rvRQTYe6OZS1dTOi0887wN+B\ni23fIuk9Yo39AXBwB9yPU6QqxbL9bvmz46/LlEhteuuoyLayDqUG+mX3D0mbEA/iVsSozc8CXwZu\nJ4q5PgbcZ/uW2ozsh5So6SXEZqShTV8ZuLk4f8NIbXpTmIhWdUNCqzoaGF1kDR1HucdOA1Zx9FOd\nqbGxKM7eC8AA23fnPdh8JH2M6BCwP/A6kYq/xPYR5ft1gH9lsCJpFpIWZII2fd7y+362f1mrYf2U\nUodyIXBBlzqUq4ksU7UO5Xhgx5TNNY9+E6muRKhnItK+exEO32KEgz2CiKBeRDgpydRT1aY/TGjT\nVyC06YNsn904MZ2ZaaeiVf0jsTFcuxy/C3wDmE7SiE5MrdseLWlvYsjNqi7tnRRt8zYBDmpE8vMe\nbC6S5mBCwZ2KrGZL4NLy/B+UgYqk2TgGhr1EFH9uRayJqU2fClSmHkPUoSiGtXWtQzmAUodSNjJZ\nh9IC+lWkWtKmhA5rZ2AQUcBwUNFQX0Gkj9ft1Cjf1DIRbfpbkoYDcwKXl+u6AbAvcZ0zOjWNdNGq\nbgD8t4tW9T9EFGF9Qqt6f33W1k/JRp1me6ik5YEbgW80Ii9Jc+ga7Ze0OPBd4D3i+j+u6FF/FbC5\n7b/UZGrSxjS06eX3AalN7x6TqkORtA+RbVrL9vOSPkPUQ2zZkM1JmjFlc82nTzvVkmYDprf9qqJx\n+QmEc3dvqWA9Dfgz8BjwVaJh/J21GdyPmJg2nWhn9Bqhm1wdqGrTr6/N2Dahq1bV9hbl87WIa/4W\nqVV9n+JYXwq8QTjUl+eLtnlU1oDNgI2I4rHDiYFOw4A5gF/afrREqrP7QpL0IYqzfDHwgTqU8t3h\nRIOBah1KDr5rMX3WqVZMmfspcCcTboo9gS0ck4CmJ3TUqxDVq8NtX1GXvf2R1Kb3HqlVnTZKpmRO\n25emQ918SoeAXxL35UZEt4UfE20M9wBmAA4kMivv1mVnkiQTJ+tQ+hZ90qkumtPfAGcAvys7sMHE\nIj838HPbD0qagUhTLmT72bxhpkwXbfrmRHX/gsB+RJHi4UzQpidNoGhVvwPsBGxaIn9DiSjsaNsH\n1WpgPyCf7eZQ5B0b2T6rHA8HFre9XzneC9gdWINwsN8qXRiSJOmjlADZqUDXOpQvUKlDSVpPnxv+\nUiQfpwBn2P6V7dfLV2sSvVHvBnaR9AlHu613bD8LWbjUHYpDvSkxGvtqYvrXN4lI/6nAbcBhimmK\nyTRS5EkA2H6DGPN+FdGmcGnbfyVGvm+lDw5ASCZCPttNY37gIEl7luMngdlK4RK2TwMeJRzt+9Kh\nTpK+j+1rgL0J/4hSh3IxMCYd6t6lzznVRMXqc0TUFABJuxAz67ciJv49DeyuGEqSTAFJs0mau/y+\nAjAcONr2W8ArwF+BtRRjYl8nmu9nsec0UtWqSjpJ0gVEGv084nrvIWlZ208Bn8zir6Q3KPflXUQv\n4H0k7QiMIVps7ShpTUmrEENf3pvMfypJkj5Gcaz3kvQ28VzvYfuyaoAnaT19yqku//izEov6WpXP\nBhHO9DrENLWngBOKU5hMhqJNvwDYU9KqRMP3jzOhneJAYne7PHAmMDKLPXtGcajXIIoSrwFeJIps\nBxL9Q038e8xMtDFMkpZT7sstiKLYhwmp1xaErG4w0eXneGB/24/XZWeSJNNGcaw3A/bMwu566Kua\n6j2A1YBTS6ePRoudNYCDgF1sv1KvlX2f1Kb3HqlVTfo6RdI1hkgT30MUeZ8MHGn7d5IGAAuWFly5\nBiRJPyaf4XroU5HqCpcS0b3dS/W/JK1NCPHPSId6yqQ2vddJrWrSJ6mkf98FngUetD2OGD50EXCG\npD1tv2v7ecg1IEn6O/kM10OfdKpt/4uIoPyZcKTPI+bYH1nSG8mUSW16L5Fa1aQvUnGm5wew/RoR\nrBhZjt8DngB+S8hBkiRJkh7QJ+UfVSQtQERYZsy0ZPcoL9M5gFuAA21fXT7bk4hMTUe8WI8HHrP9\nZG3GtgkVrer0wIrA9wnH+lCiZeFCwHHZSz3pTUqnn58QveevAUYTAYuPERNp9we2t317rq1JkiQ9\nY+CUT6kX2//ocpyL/hQo1+h1SacC20p6qWjTz6ho098EbkspTc8pWtUj+bBW9R3bw1OrmtRByY58\nkcigLAOsC8xtezdJuxLr/+62b4dcW5MkSXpKn5R/JE0jtektJLWqSV9F0jzAr4lJatcTnX3uBD5W\ndP8X2P6F7etqNDNJkqStSKe6jUltemtIrWrS1ykb5mOAYZK2tf1f4v68F1gOWKBO+5IkSdqRPq+p\nTppDatObS2pVk75EZeDQykQ9xVO2n5G0NXAE8EPbl0gaCMxv+4VaDU6SJGlD+rymOmkOqU1vHqlV\nTfoaxaHeBPgZcC7wW0nblIlq7wInSZrO9kggHeokSZIWkJHqJJkKilb1D8D9tneSNCPweWJY0ePA\n2bb/U6eNSedRBj1dAGwLLEnoqf8P2Nf26BKxftn2LfVZmSRJ0t6kpjpJpoLUqiZ9AUkLS1pW/YCz\nLAAAAklJREFU0mIAth8hWjouCBxteyHgLGCUpE/bvsz2LZV6gCRJkqTJpPwjSSbDJLSqF0gaBxwh\niaJVvQC4PrWqSauRtAwRlX4NeEnSKNsjbT8uaU2iywflz1uJQVBASpGSJElaSTrVSTIZUqua9CUq\nMo9vE3KjrYkMSYOngU0lnQysT0XbnyRJkrSWdKqTZDIUJ+ZHwCaEVvUd4FeS9rU9qgx2eblOG5OO\nYm5gRds3AUi6FjhZ0orAv2yPLTr/TxDTVNOhTpIk6SWyUDFJKkhamJB5/Nv2s+WzpYF5gZ/ZXkXS\nAcBRwMYV5ybb5iW9gqRhwOm2h0ragciiPAuMBx4Dzrc9ppyb92WSJEkvkZHqJCmkVjXpD5RuHntL\n+jfwqO35Jc0NzAocSNy/jXPzvkySJOklMlKdJExSqzqf7cPL9+sDexGTE9cntapJzUjaADjX9qJ1\n25IkSZKkU50kAEhaG7jZ9nTleEliQuLBhFb1BUkbE1rVB3PMe9IXKFKQc4Glbb82pfOTJEmS1pFO\ndZIUUqua9EckbQqMsz22bluSJEk6mdRUJ0khtapJf8T21ZAbvSRJkrrJSHWSdCG1qkmSJEmSTC05\npjxJumD7RmA3Sf+UNFfd9iRJkiRJ0vfJSHWSTILUqiZJkiRJ0l3SqU6SKZBa1SRJkiRJpkQ61UmS\nJEmSJEnSQ1JTnSRJkiRJkiQ9JJ3qJEmSJEmSJOkh6VQnSZIkSZIkSQ9JpzpJkiRJkiRJekg61UmS\nJEmSJEnSQ9KpTpIkSZIkSZIe8v9DanM+Uf/81gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc2e27c080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_results = {'RandomForest_WC': eval_RF_1_tts,\n",
    "               'RandomForest_TfIdf': eval_RF_2_tts,\n",
    "               'LogiReg_WC': eval_LR_1_tts,\n",
    "               'LogiReg_TfIdf': eval_LR_2_tts,\n",
    "               'GradBoost_WC': eval_GBC_1_tts,\n",
    "               'GradBoost_TfIdf': eval_GBC_2_tts,\n",
    "               'Voting_WC': eval_vot_1_tts,\n",
    "               'Voting_TfIdf': eval_vot_2_tts,\n",
    "              }\n",
    "\n",
    "import operator\n",
    "tup_results = sorted(dic_results.items(), key=operator.itemgetter(1))\n",
    "\n",
    "N = len(dic_results)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.40       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects = ax.bar(ind, list(zip(*tup_results))[1], width,)\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., \n",
    "            1.005*height, \n",
    "            '{0:.4f}'.format(height), \n",
    "            ha='center', \n",
    "            va='bottom',)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(ymin=0.78,ymax = 0.92)\n",
    "ax.set_title(\"Classificators' performance\")\n",
    "ax.set_xticks(ind + width/2.)\n",
    "ax.set_xticklabels(list(zip(*tup_results))[0], rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--\n",
    "\n",
    "All that remains is to run the best classifier on our test set and create a submission file. If you haven't already done so, download testData.tsv from the Data page. This file contains another 25,000 reviews and ids; our task is to predict the sentiment label.\n",
    "\n",
    "Note that when we use the Bag of Words for the test set, we only call \"transform\", not \"fit_transform\" as we did for the training set. In machine learning, you shouldn't use the test set to fit your model, otherwise you run the risk of overfitting. For this reason, we keep the test set off-limits until we are ready to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "id        25000 non-null object\n",
      "review    25000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the logistic regression to make sentiment label predictions\n",
    "result = clf_LR_2.predict(test_data_features2)\n",
    "result_prob = clf_LR_2.predict_proba(test_data_features2)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'LR_tfidf_model.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          1\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          1\n",
       "4   \"12128_7\"          1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Distributed Word Vectors\n",
    "--\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors  \n",
    "\n",
    "Introducing Distributed Word Vectors: This part of the tutorial will focus on using distributed word vectors created by the Word2Vec algorithm.\n",
    "\n",
    "Word2vec, published by Google in 2013, is a neural network implementation that learns distributed representations for words. Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. Word2vec learns quickly relative to other models.\n",
    "\n",
    "Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\"\n",
    "\n",
    "Distributed word vectors are powerful and can be used for many applications, particularly word prediction and translation. Here, we will try to apply them to sentiment analysis.\n",
    "\n",
    "Using word2vec in Python: In Python, we will use the excellent implementation of word2vec from the gensim package. If you don't already have gensim installed, you'll need to install it. There is an excellent tutorial that accompanies the Python Word2Vec implementation, here.\n",
    "\n",
    "Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is compute intensive. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). ln order to train your model in a reasonable amount of time, you will need to install cython (instructions here). Word2Vec will run without cython installed, but it will take days to run instead of minutes.\n",
    "\n",
    "Preparing to Train a Model\n",
    "\n",
    "First, we read in the data with pandas, as we did in Part 1. Unlike Part 1, we now use unlabeledTrain.tsv, which contains 50,000 additional reviews with no labels. When we built the Bag of Words model in Part 1, extra unlabeled training reviews were not useful. However, since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used.  \n",
    "\n",
    "The functions we write to clean the data are also similar to Part 1, although now there are a couple of differences. First, to train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. For this reason, we will make stop word removal optional in the functions below. It also might be better not to remove numbers, but we leave that as an exercise for the reader.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCleanReviews(reviews):\n",
    "    clean_reviews = []\n",
    "    for review in reviews[\"review\"]:\n",
    "        clean_reviews.append(KaggleWord2VecUtility.review_to_wordlist(review, remove_stopwords=False))\n",
    "    return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:198: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.happierabroad.com\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.archive.org/details/LovefromaStranger\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.loosechangeguide.com/LooseChangeGuide.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.msnbc.msn.com/id/4972055/site/newsweek/\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:198: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.youtube.com/watch?v=a0KSqelmgN8\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:207: UserWarning: \"b'http://jake-weird.blogspot.com/2007/08/beneath.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Split the labeled and unlabeled training sets into clean sentences\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "\n",
    "Training and Saving Your Model\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "\n",
    "Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n",
    "\n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = -1       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "model = Word2Vec(sentences,\n",
    "                 workers = num_workers,\n",
    "                 size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Optionally converting the model for Bigrams (to capture more context):\n",
    "bigram_transformer = gensim.models.Phrases(sentences)\n",
    "model = Word2Vec(bigram_transformer[sentences],\n",
    "                 workers = num_workers,\n",
    "                 size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It can be helpful to create a meaningful model name and save the model for later use. \n",
    "#You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_20minwords_10context\"\n",
    "model.save(os.path.join(outputs,model_name))\n",
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Model Results\n",
    "\n",
    "Congratulations on making it successfully through everything so far! Let's take a look at the model we created out of our 75,000 training reviews.\n",
    "\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n",
      "germany\n",
      "paris\n",
      "[('north', 0.23120811581611633), ('literally', 0.22275817394256592), ('exchange', 0.21795308589935303), ('hurl', 0.2158883512020111), ('heh', 0.21347549557685852), ('extremists', 0.21252413094043732), ('faints', 0.20652957260608673), ('adama', 0.2036851942539215), ('hugely', 0.1960485875606537), ('disrupted', 0.19528979063034058)]\n",
      "[('tolstoy', 0.251092791557312), ('fleshing', 0.22077207267284393), ('histories', 0.21041344106197357), ('cavern', 0.20596039295196533), ('idiocy', 0.2045907825231552), ('boyish', 0.19907249510288239), ('paxinou', 0.19874174892902374), ('bryant', 0.19759152829647064), ('doings', 0.19686591625213623), ('oh', 0.19670671224594116)]\n",
      "[('wikipedia', 0.23838505148887634), ('faith', 0.21923106908798218), ('motions', 0.21709153056144714), ('successors', 0.19928669929504395), ('entitled', 0.19770026206970215), ('harmful', 0.19639436900615692), ('weaknesses', 0.19632156193256378), ('satellite', 0.19572806358337402), ('survivor', 0.19540515542030334), ('flooded', 0.19298997521400452)]\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match(\"france england germany berlin\".split()))\n",
    "print(model.doesnt_match(\"paris berlin london austria\".split()))\n",
    "print(model.most_similar(\"man\"))\n",
    "print(model.most_similar(\"queen\"))\n",
    "print(model.most_similar(\"awful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have a reasonably good model for semantic meaning - at least as good as Bag of Words. But how can we use these fancy distributed word vectors for supervised learning? The next section takes a stab at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: More Fun With Word Vectors\n",
    "--\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors  \n",
    "\n",
    "Numeric Representations of Words\n",
    "\n",
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24156, 300)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02701798,  0.04598043,  0.01979387,  0.04681111, -0.00648649,\n",
       "       -0.0659649 ,  0.00305438, -0.06200789, -0.05690301,  0.06336904], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.index2word[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words To Paragraphs, \n",
    "--\n",
    "Attempt 1:  Vector Averaging  \n",
    "--\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from Part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 2000th review\n",
    "        if counter%2000. == 0.:\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        #Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the pickles from Part 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    clean_train_reviews, \n",
    "    clean_test_reviews = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(outputs, 'clean_reviews_sw.pkl'),'rb') as f:\n",
    "    clean_train_reviews_sw, \n",
    "    clean_test_reviews_sw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call these functions to create average vectors for each paragraph. The following operations will take a few minutes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, using the functions \n",
    "# we defined above. Notice that we now use stop word removal.\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the average paragraph vectors with the classifiers from Part 1.  \n",
    "Note that, as in Part 1, we can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvWV, X_testcvWV, y_traincvWV, y_testcvWV = train_test_split(trainDataVecs,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_WV = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_RF_WV_tts = clf_RF_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_RF_WV_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.593\n"
     ]
    }
   ],
   "source": [
    "clf_LR_WV = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=None,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_LR_WV_tts = clf_LR_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_LR_WV_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews[0], model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(testDataVecs).any()) #testando se no h valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "testDataVecs = Imputer().fit_transform(testDataVecs)\n",
    "\n",
    "print(np.isnan(testDataVecs).any()) #testando se no h valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = clf_RF_WV.predict(testDataVecs)\n",
    "result_prob = clf_RF_WV.predict_proba(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          1\n",
       "2    \"5828_4\"          0\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this produced results much better than chance, but underperformed Bag of Words by a few percentage points.\n",
    "\n",
    "Since the element-wise average of the vectors didn't produce spectacular results, perhaps we could do it in a more intelligent way? A standard way of weighting word vectors is to apply \"tf-idf\" weights, which measure how important a given word is within a given set of documents. One way to extract tf-idf weights in Python is by using scikit-learn's TfidfVectorizer, which has an interface similar to the CountVectorizer that we used in Part 1. However, when we tried weighting our word vectors in this way, we found no substantial improvement in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words to Paragraphs, Attempt 2: Clustering \n",
    "--\n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "In K-Means, the one parameter we need to set is \"K,\" or the number of clusters. How should we decide how many clusters to create? Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Clustering code is given below. We use scikit-learn to perform our K-Means.\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code took more than 40 minutes on my computer. Below, we set a timer around the K-Means function to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  3360.78120803833 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignment for each word is now stored in idx, and the vocabulary from our original Word2Vec model is still stored in model.index2word. For convenience, we zip these into one dictionary as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain. Your clusters may differ, as Word2Vec relies on a random number seed. Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n",
    "Run k-means on the word vectors and print a few clusters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['ideology', 'mainly', 'evolved', 'flint']\n",
      "\n",
      "Cluster 1\n",
      "['bang', 'purportedly', 'lit', 'conservatives']\n",
      "\n",
      "Cluster 2\n",
      "['forcefully', 'involves', 'meticulously', 'rawal', 'panther']\n",
      "\n",
      "Cluster 3\n",
      "['captivates', 'sales', 'dismiss', 'characterization']\n",
      "\n",
      "Cluster 4\n",
      "['unseen', 'shifting', 'theatrically']\n",
      "\n",
      "Cluster 5\n",
      "['trips', 'tagore', 'unbeknownst']\n",
      "\n",
      "Cluster 6\n",
      "['invisibility', 'tavern', 'dupe', 'courtship']\n",
      "\n",
      "Cluster 7\n",
      "['sleaze']\n",
      "\n",
      "Cluster 8\n",
      "['streets', 'spoof', 'les', 'squarely', 'splatter']\n",
      "\n",
      "Cluster 9\n",
      "['combo', 'flashdance', 'pushed']\n"
     ]
    }
   ],
   "source": [
    "# Print the first ten clusters\n",
    "for cluster in range(0,10):\n",
    "    # Print the cluster number\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        #print(len(word_centroid_map.values()))\n",
    "        #print(cluster)\n",
    "        #print(word_centroid_map.keys())\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are of varying quality. Some make sense, some cointain mostly names, and some contain related adjectives. On the other hand, some are a little mystifying. Perhaps our algorithm works best on adjectives.\n",
    "\n",
    "At any rate, now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    # The number of clusters is equal to the highest cluster index in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function above will give us a numpy array for each review, each with a number of features equal to the number of clusters. Finally, we create bags of centroids for our training and test set, then train a random forest and extract results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ****** Create bags of centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvCT, X_testcvCT, y_traincvCT, y_testcvCT = train_test_split(train_centroids,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "clf_RF_CT = RandomForestClassifier(n_estimators=100, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=True, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvCT, y_traincvCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.598\n"
     ]
    }
   ],
   "source": [
    "eval_RF_CT_tts = clf_RF_CT.score(X_testcvCT, y_testcvCT)\n",
    "print(eval_RF_CT_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote BagOfCentroids.csv\n"
     ]
    }
   ],
   "source": [
    "result = clf_RF_CT.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(os.path.join(outputs,\"BagOfCentroids.csv\"), index=False, quoting=3)\n",
    "print(\"Wrote BagOfCentroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the code above gives about the same (or slightly worse) results compared to the Bag of Words in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Comparing deep and non-deep learning methods\n",
    "--\n",
    "\n",
    "You may ask: Why is Bag of Words better?\n",
    "\n",
    "The biggest reason is, in our tutorial, averaging the vectors and using the centroids lose the order of words, making it very similar to the concept of Bag of Words. The fact that the performance is similar (within range of standard error) makes all three methods practically equivalent.  \n",
    "\n",
    "A few things to try:\n",
    "\n",
    "First, training Word2Vec on a lot more text should greatly improve performance. Google's results are based on word vectors that were learned out of more than a billion-word corpus; our labeled and unlabeled training sets together are only a measly 18 million words or so. Conveniently, Word2Vec provides functions to load any pre-trained model that is output by Google's original C tool, so it's also possible to train a model in C and then import it into Python.\n",
    "\n",
    "Second, in published literature, distributed word vector techniques have been shown to outperform Bag of Words models. In this paper, an algorithm called Paragraph Vector is used on the IMDB dataset to produce some of the most state-of-the-art results to date. In part, it does better than the approaches we try here because vector averaging and clustering lose the word order, whereas Paragraph Vectors preserves word order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Deep Learning?\n",
    "\n",
    "The term \"deep learning\" was coined in 2006, and refers to machine learning algorithms that have multiple non-linear layers and can learn feature hierarchies [1].\n",
    "\n",
    "Most modern machine learning relies on feature engineering or some level of domain knowledge to obtain good results. In deep learning systems, this is not the case -- instead, algorithms can automatically learn feature hierarchies, which represent objects in increasing levels of abstraction. Although the basic ingredients of many deep learning algorithms have been around for many years, they are currently increasing in popularity for many reasons, including advances in compute power, the falling cost of computing hardware, and advances in machine learning research.\n",
    "\n",
    "Deep learning algorithms can be categorized by their architecture (feed-forward, feed-back, or bi-directional) and training protocols (purely supervised, hybrid, or unsupervised) [2]. \n",
    "\n",
    "Some good background materials include:\n",
    "\n",
    "[1] \"Deep Learning for Signal and Information Processing\", by Li Deng and Dong Yu (out of Microsoft)\n",
    "\n",
    "[2] \"Deep Learning Tutorial\" (2013 Presentation by Yann LeCun and Marc'Aurelio Ranzato)\n",
    "\n",
    "Where Does Word2Vec Fit In?\n",
    "\n",
    "Word2Vec works in a way that is similar to deep approaches such as recurrent neural nets or deep neural nets, but it implements certain algorithms, such as hierarchical softmax, that make it computationally more efficient.  \n",
    "\n",
    "See Part 2 of this tutorial for more on Word2Vec, as well as this paper: Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "In this tutorial, we use a hybrid approach to training -- consisting of an unsupervised piece (Word2Vec) followed by supervised learning (the Random Forest). \n",
    "\n",
    "Libraries and Packages \n",
    "\n",
    "The lists below should in no way be considered exhaustive.\n",
    "\n",
    "In Python:\n",
    "\n",
    "Theano offers very low-level, nuts and bolts functionality for building deep learning systems. You can also find some good tutorials on their site.  \n",
    "Caffe is a deep learning framework out of the Berkeley Vision and Learning Center.  \n",
    "Pylearn2 wraps Theano and seems slightly more user friendly.  \n",
    "OverFeat was used to win the Kaggle Cats and Dogs competition.  \n",
    "\n",
    "\n",
    "More Tutorials  \n",
    "The O'Reilly Blog has a series of deep learning articles and tutorials:  \n",
    "\n",
    "http://radar.oreilly.com/2014/07/what-is-deep-learning-and-why-should-you-care.html  \n",
    "http://radar.oreilly.com/2014/07/how-to-build-and-run-your-first-deep-learning-network.html  \n",
    "Webcast: How to Get Started with Deep Learning in Computer Vision  \n",
    "There are several tutorials using Theano as well.  \n",
    "\n",
    "If you want to dive into the weeds of creating a neural network from scratch, check out Geoffrey Hinton's Coursera course.\n",
    "\n",
    "For NLP, check out this recent lecture at Stanford: http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/  \n",
    "\n",
    "This free, online book also introduces neural nets for deep learning: http://neuralnetworksanddeeplearning.com/  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#word2vec example\n",
    "# load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "sentences = word2vec.Text8Corpus('/tmp/text8')\n",
    "#train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=200)\n",
    "# ... and some hours later... just as advertised...\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "[('queen', 0.5359965)]\n",
    " \n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "model.save('/tmp/text8.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "model.save_word2vec_format('/tmp/text8.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "model = word2vec.Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "model.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
    "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)\n",
    "# which word doesn't go with the others?\n",
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "\n",
    "#http://rare-technologies.com/word2vec-tutorial/\n",
    "Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence\n",
    "\n",
    "For example, if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file, line by line:\n",
    ">>> class MySentences(object):\n",
    "...     def __init__(self, dirname):\n",
    "...         self.dirname = dirname\n",
    "... \n",
    "...     def __iter__(self):\n",
    "...         for fname in os.listdir(self.dirname):\n",
    "...             for line in open(os.path.join(self.dirname, fname)):\n",
    "...                 yield line.split()\n",
    ">>>\n",
    ">>> sentences = MySentences('/some/directory') # a memory-friendly iterator\n",
    ">>> model = gensim.models.Word2Vec(sentences)\n",
    "\n",
    "Say we want to further preprocess the words from the files  convert to unicode, lowercase, remove numbers, extract named entities All of this can be done inside the MySentences iterator and word2vec doesnt need to know. All that is required is that the input yields one sentence (list of utf8 words) after another.\n",
    "\n",
    "calling Word2Vec(sentences) will run two passes over the sentences iterator. The first pass collects words and their frequencies to build an internal dictionary tree structure.\n",
    "\n",
    "The second pass trains the neural model.\n",
    "\n",
    "These two passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and youre able to initialize the vocabulary some other way:\n",
    "\n",
    ">>> model = gensim.models.Word2Vec() # an empty model, no training\n",
    ">>> model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    ">>> model.train(other_sentences)  # can be a non-repeatable, 1-pass generator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
