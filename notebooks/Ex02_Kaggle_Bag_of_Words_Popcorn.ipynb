{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google's Word2Vec for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial  \n",
    "https://github.com/wendykan/DeepLearningMovies  \n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/  \n",
    "\n",
    "\n",
    "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.\n",
    "\n",
    "Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's charge the batteries for our analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K4200 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set  \n",
    "--\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "sampleSubmission - A comma-delimited sample submission file in the correct format.\n",
    "Data fields\n",
    "\n",
    "id - Unique ID of each review\n",
    "sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "review - Text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = \"../datasets/Kaggle/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(datapath, 'BOW_labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(os.path.join(datapath, 'BOW_testData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(os.path.join(datapath, \"BOW_unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} labeled train reviews, \\\n",
    "{} labeled test reviews, and \\\n",
    "{} unlabeled reviews\\n\".format(train[\"review\"].size,\n",
    "                               test[\"review\"].size,\n",
    "                               unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: For Beginners - Bag of Words\n",
    "--\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words  \n",
    "\n",
    "What is NLP?\n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. This page will help you get started with loading and cleaning the IMDB movie reviews, then applying a simple Bag of Words model to get surprisingly accurate predictions of whether a review is thumbs-up or thumbs-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text_Cleaning_Utilities(object):\n",
    "    \"\"\"Tools for processing text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_wordlist(text, \n",
    "                         remove_stopwords=False, \n",
    "                         remove_html=False, \n",
    "                         remove_non_letters=False, \n",
    "                         steeming=False):\n",
    "        '''Split a text into a list of words'''\n",
    "        #text = text.replace('-\\n','')\n",
    "        text = text.lower()\n",
    "        if remove_html:\n",
    "            text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "        if remove_non_letters:\n",
    "            text = re.sub(\"[^-A-Za-z0-9_]\", \" \", text)\n",
    "        list_words = word_tokenize(text)\n",
    "        list_words = [w.strip(string.punctuation) for w in list_words if w not in string.punctuation]\n",
    "        list_words = [w for w in list_words if len(w) > 1]\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            list_words = [w for w in list_words if w not in stops]\n",
    "        if steeming:\n",
    "            stemmer = PorterStemmer()\n",
    "            list_words = [stemmer.stem(item) for item in list_words]\n",
    "        return list_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_to_list_of_texts(dataframe, column, \n",
    "                            remove_stopwords=False, \n",
    "                            remove_html=False, \n",
    "                            remove_non_letters=False, \n",
    "                            steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(' '.join(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                                 remove_stopwords=remove_stopwords,\n",
    "                                                                                 remove_html=remove_html,\n",
    "                                                                                 remove_non_letters=remove_non_letters,\n",
    "                                                                                 steeming=steeming)))\n",
    "            \n",
    "        return clean_texts\n",
    "\n",
    "    @staticmethod\n",
    "    def df_to_list_of_tokens(dataframe, column, \n",
    "                             remove_stopwords=False, \n",
    "                             remove_html=False, \n",
    "                             remove_non_letters=False, \n",
    "                             steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                        remove_stopwords=remove_stopwords,\n",
    "                                                                        remove_html=remove_html,\n",
    "                                                                        remove_non_letters=remove_non_letters,\n",
    "                                                                        steeming=steeming))\n",
    "            \n",
    "        return clean_texts\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def text_to_sentences(text,\n",
    "                          remove_stopwords=False, \n",
    "                          remove_html=False, \n",
    "                          remove_non_letters=False, \n",
    "                          steeming=False,\n",
    "                          tokenizer=None,):\n",
    "        '''Split a text into parsed sentences. Returns a list of sentences, \n",
    "        where each sentence is a list of words'''\n",
    "        \n",
    "        # Load the punkt tokenizer (english) if no tokenizer passed\n",
    "        if not tokenizer:\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        raw_sentences = tokenizer.tokenize(text.strip())\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            if len(raw_sentence) > 0:\n",
    "                sentences.append(Text_Cleaning_Utilities.text_to_wordlist(raw_sentence, \n",
    "                                                                          remove_stopwords=remove_stopwords,\n",
    "                                                                          remove_html=remove_html,\n",
    "                                                                          remove_non_letters=remove_non_letters,\n",
    "                                                                          steeming=steeming))\n",
    "        return sentences\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_to_list_of_sentences(dataframe, column, \n",
    "                                remove_stopwords=False, \n",
    "                                remove_html=False, \n",
    "                                remove_non_letters=False, \n",
    "                                steeming=False,\n",
    "                                tokenizer=None):\n",
    "        sentences = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            sentences.append(Text_Cleaning_Utilities.text_to_sentences(dataframe[column][txt_id],\n",
    "                                                                       remove_stopwords=remove_stopwords,\n",
    "                                                                       remove_html=remove_html,\n",
    "                                                                       remove_non_letters=remove_non_letters,\n",
    "                                                                       steeming=steeming,\n",
    "                                                                       tokenizer=tokenizer))\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all the datasets and getting word lists\n",
    "--\n",
    "first set is without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(train, 'review', remove_stopwords=True)\n",
    "clean_test_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(test, 'review', remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched w'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally film main themes mortality nostalgia loss innocence perhaps surprising'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second set mantains stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(train, 'review')\n",
    "clean_test_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(test, 'review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with all this stuff going down at the moment with mj ve started listening to his'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally in film who main themes are of mortality nostalgia and loss of innocen'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'wb') as f:\n",
    "    pickle.dump((clean_train_reviews, \n",
    "                 clean_test_reviews,\n",
    "                 clean_train_reviews_sw, \n",
    "                 clean_test_reviews_sw),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    (clean_train_reviews, \n",
    "     clean_test_reviews,\n",
    "     clean_train_reviews_sw,\n",
    "     clean_test_reviews_sw) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Features from a Bag of Words (Using scikit-learn)\n",
    "--\n",
    "\n",
    "Now that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "Sentence 1: \"The cat sat on the hat\"  \n",
    "Sentence 2: \"The dog ate the cat and the hat\"  \n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Sentence 1: [ 2, 1, 1, 1, 1, 0, 0, 0 ]\n",
    "\n",
    "Similarly, the features for Sentence 2 are: [ 3, 1, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
    "\n",
    "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features.  \n",
    "We will test two strategies: CountVectorizer (term frequecies - TF) and TFIDF Vectorizer:  \n",
    "First we'll start with plain word counts (TF):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer_tf = CountVectorizer(input='content', \n",
    "                               encoding='utf-8', \n",
    "                               decode_error='strict', \n",
    "                               strip_accents=None, \n",
    "                               lowercase=True, \n",
    "                               preprocessor=None, \n",
    "                               tokenizer=None, \n",
    "                               stop_words=None, \n",
    "                               #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                               ngram_range=(1, 2),\n",
    "                               analyzer='word', \n",
    "                               max_df=1.0, \n",
    "                               min_df=1, \n",
    "                               max_features=5000, \n",
    "                               vocabulary=None, \n",
    "                               binary=False, \n",
    "                               dtype=np.int64,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "second, it transforms our training data into feature vectors. \n",
    "The input to fit_transform should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tf = vectorizer_tf.fit_transform(clean_train_reviews)\n",
    "train_data_features_tf = train_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tf = vectorizer_tf.fit_transform(clean_test_reviews)\n",
    "test_data_features_tf = test_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use TfIDf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(input='content',\n",
    "                                  #encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents=None,\n",
    "                                  lowercase=True,\n",
    "                                  preprocessor=None,\n",
    "                                  tokenizer=None,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words=None,\n",
    "                                  #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=5000,\n",
    "                                  vocabulary=None, \n",
    "                                  binary=False, \n",
    "                                  dtype=np.int64,\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_train_reviews_sw)\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_test_reviews_sw)\n",
    "test_data_features_tfidf = test_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_test_data_features.pkl'),'wb') as f:\n",
    "    pickle.dump((train_data_features_tf, \n",
    "                 test_data_features_tf,\n",
    "                 train_data_features_tfidf,\n",
    "                 test_data_features_tfidf),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_test_data_features.pkl'),'rb') as f:\n",
    "    (train_data_features_tf, \n",
    "    test_data_features_tf,\n",
    "    train_data_features_tfidf,\n",
    "    test_data_features_tfidf) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing Train set for Cross Validation  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://github.com/zygmuntz/classifying-text/blob/master/bow_validate.py  \n",
    "Alternatively, we can use the indexes to divide the train samples  \n",
    "\n",
    "train_i, test_i = train_test_split(np.arange(len(train)), train_size = 0.8, random_state = 44)  \n",
    "\n",
    "After generating indexes, we can divide ou datasets:  \n",
    "traincv = train_data_features1[train_i]  \n",
    "testcv = train_data_features1[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plain Word Counts\n",
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = train_test_split(train_data_features_tf,\n",
    "                                                                        train[\"sentiment\"],\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "X_traincv_tfidf, X_testcv_tfidf, y_traincv_tfidf, y_testcv_tfidf = train_test_split(train_data_features_tfidf,\n",
    "                                                                                    train[\"sentiment\"],\n",
    "                                                                                    test_size=0.2,\n",
    "                                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training some Classifiers  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning! Here, we'll use some classifiers implementations included in  the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_tf = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=0, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tf_tts = clf_RF_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_RF_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44666667,  0.55333333],\n",
       "       [ 0.36666667,  0.63333333],\n",
       "       [ 0.36333333,  0.63666667],\n",
       "       ..., \n",
       "       [ 0.32      ,  0.68      ],\n",
       "       [ 0.78      ,  0.22      ],\n",
       "       [ 0.20333333,  0.79666667]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tf.predict_proba(X_testcv_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train on the TfIdf samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_tfidf = RandomForestClassifier(n_estimators=300, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth=None, \n",
    "                                      min_samples_split=2, \n",
    "                                      min_samples_leaf=1, \n",
    "                                      min_weight_fraction_leaf=0.0, \n",
    "                                      max_features='auto', \n",
    "                                      max_leaf_nodes=None, \n",
    "                                      bootstrap=False, \n",
    "                                      oob_score=False, \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=0, \n",
    "                                      verbose=0, \n",
    "                                      warm_start=False, \n",
    "                                      class_weight=None).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tfidf_tts = clf_RF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_RF_tfidf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44333333,  0.55666667],\n",
       "       [ 0.32333333,  0.67666667],\n",
       "       [ 0.24      ,  0.76      ],\n",
       "       ..., \n",
       "       [ 0.27      ,  0.73      ],\n",
       "       [ 0.71      ,  0.29      ],\n",
       "       [ 0.31      ,  0.69      ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tfidf.predict_proba(X_testcv_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=0,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_tts = clf_LR_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_LR_tfidf = LR(penalty='l2',\n",
    "                  dual=False,\n",
    "                  tol=0.0001,\n",
    "                  C=1.0,\n",
    "                  fit_intercept=True,\n",
    "                  intercept_scaling=1,\n",
    "                  class_weight=None,\n",
    "                  random_state=0,\n",
    "                  solver='liblinear',\n",
    "                  max_iter=100,\n",
    "                  multi_class='ovr',\n",
    "                  verbose=0).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8926\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tfidf_tts = clf_LR_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_LR_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GBC_tf = GradientBoostingClassifier(loss='deviance',\n",
    "                                        learning_rate=0.1,\n",
    "                                        n_estimators=100,\n",
    "                                        subsample=1.0,\n",
    "                                        min_samples_split=2,\n",
    "                                        min_samples_leaf=1,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        max_depth=3,\n",
    "                                        init=None,\n",
    "                                        random_state=0,\n",
    "                                        max_features=None,\n",
    "                                        verbose=0,\n",
    "                                        max_leaf_nodes=None,\n",
    "                                        warm_start=False,\n",
    "                                        presort='auto').fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8082\n"
     ]
    }
   ],
   "source": [
    "eval_GBC_tf_tts = clf_GBC_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_GBC_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_GBC_tfidf = GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=100,\n",
    "                                           subsample=1.0,\n",
    "                                           min_samples_split=2,\n",
    "                                           min_samples_leaf=1,\n",
    "                                           min_weight_fraction_leaf=0.0,\n",
    "                                           max_depth=3,\n",
    "                                           init=None,\n",
    "                                           random_state=0,\n",
    "                                           max_features=None,\n",
    "                                           verbose=0,\n",
    "                                           max_leaf_nodes=None,\n",
    "                                           warm_start=False,\n",
    "                                           presort='auto').fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8132\n"
     ]
    }
   ],
   "source": [
    "eval_GBC_tfidf_tts = clf_GBC_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_GBC_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do some voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf_vot_tf = VotingClassifier(estimators=[('rf', clf_RF_tf),\n",
    "                                          ('lr', clf_LR_tf),\n",
    "                                          ('gbc', clf_GBC_tf)], voting='soft').fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8684\n"
     ]
    }
   ],
   "source": [
    "eval_vot_tf_tts = clf_vot_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_vot_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_vot_tfidf = VotingClassifier(estimators=[('rf', clf_RF_tfidf),\n",
    "                                             ('lr', clf_LR_tfidf),\n",
    "                                             ('gbc', clf_GBC_tfidf)], voting='soft').fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8798\n"
     ]
    }
   ],
   "source": [
    "eval_vot_tfidf_tts = clf_vot_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_vot_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'classifiers.pkl'),'wb') as f:\n",
    "    pickle.dump((clf_RF_tf, eval_RF_tf_tts,\n",
    "                 clf_RF_tfidf, eval_RF_tfidf_tts,\n",
    "                 clf_LR_tf, eval_LR_tf_tts,\n",
    "                 clf_LR_tfidf, eval_LR_tfidf_tts,                \n",
    "                 clf_GBC_tf, eval_GBC_tf_tts,\n",
    "                 clf_GBC_tfidf, eval_GBC_tfidf_tts,\n",
    "                 clf_vot_tf, eval_vot_tf_tts,\n",
    "                 clf_vot_tfidf, eval_vot_tfidf_tts),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the classifiers from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'classifiers.pkl'),'rb') as f:\n",
    "    (clf_RF_tf, eval_RF_tf_tts,\n",
    "     clf_RF_tfidf, eval_RF_tfidf_tts,\n",
    "     clf_LR_tf, eval_LR_tf_tts,\n",
    "     clf_LR_tfidf, eval_LR_tfidf_tts,                \n",
    "     clf_GBC_tf, eval_GBC_tf_tts,\n",
    "     clf_GBC_tfidf, eval_GBC_tfidf_tts,\n",
    "     clf_vot_tf, eval_vot_tf_tts,\n",
    "     clf_vot_tfidf, eval_vot_tfidf_tts) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAIyCAYAAAAE8jZRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2cf83094a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_results = {'RandomForest_TF': eval_RF_tf_tts,\n",
    "               'RandomForest_TFIDF': eval_RF_tfidf_tts,\n",
    "               'LogiReg_TF': eval_LR_tf_tts,\n",
    "               'LogiReg_TFIDF': eval_LR_tfidf_tts,\n",
    "               'GradBoost_TF': eval_GBC_tf_tts,\n",
    "               'GradBoost_TFIDF': eval_GBC_tfidf_tts,\n",
    "               'Voting_TF': eval_vot_tf_tts,\n",
    "               'Voting_TFIDF': eval_vot_tfidf_tts,\n",
    "              }\n",
    "\n",
    "import operator\n",
    "tup_results = sorted(dic_results.items(), key=operator.itemgetter(1))\n",
    "\n",
    "N = len(dic_results)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.40       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects = ax.bar(ind, list(zip(*tup_results))[1], width,)\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., \n",
    "            1.005*height, \n",
    "            '{0:.4f}'.format(height), \n",
    "            ha='center', \n",
    "            va='bottom',)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(ymin=0.78,ymax = 0.92)\n",
    "ax.set_title(\"Classificators' performance\")\n",
    "ax.set_xticks(ind + width/2.)\n",
    "ax.set_xticklabels(list(zip(*tup_results))[0], rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--\n",
    "\n",
    "All that remains is to run the best classifier on our test set and create a submission file. If you haven't already done so, download testData.tsv from the Data page. This file contains another 25,000 reviews and ids; our task is to predict the sentiment label.\n",
    "\n",
    "Note that when we use the Bag of Words for the test set, we only call \"transform\", not \"fit_transform\" as we did for the training set. In machine learning, you shouldn't use the test set to fit your model, otherwise you run the risk of overfitting. For this reason, we keep the test set off-limits until we are ready to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "id        25000 non-null object\n",
      "review    25000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the logistic regression with tfidf vectors to make sentiment label predictions\n",
    "result = clf_LR_tfidf.predict(test_data_features_tfidf)\n",
    "result_prob = clf_LR_tfidf.predict_proba(test_data_features_tfidf)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'LR_tfidf_model.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          0\n",
       "1    \"8348_2\"          1\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Alternative Vectors\n",
    "--\n",
    "\n",
    "In the subsequent sections, we are going to explore alternate ways to codify text into vectors. We are going to explore three techniques, namely Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA) and Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling and Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the models, the more we have texts, the better. The size of the Corpus is essential for having good results. We don't need labels in order to create the models, so we will use the train examples and also some unlabeled reviews. The list of cleaned sentences will be used for all the subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "sentences = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                         'review', \n",
    "                                                         remove_html=True,\n",
    "                                                         remove_stopwords=True,)\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "sentences += Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                          'review', \n",
    "                                                          remove_html=True,\n",
    "                                                          remove_stopwords=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving sentences to a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'sentences.pkl'),'wb') as f:\n",
    "    pickle.dump(sentences,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading sentences from a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'sentences.pkl'),'rb') as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=4, no_above=0.7):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    print('{} Tokens extracted from {} texts'.format(len(dictionary.keys()), dictionary.num_docs))\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    #stopword_ids = [dictionary.token2id[sw] for sw in stopwords if sw in dictionary.token2id]\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    #low_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < 4]\n",
    "    #dictionary.filter_tokens(low_freq_ids)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    print('{} Tokens after cleaning'.format(len(dictionary.keys())))\n",
    "    #print('Building corpus...')\n",
    "    #corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    return dictionary #, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compacting and saving the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "256791 Tokens extracted from 75000 texts\n",
      "56377 Tokens after cleaning\n",
      "dictionary done\n",
      "dictionary saved\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords=set(['n\\'t', 'movie'])\n",
    "\n",
    "dictionary = prep_corpus(sentences, additional_stopwords)\n",
    "dictionary.compactify()\n",
    "print('dictionary done')\n",
    "\n",
    "dictionary.save(os.path.join(outputs, 'reviews.dict'))\n",
    "print('dictionary saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22021\n"
     ]
    }
   ],
   "source": [
    "#print(dictionary.token2id['movie'])\n",
    "#print(dictionary.token2id['n\\'t'])\n",
    "print(dictionary.token2id['like'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.dictionary.Dictionary.load(os.path.join(outputs, 'reviews.dict'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Corpora (tf and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus tf done\n",
      "corpus tfidf done\n"
     ]
    }
   ],
   "source": [
    "corpus_tf = [dictionary.doc2bow(sentence) for sentence in sentences]\n",
    "print('corpus tf done')\n",
    "tfidf = models.TfidfModel(corpus_tf)\n",
    "corpus_tfidf = tfidf[corpus_tf]\n",
    "print('corpus tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tf.mm'), corpus_tf)\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tfidf.mm'), corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tf.mm'))\n",
    "corpus_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing\n",
    "https://en.wikipedia.org/wiki/Latent_semantic_analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lsi - TF done\n",
      "corpus lsi - TFIDF done\n"
     ]
    }
   ],
   "source": [
    "lsi_tf = models.LsiModel(corpus_tf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tf = lsi_tf[corpus_tf]\n",
    "print('corpus lsi - TF done')\n",
    "lsi_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tfidf = lsi_tfidf[corpus_tfidf]\n",
    "print('corpus lsi - TFIDF done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_tf.save(os.path.join(outputs, 'model_tf.lsi'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tf.mm'), corpus_lsi_tf)\n",
    "lsi_tfidf.save(os.path.join(outputs, 'model_tfidf.lsi'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tfidf.mm'), corpus_lsi_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_tf = models.LsiModel.load(os.path.join(outputs, 'model_tf.lsi'))\n",
    "corpus_lsi_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tf.mm'))\n",
    "lsi_tfidf = models.LsiModel.load(os.path.join(outputs, 'model_tfidf.lsi'))\n",
    "corpus_lsi_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.508*\"film\" + 0.290*\"one\" + 0.218*\"like\" + 0.147*\"would\" + 0.145*\"good\" + 0.137*\"even\" + 0.125*\"time\" + 0.123*\"really\" + 0.119*\"story\" + 0.117*\"see\"'),\n",
       " (1,\n",
       "  '-0.833*\"film\" + 0.214*\"one\" + 0.195*\"like\" + 0.113*\"show\" + 0.105*\"movies\" + 0.105*\"good\" + 0.101*\"would\" + 0.093*\"even\" + 0.092*\"really\" + 0.085*\"get\"'),\n",
       " (2,\n",
       "  '0.727*\"one\" + -0.456*\"like\" + -0.179*\"bad\" + -0.164*\"really\" + -0.152*\"good\" + -0.124*\"would\" + -0.096*\"people\" + -0.079*\"think\" + 0.076*\"two\" + 0.072*\"best\"'),\n",
       " (3,\n",
       "  '-0.493*\"one\" + -0.422*\"like\" + 0.331*\"story\" + -0.244*\"bad\" + 0.139*\"show\" + 0.138*\"great\" + 0.137*\"life\" + 0.137*\"also\" + 0.128*\"love\" + 0.119*\"character\"'),\n",
       " (4,\n",
       "  '-0.622*\"good\" + 0.409*\"like\" + -0.309*\"bad\" + -0.249*\"really\" + 0.157*\"would\" + 0.144*\"people\" + 0.131*\"show\" + -0.129*\"acting\" + -0.116*\"great\" + 0.116*\"life\"'),\n",
       " (5,\n",
       "  '0.524*\"like\" + -0.478*\"would\" + -0.234*\"bad\" + 0.226*\"story\" + -0.223*\"even\" + -0.195*\"could\" + 0.195*\"good\" + 0.183*\"great\" + 0.143*\"also\" + -0.100*\"get\"'),\n",
       " (6,\n",
       "  '-0.690*\"show\" + 0.389*\"story\" + -0.179*\"great\" + 0.173*\"even\" + -0.160*\"good\" + -0.147*\"series\" + 0.146*\"bad\" + -0.116*\"really\" + -0.103*\"episode\" + -0.090*\"shows\"'),\n",
       " (7,\n",
       "  '-0.676*\"would\" + -0.281*\"story\" + 0.225*\"bad\" + 0.200*\"get\" + -0.182*\"good\" + 0.170*\"time\" + -0.164*\"one\" + -0.156*\"like\" + 0.134*\"people\" + 0.132*\"really\"'),\n",
       " (8,\n",
       "  '-0.512*\"really\" + -0.342*\"people\" + -0.324*\"see\" + 0.262*\"even\" + -0.245*\"story\" + 0.202*\"good\" + -0.143*\"think\" + 0.135*\"time\" + 0.128*\"first\" + -0.121*\"love\"'),\n",
       " (9,\n",
       "  '-0.467*\"story\" + -0.389*\"show\" + 0.349*\"good\" + -0.250*\"even\" + -0.244*\"characters\" + 0.225*\"would\" + -0.190*\"bad\" + 0.171*\"see\" + 0.139*\"man\" + -0.116*\"character\"')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', -0.83285234036586853),\n",
       " ('one', 0.21358268067903974),\n",
       " ('like', 0.19496957046512681),\n",
       " ('show', 0.11342207206524174),\n",
       " ('movies', 0.10549817696234441),\n",
       " ('good', 0.10517230506037015),\n",
       " ('would', 0.10066706116248215),\n",
       " ('even', 0.092797106077930694),\n",
       " ('really', 0.091536863025829612),\n",
       " ('get', 0.08475376055672991)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.146*\"film\" + 0.103*\"bad\" + 0.101*\"good\" + 0.101*\"like\" + 0.101*\"really\" + 0.095*\"one\" + 0.094*\"would\" + 0.091*\"story\" + 0.089*\"see\" + 0.088*\"even\"'),\n",
       " (1,\n",
       "  '-0.335*\"bad\" + -0.198*\"worst\" + -0.132*\"waste\" + -0.131*\"movies\" + -0.129*\"acting\" + -0.114*\"ever\" + -0.114*\"terrible\" + -0.108*\"horrible\" + -0.105*\"horror\" + -0.104*\"stupid\"'),\n",
       " (2,\n",
       "  '-0.671*\"show\" + -0.213*\"series\" + -0.213*\"episode\" + 0.171*\"film\" + -0.152*\"episodes\" + -0.150*\"tv\" + 0.143*\"horror\" + -0.140*\"season\" + -0.125*\"funny\" + -0.113*\"shows\"'),\n",
       " (3,\n",
       "  '0.232*\"show\" + -0.223*\"great\" + -0.217*\"book\" + 0.179*\"horror\" + -0.117*\"movies\" + -0.110*\"love\" + -0.107*\"best\" + -0.107*\"seen\" + -0.106*\"read\" + 0.102*\"episode\"'),\n",
       " (4,\n",
       "  '-0.242*\"horror\" + -0.218*\"series\" + -0.167*\"action\" + 0.157*\"life\" + 0.156*\"people\" + -0.143*\"show\" + -0.131*\"effects\" + -0.129*\"great\" + -0.116*\"good\" + 0.115*\"book\"'),\n",
       " (5,\n",
       "  '-0.393*\"book\" + 0.366*\"funny\" + 0.247*\"comedy\" + -0.194*\"series\" + -0.160*\"read\" + 0.122*\"laugh\" + 0.117*\"jokes\" + 0.114*\"fun\" + -0.109*\"show\" + -0.109*\"episode\"'),\n",
       " (6,\n",
       "  '-0.410*\"horror\" + 0.235*\"bad\" + 0.142*\"worst\" + -0.128*\"scary\" + 0.118*\"script\" + -0.115*\"gore\" + 0.114*\"comedy\" + 0.113*\"show\" + -0.099*\"saw\" + 0.097*\"acting\"'),\n",
       " (7,\n",
       "  '-0.337*\"book\" + 0.250*\"ever\" + 0.228*\"worst\" + 0.201*\"seen\" + -0.156*\"characters\" + -0.139*\"funny\" + -0.135*\"really\" + 0.131*\"films\" + 0.125*\"horror\" + 0.123*\"dvd\"'),\n",
       " (8,\n",
       "  '0.501*\"book\" + -0.166*\"film\" + 0.156*\"read\" + 0.138*\"version\" + -0.133*\"show\" + -0.127*\"people\" + -0.118*\"characters\" + 0.111*\"bad\" + 0.096*\"original\" + 0.094*\"series\"'),\n",
       " (9,\n",
       "  '-0.360*\"horror\" + 0.316*\"game\" + 0.316*\"action\" + -0.192*\"show\" + -0.172*\"comedy\" + -0.160*\"book\" + -0.159*\"funny\" + -0.154*\"film\" + 0.123*\"series\" + 0.099*\"war\"')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', -0.33480885722659764),\n",
       " ('worst', -0.19764518911219711),\n",
       " ('waste', -0.13229255193966069),\n",
       " ('movies', -0.13067233145322313),\n",
       " ('acting', -0.12897121887748933),\n",
       " ('ever', -0.11449495146635832),\n",
       " ('terrible', -0.11417022402880846),\n",
       " ('horrible', -0.10792964772005968),\n",
       " ('horror', -0.10470726413032846),\n",
       " ('stupid', -0.10351850220693579)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lda tf done\n",
      "corpus lda tfidf done\n"
     ]
    }
   ],
   "source": [
    "lda_tf = models.LdaModel(corpus_tf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf = lda_tf[corpus_tf]\n",
    "print('corpus lda tf done')\n",
    "lda_tfidf = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tfidf = lda_tfidf[corpus_tfidf]\n",
    "print('corpus lda tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tf.save(os.path.join(outputs, 'model_tf.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf.mm'), corpus_lda_tf)\n",
    "lda_tfidf.save(os.path.join(outputs, 'model_tfidf.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tfidf.mm'), corpus_lda_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_tf = models.LdaModel.load(os.path.join(outputs, 'model_tf.lda'))\n",
    "corpus_lda_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf.mm'))\n",
    "lda_tfidf = models.LdaModel.load(os.path.join(outputs, 'model_tfidf.lda'))\n",
    "corpus_lda_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*one + 0.004*man + 0.004*police + 0.004*character + 0.004*scene + 0.004*gets + 0.004*murder'),\n",
       " (1,\n",
       "  '0.017*film + 0.015*horror + 0.011*one + 0.009*action + 0.008*films + 0.007*good + 0.006*also'),\n",
       " (2,\n",
       "  '0.015*like + 0.013*bad + 0.012*one + 0.010*even + 0.010*would + 0.009*could + 0.008*film'),\n",
       " (3,\n",
       "  '0.009*young + 0.009*girl + 0.008*family + 0.007*father + 0.007*mother + 0.007*love + 0.007*man'),\n",
       " (4,\n",
       "  '0.018*music + 0.015*game + 0.012*song + 0.010*songs + 0.009*musical + 0.008*animation + 0.007*cartoon'),\n",
       " (5,\n",
       "  '0.011*war + 0.006*american + 0.005*world + 0.005*one + 0.005*would + 0.004*years + 0.004*time'),\n",
       " (6,\n",
       "  '0.044*film + 0.012*story + 0.008*one + 0.007*characters + 0.007*much + 0.007*films + 0.006*like'),\n",
       " (7,\n",
       "  '0.022*film + 0.009*role + 0.008*one + 0.008*cast + 0.007*performance + 0.007*best + 0.006*john'),\n",
       " (8,\n",
       "  '0.012*life + 0.008*people + 0.008*film + 0.006*us + 0.006*one + 0.006*love + 0.005*world'),\n",
       " (9,\n",
       "  '0.015*one + 0.014*great + 0.013*good + 0.012*like + 0.012*show + 0.010*see + 0.010*really')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.print_topics(num_topics=10, num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('war', 0.010701545295125004),\n",
       " ('american', 0.0056249042519947072),\n",
       " ('world', 0.0053370536367931092),\n",
       " ('one', 0.0049164105639659377),\n",
       " ('would', 0.0046726790144998925),\n",
       " ('years', 0.0040095272409514573),\n",
       " ('time', 0.0039952758505885314),\n",
       " ('history', 0.0036423872129523428),\n",
       " ('western', 0.0034271387438728389),\n",
       " ('man', 0.0031394974029663814)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.show_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*khan + 0.004*bollywood + 0.003*india + 0.003*reno + 0.002*travolta + 0.002*frogs + 0.002*clerks'),\n",
       " (1,\n",
       "  '0.004*duvall + 0.003*tripe + 0.003*hannah + 0.002*races + 0.002*hawn + 0.002*arizona + 0.002*unsatisfying'),\n",
       " (2,\n",
       "  '0.003*karloff + 0.003*powell + 0.003*candyman + 0.002*chicago + 0.002*norris + 0.002*costello + 0.002*mel'),\n",
       " (3,\n",
       "  '0.002*wait + 0.002*martin + 0.001*joan + 0.001*news + 0.001*chinese + 0.001*hitchcock + 0.001*gangster'),\n",
       " (4,\n",
       "  '0.003*sandler + 0.002*quaid + 0.002*crying + 0.002*miike + 0.001*troma + 0.001*ringu + 0.001*frustrating'),\n",
       " (5,\n",
       "  '0.004*ninja + 0.004*holocaust + 0.003*mickey + 0.003*freddie + 0.002*crystal + 0.002*thru + 0.002*malkovich'),\n",
       " (6,\n",
       "  '0.002*laura + 0.002*highlander + 0.001*13th + 0.001*hall + 0.001*fields + 0.001*flashbacks + 0.001*ants'),\n",
       " (7,\n",
       "  '0.002*jaws + 0.002*tarantino + 0.002*chaney + 0.002*shark + 0.002*punk + 0.001*dinosaurs + 0.001*angus'),\n",
       " (8,\n",
       "  '0.004*bugs + 0.003*predator + 0.003*mitchum + 0.003*cannibal + 0.002*hellraiser + 0.002*pink + 0.002*holden'),\n",
       " (9,\n",
       "  '0.003*film + 0.002*good + 0.002*like + 0.002*bad + 0.002*one + 0.002*really + 0.002*story')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.print_topics(num_topics=10, num_words=7)\n",
    "#lda_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('khan', 0.0040491894834103854),\n",
       "   ('bollywood', 0.0036872642212237416),\n",
       "   ('india', 0.0026234314076322806),\n",
       "   ('reno', 0.0026113894491216153),\n",
       "   ('travolta', 0.0024844654108572327),\n",
       "   ('frogs', 0.0024834503443830467),\n",
       "   ('clerks', 0.0022440996943056488),\n",
       "   ('rebecca', 0.0021018064020160523),\n",
       "   ('regards', 0.0020361927673113122),\n",
       "   ('rani', 0.0019994605292414547),\n",
       "   ('avery', 0.0019957446465305607),\n",
       "   ('sundance', 0.0019863189103957249),\n",
       "   ('troy', 0.0019424356859702051),\n",
       "   ('symbolic', 0.0019269982475875666),\n",
       "   ('cate', 0.0019197203334367255),\n",
       "   ('laws', 0.0018679936761916165),\n",
       "   ('hindi', 0.0018200198192834666),\n",
       "   ('dundee', 0.0017733209561637034),\n",
       "   ('kinski', 0.0017717196199326453),\n",
       "   ('parade', 0.0017524507075075248)]),\n",
       " (1,\n",
       "  [('duvall', 0.0035042257512108914),\n",
       "   ('tripe', 0.0028271371199268917),\n",
       "   ('hannah', 0.002716546258834445),\n",
       "   ('races', 0.0024546152371154599),\n",
       "   ('hawn', 0.0024267048842195661),\n",
       "   ('arizona', 0.0023986071996406105),\n",
       "   ('unsatisfying', 0.002244576715255095),\n",
       "   ('goldie', 0.0021569087355186724),\n",
       "   ('bullock', 0.0021056446711520617),\n",
       "   ('leopard', 0.0020718821682413242),\n",
       "   ('lewton', 0.0019137371701563135),\n",
       "   ('delta', 0.0018349719182652344),\n",
       "   ('che', 0.0018136568485637883),\n",
       "   ('romeo', 0.0018102019269228861),\n",
       "   ('rathbone', 0.0017911861948272339),\n",
       "   ('melvin', 0.0017713988450850505),\n",
       "   ('sf', 0.0017710404577374573),\n",
       "   ('confuse', 0.0017268941136971193),\n",
       "   ('volcano', 0.001685104196009134),\n",
       "   ('woodstock', 0.001682765974454847)]),\n",
       " (2,\n",
       "  [('karloff', 0.0029673898353324499),\n",
       "   ('powell', 0.0027354325886781161),\n",
       "   ('candyman', 0.0025524065615085078),\n",
       "   ('chicago', 0.0022933160445343589),\n",
       "   ('norris', 0.0018578113827817148),\n",
       "   ('costello', 0.0017917485910974318),\n",
       "   ('mel', 0.0017348689924520837),\n",
       "   (\"o'hara\", 0.0016121014223668515),\n",
       "   ('gibson', 0.0015788615592827405),\n",
       "   ('boris', 0.001576655832237415),\n",
       "   ('wesley', 0.0015753302302270672),\n",
       "   ('ho', 0.0015412928227877634),\n",
       "   ('liners', 0.0015182179518799072),\n",
       "   ('hogan', 0.0013846963917732104),\n",
       "   ('morals', 0.0013628418406365368),\n",
       "   ('et', 0.001355932891591087),\n",
       "   ('ace', 0.0013412885555424488),\n",
       "   ('whiny', 0.0013053657626489776),\n",
       "   ('abysmal', 0.0012966574011883934),\n",
       "   ('reaches', 0.0012936484923176243)]),\n",
       " (3,\n",
       "  [('wait', 0.00169074841163729),\n",
       "   ('martin', 0.001610931876072953),\n",
       "   ('joan', 0.001389467201139236),\n",
       "   ('news', 0.0013153953302131596),\n",
       "   ('chinese', 0.0011971479266890733),\n",
       "   ('hitchcock', 0.0011921978487905336),\n",
       "   ('gangster', 0.0011912396142973616),\n",
       "   ('ii', 0.0011732043927175819),\n",
       "   ('developed', 0.001106864729914143),\n",
       "   ('bond', 0.0011029453289433308),\n",
       "   ('howard', 0.0010937759581079196),\n",
       "   ('worthy', 0.0010912438031471534),\n",
       "   ('vote', 0.0010633203031190622),\n",
       "   ('six', 0.0010616432771217354),\n",
       "   ('random', 0.0010549753047086391),\n",
       "   ('holmes', 0.0010411560318656242),\n",
       "   ('patrick', 0.0010405926279484607),\n",
       "   ('adds', 0.0010309245361138886),\n",
       "   ('merely', 0.0010127704552632315),\n",
       "   ('price', 0.0010032468554640967)]),\n",
       " (4,\n",
       "  [('sandler', 0.0033698084975340398),\n",
       "   ('quaid', 0.0020046290715678263),\n",
       "   ('crying', 0.0019050659341849814),\n",
       "   ('miike', 0.0016022605840620462),\n",
       "   ('troma', 0.0014122128683323242),\n",
       "   ('ringu', 0.0013781110266789475),\n",
       "   ('frustrating', 0.0013770781809739636),\n",
       "   ('orange', 0.0013197981390040041),\n",
       "   ('christie', 0.001299048211370173),\n",
       "   ('robots', 0.0012718145531004578),\n",
       "   ('listen', 0.0012653449265263613),\n",
       "   ('pryor', 0.0012542916275870581),\n",
       "   ('turkish', 0.0012175563548461126),\n",
       "   ('clown', 0.0012019927541018411),\n",
       "   ('unconvincing', 0.0011783113125852767),\n",
       "   ('rip', 0.0011579101306719972),\n",
       "   ('costner', 0.0011357104983122234),\n",
       "   ('strip', 0.0011302763621991489),\n",
       "   ('guests', 0.0011276489096435432),\n",
       "   ('hopkins', 0.0011232206104171709)]),\n",
       " (5,\n",
       "  [('ninja', 0.0042103787867654313),\n",
       "   ('holocaust', 0.0041385316240415016),\n",
       "   ('mickey', 0.0034331421787749637),\n",
       "   ('freddie', 0.0025647695700237853),\n",
       "   ('crystal', 0.0023247527639559122),\n",
       "   ('thru', 0.0022967652550701934),\n",
       "   ('malkovich', 0.0021967745247044464),\n",
       "   ('fellini', 0.0021728080545698738),\n",
       "   ('marx', 0.0020764707786756916),\n",
       "   ('intro', 0.0018775390981284356),\n",
       "   ('popularity', 0.0018687560932099069),\n",
       "   ('hercules', 0.0018401763434743982),\n",
       "   ('p.s', 0.0017964723059576363),\n",
       "   ('farrah', 0.001780444844778704),\n",
       "   ('apes', 0.0016883845335982526),\n",
       "   ('frozen', 0.0015735755720899499),\n",
       "   ('virginia', 0.001564881910525932),\n",
       "   ('nemo', 0.0015563407429477814),\n",
       "   ('fawcett', 0.001550474710714476),\n",
       "   ('ninjas', 0.0015265195604592344)]),\n",
       " (6,\n",
       "  [('laura', 0.0019744322203329957),\n",
       "   ('highlander', 0.0015782775906853523),\n",
       "   ('13th', 0.0014750303047849455),\n",
       "   ('hall', 0.0014127124187773396),\n",
       "   ('fields', 0.0012990398530478905),\n",
       "   ('flashbacks', 0.0012933689577470716),\n",
       "   ('ants', 0.0012846548371966509),\n",
       "   ('rose', 0.0012306993245234897),\n",
       "   ('bunny', 0.0012264227628102314),\n",
       "   ('inspiring', 0.0012099076503991105),\n",
       "   ('fly', 0.0011645013516262358),\n",
       "   ('70s', 0.0011415805826706846),\n",
       "   ('collins', 0.0011388789385788467),\n",
       "   ('bunuel', 0.0011382486758090699),\n",
       "   ('provoking', 0.0011333999334690399),\n",
       "   ('wealthy', 0.0011169080812715605),\n",
       "   ('nicole', 0.0011036691185578719),\n",
       "   ('bernie', 0.0010822391060664328),\n",
       "   ('cronenberg', 0.0010726076378428674),\n",
       "   ('hughes', 0.0010588719491308604)]),\n",
       " (7,\n",
       "  [('jaws', 0.0017830739927391297),\n",
       "   ('tarantino', 0.0017461755401981802),\n",
       "   ('chaney', 0.0017439184896061363),\n",
       "   ('shark', 0.0017051281312648109),\n",
       "   ('punk', 0.0015573887762224872),\n",
       "   ('dinosaurs', 0.0014881696075540506),\n",
       "   ('angus', 0.001336897098366916),\n",
       "   ('hudson', 0.0013332578601488977),\n",
       "   ('coppola', 0.0012399571149115517),\n",
       "   ('lance', 0.001211582315076578),\n",
       "   ('franco', 0.0011922112901374042),\n",
       "   ('mason', 0.0011910995751830617),\n",
       "   ('visitor', 0.0011656128742044782),\n",
       "   ('sharks', 0.0011311885194060867),\n",
       "   ('terminator', 0.0011252885086914918),\n",
       "   ('jenny', 0.0011110688370256203),\n",
       "   ('cringe', 0.0011062053037876433),\n",
       "   ('ranks', 0.0011053177473485626),\n",
       "   ('scifi', 0.0010821408803827859),\n",
       "   ('yesterday', 0.001081618976163541)]),\n",
       " (8,\n",
       "  [('bugs', 0.0037411024627148869),\n",
       "   ('predator', 0.0029645560938677058),\n",
       "   ('mitchum', 0.0026691674995903644),\n",
       "   ('cannibal', 0.0026363233115240287),\n",
       "   ('hellraiser', 0.0022609800683590672),\n",
       "   ('pink', 0.0020403513303500669),\n",
       "   ('holden', 0.0020275155098146698),\n",
       "   ('slater', 0.0019317374573940742),\n",
       "   ('panther', 0.0019113383994891202),\n",
       "   ('troll', 0.0018549981320299426),\n",
       "   ('hack', 0.0017790917541431626),\n",
       "   ('mins', 0.0017789543294952396),\n",
       "   ('rotten', 0.0017209025228781859),\n",
       "   ('stuart', 0.0017122849179546196),\n",
       "   ('lemmon', 0.0016749495051847651),\n",
       "   ('juliet', 0.0016507990117129324),\n",
       "   ('mimi', 0.0015923383784691787),\n",
       "   ('mysteries', 0.001577748994105942),\n",
       "   ('sh*t', 0.0015443245586902882),\n",
       "   ('fi', 0.0015436369066188281)]),\n",
       " (9,\n",
       "  [('film', 0.0030947761377360639),\n",
       "   ('good', 0.0020870368881301189),\n",
       "   ('like', 0.0020519891639777343),\n",
       "   ('bad', 0.0020204702112240503),\n",
       "   ('one', 0.0020030624472114665),\n",
       "   ('really', 0.0019676026131691254),\n",
       "   ('story', 0.0019119485163375176),\n",
       "   ('would', 0.001908554643940401),\n",
       "   ('great', 0.0018703685413589113),\n",
       "   ('see', 0.0018244545663361911),\n",
       "   ('even', 0.001785270874810161),\n",
       "   ('time', 0.0017520552062872305),\n",
       "   ('movies', 0.0017386576833569177),\n",
       "   ('people', 0.0017266009249213875),\n",
       "   ('could', 0.0016965964641748434),\n",
       "   ('well', 0.0016600201684655582),\n",
       "   ('watch', 0.0016081177181764409),\n",
       "   ('much', 0.0015906915139560992),\n",
       "   ('get', 0.001574160847750067),\n",
       "   ('show', 0.0015697500273855571)])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.show_topics(formatted=False, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing using PyLDAvis  \n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el233823139830939239192202009675\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el233823139830939239192202009675_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"tinfo\": {\"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.5146, -5.954, -6.088, -6.2359, -6.5431, -7.3818, -7.7421, -7.8101, -8.1163, -8.196, -8.2993, -8.3314, -8.5534, -8.6062, -8.6191, -8.6422, -8.7085, -8.7299, -8.7373, -8.885, -8.891, -8.9211, -8.9539, -9.0037, -9.0807, -9.1036, -9.1101, -9.1331, -9.1482, -9.1788, -6.0029, -6.0789, -4.3536, -6.207, -7.6794, -7.8482, -7.3029, -6.8915, -6.319, -7.8889, -8.279, -7.2297, -8.1126, -8.3238, -6.2809, -6.71, -6.0844, -7.427, -6.6186, -7.0537, -6.921, -5.7232, -6.2401, -5.3513, -6.7145, -5.9393, -5.4338, -4.758, -4.1691, -5.9816, -4.5765, -5.7437, -5.21, -4.5788, -4.9136, -5.2589, -5.6601, -4.9298, -5.0706, -5.3048, -5.6701, -4.8506, -5.4388, -5.2817, -5.2515, -4.4478, -5.462, -4.9957, -5.2525, -5.0302, -4.9263, -5.3069, -5.4498, -5.4531, -4.7875, -5.6069, -5.4602, -5.5394, -8.1003, -8.2005, -8.5308, -8.5443, -8.5661, -8.6176, -8.7735, -8.8346, -8.8362, -8.877, -8.8896, -8.9075, -8.9164, -8.954, -9.0259, -9.0376, -9.0442, -9.0456, -9.0789, -9.1669, -9.1676, -9.1886, -9.1948, -9.1964, -9.1976, -9.2035, -9.2449, -9.2967, -9.3059, -9.3145, -7.6804, -8.619, -7.7074, -7.9043, -7.8419, -7.847, -7.0811, -8.6782, -3.1225, -7.2185, -7.1273, -7.3918, -8.0386, -4.3918, -5.958, -7.691, -6.4085, -6.9844, -5.5736, -6.6405, -6.8054, -8.2602, -4.9424, -6.3024, -4.9921, -6.3804, -7.4558, -7.2368, -5.5529, -6.6745, -6.3097, -5.9688, -5.907, -5.7747, -4.9725, -6.1526, -5.7209, -5.2934, -6.6206, -5.5422, -5.7089, -5.1091, -5.4048, -5.4816, -6.361, -5.8681, -6.1999, -6.0411, -4.7708, -5.1878, -5.8278, -5.4329, -5.3271, -5.0771, -5.6513, -5.2871, -5.4783, -5.5677, -5.3865, -5.545, -5.477, -5.4874, -5.6949, -5.7343, -5.7119, -6.3462, -6.4935, -7.742, -7.859, -7.9076, -8.1266, -8.1488, -8.2109, -8.2563, -8.284, -8.2847, -8.3394, -8.615, -8.6352, -8.6619, -8.6667, -8.713, -8.729, -8.7493, -8.7644, -8.7752, -8.7758, -8.8668, -8.8732, -8.8738, -8.9033, -8.9055, -8.9197, -8.9332, -8.9958, -5.6803, -4.4523, -6.8844, -5.901, -7.0912, -5.2217, -5.1378, -4.867, -6.0861, -4.2484, -5.4405, -5.9409, -7.8806, -6.2466, -6.1299, -8.269, -6.0326, -6.5813, -5.3979, -5.7567, -5.6431, -6.462, -4.943, -6.0651, -4.3102, -5.0665, -4.6168, -5.0265, -4.6221, -4.9568, -5.6267, -5.9731, -4.2237, -4.8939, -4.4353, -4.9761, -5.1078, -5.2255, -4.8952, -5.3591, -5.1038, -4.9374, -5.4742, -5.2313, -5.3792, -5.4733, -5.413, -5.1498, -5.3858, -5.4461, -6.1906, -7.0761, -7.4896, -7.5303, -7.5447, -7.7085, -7.7415, -7.7741, -7.8125, -7.9122, -7.9338, -7.9365, -8.0529, -8.0896, -8.1237, -8.1609, -8.17, -8.1789, -8.1881, -8.1887, -8.1895, -8.191, -8.2187, -8.2633, -8.2718, -8.2735, -8.2758, -8.2818, -8.2824, -8.2916, -5.4793, -5.4437, -5.8823, -7.1771, -7.6481, -7.8208, -7.9692, -5.8709, -6.7998, -7.625, -7.7471, -6.6094, -7.9265, -6.907, -5.6855, -7.0483, -6.3386, -6.6113, -6.9531, -7.1501, -6.2627, -7.0246, -6.3642, -6.4298, -6.0375, -5.8765, -6.819, -6.8499, -6.0941, -6.8043, -6.2074, -6.3464, -5.4707, -6.0571, -5.7171, -6.2913, -5.8306, -6.2348, -6.594, -5.4248, -5.4595, -5.9244, -5.4469, -5.9268, -6.273, -5.629, -5.5215, -5.3391, -5.958, -6.0636, -6.2017, -5.9972, -6.1016, -6.1383, -6.166, -6.1946, -6.2038, -7.5291, -7.6097, -7.6874, -7.8365, -7.9193, -7.931, -7.9438, -7.9602, -7.99, -8.1144, -8.1255, -8.1349, -8.1842, -8.2803, -8.2886, -8.2893, -8.317, -8.3175, -8.3247, -8.3347, -8.3382, -8.3423, -8.3702, -8.4532, -8.513, -8.5155, -8.5314, -8.5332, -8.5495, -8.5712, -6.0535, -5.9736, -7.8422, -7.6387, -5.8022, -7.8767, -7.7713, -6.6895, -6.1072, -6.4714, -7.5029, -7.4115, -4.4261, -6.8084, -6.4509, -6.9947, -7.6283, -5.655, -6.3909, -5.7427, -6.9787, -7.0577, -6.093, -5.0857, -5.6329, -6.1459, -6.8294, -6.8957, -5.748, -7.0179, -5.265, -6.4631, -5.7763, -6.5262, -4.8148, -6.4466, -5.1484, -6.7143, -5.3168, -5.7508, -5.282, -5.5569, -6.29, -6.0433, -4.8822, -5.124, -5.9304, -5.8751, -5.7078, -5.9607, -5.7675, -6.0957, -5.9804, -6.0118, -6.0634, -6.0785, -5.5119, -6.1601, -6.2381, -6.2811, -6.5082, -6.5427, -6.6872, -6.7786, -7.0532, -7.0968, -7.1447, -7.1529, -7.1564, -7.1592, -7.1804, -7.1958, -7.1861, -7.2014, -7.218, -7.2289, -7.2352, -7.2468, -7.3337, -7.4922, -7.5027, -7.5381, -7.5867, -7.6571, -7.6658, -7.7546, -4.1673, -6.7468, -7.2168, -6.4909, -7.2045, -5.8527, -6.5823, -5.4535, -6.5994, -4.6976, -6.2708, -5.1926, -6.7435, -6.2672, -5.8783, -5.7719, -6.4305, -5.7663, -6.3601, -6.0608, -5.7626, -5.5793, -5.9131, -5.7469, -5.958, -4.8814, -5.8494, -5.637, -4.0472, -5.203, -4.4926, -5.4661, -5.5316, -4.8978, -5.1583, -5.7694, -5.4627, -5.5879, -5.4293, -5.7154, -5.689, -5.7206, -5.7366, -5.7551, -5.7985, -5.7767, -6.752, -6.9795, -7.3272, -7.314, -7.3611, -7.548, -7.6245, -7.6316, -7.7358, -7.7689, -7.9273, -7.9356, -7.9762, -8.0081, -8.0448, -8.0745, -8.0915, -8.1409, -8.2161, -8.2229, -8.239, -8.2458, -8.3039, -8.3057, -8.3345, -8.3498, -8.2763, -8.3674, -8.3877, -8.3941, -4.9156, -4.8931, -5.216, -5.6283, -6.4436, -5.3141, -6.0157, -5.8357, -7.0386, -6.9213, -5.5266, -6.3683, -5.256, -5.2785, -4.7224, -4.7737, -5.5593, -7.2132, -6.1634, -4.6832, -5.2865, -5.8377, -6.3657, -5.078, -6.4598, -5.6965, -5.3592, -6.277, -5.6824, -5.0318, -5.6044, -5.7151, -5.845, -4.9281, -5.016, -5.0938, -5.2835, -5.1609, -5.0989, -5.1258, -5.4823, -5.4884, -5.6093, -5.6428, -5.6473, -5.6594, -4.5374, -6.0192, -6.1108, -6.2863, -6.4839, -6.5799, -6.618, -6.7184, -6.7846, -6.822, -6.955, -7.1283, -7.1612, -7.2178, -7.2268, -7.2453, -7.2764, -7.2862, -7.3645, -7.376, -7.3781, -7.4977, -7.5276, -7.5356, -7.5546, -7.595, -7.5996, -7.6197, -7.6419, -7.6541, -6.7558, -6.189, -7.2883, -5.676, -7.3821, -6.2377, -6.1593, -7.1254, -6.4163, -6.2068, -6.8504, -6.6758, -6.7097, -5.8815, -6.1305, -6.4919, -6.7776, -5.8071, -6.4267, -5.1806, -6.09, -5.6151, -6.5963, -5.2331, -6.5394, -5.5191, -6.2708, -6.1564, -5.8967, -6.1315, -5.366, -5.83, -5.5226, -5.3152, -5.7637, -6.0874, -5.8284, -5.9127, -6.0353, -6.1144, -6.2003, -5.8901, -6.2216, -6.5219, -6.62, -6.7156, -6.9212, -6.9275, -6.9632, -6.9948, -7.0462, -7.1115, -7.1128, -7.2473, -7.3294, -7.3917, -7.393, -7.4367, -7.4372, -7.4446, -7.4578, -7.4868, -7.4881, -7.5051, -7.5061, -7.5246, -7.5305, -7.5424, -7.5817, -7.5923, -7.5972, -7.2128, -6.8988, -6.8727, -7.1134, -6.3577, -6.2582, -6.4368, -6.8095, -6.2925, -7.032, -5.9434, -7.185, -6.0726, -6.2785, -5.7069, -4.7631, -5.7701, -5.8453, -5.0893, -5.9193, -4.9675, -5.6998, -5.5624, -4.8815, -6.2473, -5.7851, -5.3768, -5.4799, -5.3833, -5.5256, -6.2626, -3.828, -5.4648, -4.9728, -5.538, -5.8693, -5.9311, -5.6006, -4.8274, -5.3203, -5.7789, -5.8956, -5.6808, -5.8651, -5.6065, -5.8219, -5.8881, -4.5972, -5.2077, -5.6598, -5.9599, -6.1185, -6.2205, -6.4685, -6.5451, -6.5494, -6.5567, -6.5854, -6.6068, -6.6141, -6.6269, -6.661, -6.6775, -6.6831, -6.7109, -6.7641, -6.8181, -6.8487, -6.8677, -6.9077, -6.9103, -6.9262, -6.9431, -6.9527, -6.9969, -7.028, -7.0402, -4.4361, -4.9856, -6.2928, -5.7356, -4.9909, -4.8484, -5.0571, -4.7202, -5.2599, -4.2085, -5.3857, -5.6434, -5.2523, -5.407, -5.7235, -5.6524, -5.96, -4.0265, -5.9752, -5.3219, -5.1539, -6.0632, -6.1213, -5.9172, -5.4316, -5.9985, -5.8591, -5.813, -5.8626, -5.7491, -5.833, -5.8645, -5.9063], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Term\": [\"film\", \"bad\", \"horror\", \"music\", \"life\", \"show\", \"great\", \"good\", \"really\", \"films\", \"funny\", \"love\", \"war\", \"people\", \"movies\", \"story\", \"would\", \"girl\", \"action\", \"watch\", \"family\", \"like\", \"think\", \"game\", \"best\", \"role\", \"young\", \"cast\", \"see\", \"mother\", \"worst\", \"stupid\", \"waste\", \"horrible\", \"crap\", \"garbage\", \"sucks\", \"sandler\", \"blah\", \"ninja\", \"ha\", \"suck\", \"1/10\", \"mst3k\", \"dinosaurs\", \"stinks\", \"kidding\", \"stinker\", \"highlander\", \"dinosaur\", \"2/10\", \"snipes\", \"tripe\", \"roswell\", \"scifi\", \"zohan\", \"dumber\", \"nope\", \"ripoff\", \"3000\", \"awful\", \"terrible\", \"bad\", \"worse\", \"redeeming\", \"rubbish\", \"yeah\", \"lame\", \"oh\", \"crappy\", \"junk\", \"pathetic\", \"idiot\", \"idiotic\", \"supposed\", \"ridiculous\", \"boring\", \"bother\", \"ok\", \"unless\", \"avoid\", \"minutes\", \"mean\", \"thing\", \"annoying\", \"money\", \"nothing\", \"could\", \"like\", \"someone\", \"even\", \"guy\", \"know\", \"would\", \"get\", \"acting\", \"ca\", \"people\", \"make\", \"ever\", \"want\", \"really\", \"say\", \"watch\", \"think\", \"one\", \"something\", \"see\", \"movies\", \"time\", \"good\", \"made\", \"seen\", \"plot\", \"film\", \"watching\", \"way\", \"better\", \"miike\", \"candyman\", \"asoka\", \"ali\", \"kurosawa\", \"fellini\", \"kidman\", \"kubrick\", \"nero\", \"apes\", \"akira\", \"tokyo\", \"hand-held\", \"hindi\", \"translated\", \"manga\", \"self-indulgent\", \"politicians\", \"saif\", \"singapore\", \"sanjay\", \"aladdin\", \"norwegian\", \"colours\", \"donnie\", \"shyamalan\", \"abstract\", \"dev\", \"amelie\", \"woven\", \"subtitles\", \"violin\", \"khan\", \"visually\", \"visuals\", \"lynch\", \"visual\", \"translation\", \"film\", \"images\", \"festival\", \"narrative\", \"storytelling\", \"story\", \"book\", \"artistic\", \"viewer\", \"development\", \"interesting\", \"cinematography\", \"elements\", \"imagery\", \"characters\", \"style\", \"films\", \"cinema\", \"detail\", \"focus\", \"director\", \"art\", \"camera\", \"different\", \"rather\", \"however\", \"much\", \"audience\", \"quite\", \"many\", \"stories\", \"scenes\", \"work\", \"well\", \"character\", \"plot\", \"perhaps\", \"seems\", \"main\", \"feel\", \"one\", \"would\", \"though\", \"also\", \"time\", \"like\", \"little\", \"good\", \"could\", \"way\", \"even\", \"first\", \"really\", \"see\", \"make\", \"made\", \"great\", \"episodes\", \"season\", \"seasons\", \"nikki\", \"aired\", \"snoopy\", \"porky\", \"travolta\", \"sopranos\", \"carrey\", \"daffy\", \"beowulf\", \"thurman\", \"uma\", \"hawn\", \"abc\", \"poirot\", \"stiller\", \"goldie\", \"paltrow\", \"nbc\", \"sebastian\", \"nikita\", \"mimi\", \"contestants\", \"canceled\", \"rangers\", \"wally\", \"sitcoms\", \"downside\", \"episode\", \"show\", \"comedies\", \"loved\", \"funniest\", \"series\", \"comedy\", \"funny\", \"favorite\", \"great\", \"tv\", \"humor\", \"quaid\", \"hilarious\", \"enjoyed\", \"sitcom\", \"liked\", \"television\", \"saw\", \"dvd\", \"fun\", \"jokes\", \"best\", \"remember\", \"good\", \"love\", \"see\", \"watch\", \"really\", \"movies\", \"always\", \"fan\", \"one\", \"well\", \"like\", \"first\", \"think\", \"seen\", \"time\", \"still\", \"also\", \"would\", \"years\", \"much\", \"many\", \"ever\", \"made\", \"film\", \"story\", \"get\", \"detective\", \"murderer\", \"pitt\", \"heist\", \"robbery\", \"karen\", \"duvall\", \"melanie\", \"niro\", \"sinbad\", \"hank\", \"seth\", \"keane\", \"farrow\", \"deniro\", \"leatherface\", \"dana\", \"laughton\", \"detectives\", \"reilly\", \"tina\", \"unsatisfying\", \"wakes\", \"sarandon\", \"palma\", \"las\", \"englund\", \"marple\", \"motel\", \"wilbur\", \"murder\", \"police\", \"cop\", \"mob\", \"bronson\", \"caine\", \"dealer\", \"crime\", \"cops\", \"attorney\", \"vegas\", \"boss\", \"allan\", \"sheriff\", \"car\", \"vincent\", \"gang\", \"chase\", \"criminal\", \"alex\", \"dr\", \"murdered\", \"joe\", \"prison\", \"david\", \"killer\", \"agent\", \"apartment\", \"robert\", \"serial\", \"jack\", \"thriller\", \"gets\", \"turns\", \"plays\", \"finds\", \"wife\", \"michael\", \"doctor\", \"man\", \"scene\", \"takes\", \"character\", \"goes\", \"killed\", \"two\", \"get\", \"one\", \"seems\", \"guy\", \"death\", \"also\", \"back\", \"around\", \"new\", \"played\", \"end\", \"lenny\", \"pammy\", \"homosexuality\", \"aids\", \"bernie\", \"prostitution\", \"underlying\", \"passionate\", \"compassion\", \"juliet\", \"midler\", \"swanson\", \"sensitivity\", \"affecting\", \"gail\", \"harriet\", \"crumb\", \"fianc\\u00e9e\", \"solondz\", \"dern\", \"conveys\", \"o'neill\", \"commitment\", \"nuanced\", \"rebellious\", \"psyche\", \"locusts\", \"immigrants\", \"harvard\", \"turmoil\", \"social\", \"society\", \"bunuel\", \"portrait\", \"gay\", \"desires\", \"morality\", \"relationships\", \"sexual\", \"issues\", \"sexuality\", \"scorsese\", \"life\", \"moral\", \"culture\", \"deeply\", \"struggles\", \"human\", \"emotional\", \"lives\", \"community\", \"passion\", \"relationship\", \"us\", \"women\", \"reality\", \"emotions\", \"feelings\", \"sex\", \"tragedy\", \"world\", \"powerful\", \"men\", \"personal\", \"people\", \"message\", \"love\", \"portrayal\", \"man\", \"yet\", \"character\", \"real\", \"violence\", \"true\", \"film\", \"one\", \"may\", \"every\", \"way\", \"young\", \"characters\", \"shows\", \"makes\", \"many\", \"also\", \"never\", \"gore\", \"zombie\", \"vampire\", \"slasher\", \"martial\", \"dracula\", \"gory\", \"kong\", \"vampires\", \"hong\", \"fu\", \"scares\", \"seagal\", \"cannibal\", \"karloff\", \"lugosi\", \"halloween\", \"chainsaw\", \"chaney\", \"kung\", \"chan\", \"werewolf\", \"13th\", \"costello\", \"argento\", \"giallo\", \"ringu\", \"li\", \"sadako\", \"damme\", \"horror\", \"massacre\", \"predator\", \"zombies\", \"demon\", \"monster\", \"creature\", \"blood\", \"arts\", \"action\", \"creepy\", \"effects\", \"monsters\", \"nudity\", \"scary\", \"genre\", \"cult\", \"evil\", \"alien\", \"sequel\", \"fans\", \"dead\", \"flick\", \"budget\", \"fight\", \"films\", \"killer\", \"special\", \"film\", \"scenes\", \"one\", \"pretty\", \"original\", \"good\", \"also\", \"fun\", \"well\", \"movies\", \"like\", \"plot\", \"great\", \"much\", \"first\", \"really\", \"still\", \"even\", \"marry\", \"bambi\", \"grandfather\", \"sons\", \"daughters\", \"streep\", \"lana\", \"visitor\", \"mothers\", \"walsh\", \"meryl\", \"marries\", \"boxing\", \"lombard\", \"leopard\", \"fishing\", \"shelter\", \"ladd\", \"squid\", \"cinderella\", \"harding\", \"connie\", \"housekeeper\", \"salman\", \"peanuts\", \"marrying\", \"melody\", \"leonardo\", \"valet\", \"orphan\", \"mother\", \"father\", \"son\", \"parents\", \"mom\", \"daughter\", \"married\", \"sister\", \"aunt\", \"cary\", \"husband\", \"dad\", \"boy\", \"children\", \"girl\", \"family\", \"child\", \"farm\", \"baby\", \"young\", \"home\", \"brother\", \"grant\", \"woman\", \"charlie\", \"town\", \"wife\", \"meets\", \"school\", \"old\", \"house\", \"friend\", \"wants\", \"love\", \"man\", \"life\", \"back\", \"little\", \"story\", \"one\", \"two\", \"get\", \"go\", \"find\", \"years\", \"also\", \"war\", \"government\", \"military\", \"president\", \"holmes\", \"powell\", \"flynn\", \"united\", \"bogart\", \"propaganda\", \"westerns\", \"wwii\", \"errol\", \"revolution\", \"civil\", \"watson\", \"iraq\", \"union\", \"sherlock\", \"norma\", \"nazis\", \"commander\", \"humphrey\", \"loy\", \"che\", \"jews\", \"hitler\", \"soviet\", \"germans\", \"communist\", \"nazi\", \"soldiers\", \"terrorists\", \"western\", \"navy\", \"army\", \"ship\", \"terrorist\", \"americans\", \"german\", \"soldier\", \"u.s\", \"cagney\", \"america\", \"political\", \"states\", \"enemy\", \"country\", \"captain\", \"american\", \"battle\", \"history\", \"russian\", \"world\", \"west\", \"years\", \"earth\", \"power\", \"men\", \"white\", \"would\", \"us\", \"time\", \"one\", \"man\", \"black\", \"also\", \"made\", \"people\", \"many\", \"back\", \"joan\", \"crawford\", \"chaplin\", \"arthur\", \"kirk\", \"carol\", \"connery\", \"welles\", \"harold\", \"hardy\", \"barrymore\", \"robinson\", \"judy\", \"stan\", \"hughes\", \"andre\", \"dorothy\", \"sharpe\", \"o'hara\", \"everett\", \"cassavetes\", \"burton\", \"orson\", \"troy\", \"lancaster\", \"enterprise\", \"fairbanks\", \"orphanage\", \"mae\", \"panther\", \"stanwyck\", \"gregory\", \"walter\", \"neil\", \"douglas\", \"howard\", \"ms\", \"penn\", \"jane\", \"brooks\", \"william\", \"francis\", \"oscar\", \"match\", \"novel\", \"role\", \"supporting\", \"george\", \"john\", \"richard\", \"performance\", \"career\", \"mr\", \"cast\", \"charles\", \"roles\", \"actor\", \"star\", \"play\", \"version\", \"silent\", \"film\", \"screen\", \"best\", \"plays\", \"fine\", \"actress\", \"played\", \"one\", \"films\", \"performances\", \"picture\", \"director\", \"production\", \"great\", \"script\", \"hollywood\", \"songs\", \"animated\", \"cartoons\", \"batman\", \"concert\", \"superman\", \"tunes\", \"bunny\", \"coppola\", \"ants\", \"columbo\", \"ginger\", \"lyrics\", \"tarzan\", \"album\", \"musicals\", \"voiced\", \"pirates\", \"musicians\", \"cabaret\", \"sung\", \"bachchan\", \"tucker\", \"pierce\", \"looney\", \"gremlins\", \"avery\", \"ballet\", \"turtles\", \"amitabh\", \"song\", \"cartoon\", \"rogers\", \"sing\", \"dance\", \"animation\", \"disney\", \"musical\", \"singing\", \"game\", \"dancing\", \"numbers\", \"robin\", \"band\", \"singer\", \"games\", \"robot\", \"music\", \"bugs\", \"rock\", \"voice\", \"kelly\", \"hood\", \"prince\", \"play\", \"bond\", \"stage\", \"number\", \"soundtrack\", \"version\", \"playing\", \"best\", \"original\"], \"Total\": [117745.0, 26861.0, 10140.0, 9054.0, 17458.0, 16975.0, 27161.0, 43136.0, 34579.0, 20778.0, 13013.0, 18484.0, 5896.0, 27622.0, 22357.0, 34421.0, 39707.0, 8292.0, 10310.0, 19711.0, 8074.0, 58689.0, 20974.0, 4124.0, 18912.0, 9375.0, 10683.0, 11243.0, 33666.0, 4898.0, 7627.133922936171, 4915.577821270011, 4299.07837349378, 3708.1510488936237, 2727.6133458227678, 1179.6240280040706, 822.9758175565994, 768.9684502313129, 566.3903100873541, 523.0334241351376, 471.81861763851197, 456.9237294974454, 366.14962419262986, 347.3543594266225, 342.9037423352794, 335.1075725986167, 313.6546334848857, 307.0430952389179, 304.7866298372735, 263.0484738658, 261.49001890854925, 253.7633562997999, 245.61471902084836, 233.71597292560634, 216.4647099676536, 211.58333402371187, 210.21604999980443, 205.46159194628157, 202.3945386290203, 196.32524106491766, 4758.189223350363, 4503.703487531886, 26861.92005761758, 4082.978279390816, 906.6198796732028, 762.7144200313247, 1342.915559605625, 2057.308418209972, 3821.6530397328797, 747.2428145553533, 495.83540277306514, 1501.3428743137376, 592.0469236954679, 474.00843838785295, 4300.232721955533, 2697.38497193976, 5440.242037251871, 1257.827985741691, 3058.851151635402, 1896.838135230792, 2218.557327499988, 8522.644145242715, 4804.829151600571, 13214.261108691717, 2807.7114818561813, 6777.359873960543, 12597.777364887635, 28389.8891793681, 58689.48910743428, 6783.562681980012, 36771.90110164259, 9176.965430390108, 18049.47098440958, 39707.30688437456, 27427.721945805093, 17851.314583288105, 10699.863456857353, 27622.028612549464, 23676.094375932644, 17410.928973104044, 10655.650430091438, 34579.674077799966, 15318.460665596514, 19711.963186727542, 20974.880782558455, 77493.31835447827, 15144.99055011258, 33666.9436287458, 22357.888531624, 35130.68311283235, 43136.095532703264, 23101.588643468956, 19119.795107578277, 19037.857156692582, 117745.79365324546, 13134.604277813469, 23394.6432249273, 17139.320093339622, 455.8490574469823, 412.4658238633857, 296.68588250778834, 292.7262063287231, 286.4261624655435, 272.094952526435, 232.95548925587516, 219.19677120745777, 218.8423129291787, 210.14119845182913, 207.50411974597657, 203.84457025441097, 202.04610573701478, 194.63167610694802, 181.17768178498724, 179.09068626234966, 177.92135585042467, 177.67687616754316, 171.88774660959874, 157.4707870433287, 157.37469747106107, 154.11534146284282, 153.16242218636944, 152.91819860956946, 152.74826626454367, 151.85145580080479, 145.73269253219212, 138.41195273018363, 137.15960799255464, 135.9918923788321, 699.7561050592453, 276.88892011556794, 726.6230156855303, 594.2139523745982, 639.3839691989979, 647.490850800113, 1484.0819681720723, 267.26207204674625, 117745.79365324546, 1323.587524933805, 1470.317662809424, 1116.1086022877005, 544.4248606326576, 34421.77290596926, 5810.717333174892, 816.7478673495758, 3580.138203315342, 1849.7348770301599, 9457.038902036607, 2805.38075136832, 2334.6838859843388, 431.17281364426066, 21544.35230456205, 4350.195688791696, 20778.212312996868, 3982.7627395574373, 1116.857404563854, 1479.4486270325933, 12027.80008677036, 2990.224579030614, 4737.521933605197, 7479.127440738896, 8195.181492066433, 9779.928292877914, 28359.275257365265, 6180.848418403293, 11049.80228069997, 19825.80367675944, 3325.5652136034473, 15169.341316967704, 12111.9977158354, 28613.959006558824, 20430.467031463115, 19037.857156692582, 4977.295550752035, 10984.0383281617, 6599.306948234349, 8665.145057153106, 77493.31835447827, 39707.30688437456, 12995.696951625008, 27057.764904001844, 35130.68311283235, 58689.48910743428, 19289.54066354477, 43136.095532703264, 28389.8891793681, 23394.6432249273, 36771.90110164259, 25712.554875977577, 34579.674077799966, 33666.9436287458, 23676.094375932644, 23101.588643468956, 27161.997717498532, 2014.9182241773394, 1739.1488959851335, 499.6348190538966, 444.59567519799134, 423.6866943834374, 340.4178618880747, 332.97155891757126, 312.971605383824, 299.12733775389177, 290.97929690287765, 290.7556834684614, 275.3361787192413, 209.22544136553083, 205.055661201056, 199.67893261371557, 198.73513133108358, 189.77703736599983, 186.78300418817702, 183.0418146382112, 180.31657289083677, 178.3797532061755, 178.28734477298678, 162.8443243565528, 161.8224057824457, 161.71596531346702, 157.0502670221878, 156.7093459231376, 154.50012944140596, 152.4484727499171, 143.25483059623343, 4177.828799665775, 16975.382101023555, 1351.3268415209118, 3966.296033299094, 1103.398470192915, 8488.680488233824, 9387.888016546167, 13013.143314339488, 3482.3541366145414, 27161.997717498532, 7263.252259753411, 4266.6943038071695, 483.9560577570268, 3057.061937305835, 3708.503959373016, 315.20930557673904, 4382.313848233619, 2322.7833602091673, 9637.759833731552, 6762.971272699962, 7796.930467677238, 2805.1373032550196, 18912.370093593814, 4671.371280176237, 43136.095532703264, 18484.82672607008, 33666.9436287458, 19711.963186727542, 34579.674077799966, 22357.888531624, 9084.798308783671, 5678.817213131663, 77493.31835447827, 28613.959006558824, 58689.48910743428, 25712.554875977577, 20974.880782558455, 19119.795107578277, 35130.68311283235, 15796.408237689906, 27057.764904001844, 39707.30688437456, 13048.131681197134, 28359.275257365265, 19825.80367675944, 17410.928973104044, 23101.588643468956, 117745.79365324546, 34421.77290596926, 27427.721945805093, 1491.9178708613545, 615.946684909849, 407.6406063109489, 391.41454168690166, 385.8456023612822, 327.695255728999, 317.08630307227503, 306.9352112171797, 295.41627250469986, 267.4735622478077, 261.77895746329256, 261.0560430634683, 232.47185443521442, 224.13077171747815, 216.65792441338172, 208.77168144256868, 206.8933046004157, 205.0585960163242, 203.18860423173516, 203.07057333733425, 202.9025990626066, 202.61166984749232, 197.10426772240262, 188.53130972464908, 186.94381659893716, 186.64206219235325, 186.20376561954387, 185.10305882692333, 184.9841543467419, 183.293445368926, 3150.375307666431, 3320.9451024027007, 2150.13967243947, 579.0187379518246, 357.99764345701556, 300.8774494278179, 257.588937232324, 2436.106756755208, 901.1576550313772, 372.4718616349997, 327.06681904168204, 1159.7558318505903, 271.27789267053964, 852.2264752378788, 3408.4657981295113, 732.7863604768604, 1696.8276566424397, 1287.4522732372761, 859.3522033521834, 686.5131923097446, 2084.7604600281866, 807.5858285635426, 1889.9246413054166, 1740.0238955370553, 3047.442507893403, 4117.5643858001185, 1133.2466094565716, 1088.8800387946294, 3138.4512251784317, 1170.8765843942576, 2908.1756692470935, 2382.000981994119, 9237.183857381444, 3853.7212529463795, 6821.369569776687, 2831.40449432366, 6275.073229565182, 3238.780806953825, 1713.6054964011823, 16254.293567514213, 16268.296673617273, 6889.990228258829, 20430.467031463115, 7323.422750355562, 3707.3680541242793, 20123.126839427387, 27427.721945805093, 77493.31835447827, 10984.0383281617, 9176.965430390108, 5928.919961411299, 27057.764904001844, 14216.693373840762, 10682.356463187862, 12569.838284286221, 7875.652216217256, 15576.264708004876, 383.2933200684901, 353.6557212945571, 327.309034121429, 282.0875439407088, 259.73546656795725, 256.7351584059775, 253.47734932618025, 249.37309570443313, 242.0824659682175, 213.86659270712673, 211.52132823146002, 209.53308436960785, 199.49945757014805, 181.31016992524744, 179.82068370849052, 179.69760479072232, 174.80202469093464, 174.7244377647912, 173.4680876490711, 171.75932000778724, 171.15766400153578, 170.46559449199023, 165.7877970433754, 152.6602478017073, 143.853252764198, 143.49923146413727, 141.24740703506788, 140.99111814050144, 138.72635522130048, 135.77348612613338, 1727.4853138475632, 1965.0235384326038, 288.7281379696567, 357.9980828980903, 2543.5002910320313, 283.24148156863555, 317.68263928794704, 1035.9077052193948, 2046.60533641403, 1376.4868154937458, 433.4539590362776, 481.89298834400086, 17458.067185452073, 1031.8195612135717, 1626.233162635542, 844.7411325390765, 395.6246845110538, 4320.600101682066, 1843.1937841272188, 4205.930092662375, 905.804018534691, 821.6719304141221, 2844.177709913754, 10809.280604513951, 5364.037540949834, 2731.694387349286, 1124.8008381580514, 1058.0589282933306, 5213.451920188191, 901.7643402419355, 11124.172824581265, 2035.2453180557811, 5710.301354791885, 1943.630341816599, 27622.028612549464, 2258.6107379646537, 18484.82672607008, 1505.179455908361, 16254.293567514213, 8510.649833198628, 20430.467031463115, 14049.978947861013, 3398.0791979242176, 6282.103745346227, 117745.79365324546, 77493.31835447827, 9891.407713607328, 11756.532585664172, 23394.6432249273, 10683.431861242114, 21544.35230456205, 7109.398580396192, 13410.825252721419, 19825.80367675944, 27057.764904001844, 19222.97320531525, 2631.4811632772407, 1376.6594779994925, 1273.4509234069308, 1219.7763904603014, 972.201329426313, 939.2678946813398, 812.9953037810801, 742.0466461896002, 564.1087772321557, 540.091553984522, 514.8497911888645, 510.6404717538185, 508.85398317106615, 507.454588751972, 496.8042379951075, 489.23278557042903, 494.04024494679686, 486.52640635410285, 478.5478728103201, 473.33488824228544, 470.37762766524713, 464.96724398106414, 426.33224121562273, 363.96675891416436, 360.18767648469736, 347.71018650433047, 331.24543596242967, 308.80119835465854, 306.1103210505612, 280.18831710535636, 10140.025744365403, 793.3157658965699, 488.1223513847684, 1044.538668927289, 494.6930923139509, 2089.6100972392846, 964.1698661868903, 3267.906893964592, 979.7538062307354, 10310.307241915112, 1584.0017263438733, 6641.337401088013, 901.8386255538334, 1709.350132669816, 2941.4021217709537, 3696.4558475055014, 1445.996297049456, 3879.361734970702, 1688.6334839796348, 2674.519281042482, 4211.055450587906, 5606.559821328592, 3476.830703232174, 4527.745628431611, 3338.255832758639, 20778.212312996868, 4117.5643858001185, 6101.027541991457, 117745.79365324546, 15169.341316967704, 77493.31835447827, 10475.266642297867, 9235.981301007087, 43136.095532703264, 27057.764904001844, 7796.930467677238, 28613.959006558824, 22357.888531624, 58689.48910743428, 19037.857156692582, 27161.997717498532, 28359.275257365265, 25712.554875977577, 34579.674077799966, 15796.408237689906, 36771.90110164259, 751.1655614693676, 598.497747343354, 422.9917074327585, 428.5931958122723, 408.91320381244566, 339.36566937172176, 314.4366543049385, 312.19631157967854, 281.41391788912125, 272.28461239924354, 232.51158339967623, 230.5991158003786, 221.45883284344148, 214.52931762515365, 206.8462092318145, 200.81987025939733, 197.439447424382, 187.96597453893537, 174.41099225447624, 173.2357315376039, 170.4886033724469, 169.33493224414022, 159.83618265089984, 159.54510119442998, 155.04392876860766, 152.70801299854466, 164.34345886401795, 150.05588554426146, 147.05170437783468, 146.1254584232747, 4898.886459171529, 5454.329779195409, 3944.9987044724794, 2570.515474362607, 1106.3263096414987, 3666.9570110534837, 1792.393278806749, 2179.516179594443, 606.2602343946579, 688.8168301623357, 3130.542182699504, 1308.726498152281, 4477.642355504295, 4374.018035872645, 8292.844577935628, 8074.225424853183, 3471.7759400027726, 532.3658312797243, 1854.4376025397296, 10683.431861242114, 5417.703583709024, 2922.555129835548, 1523.015604132002, 7718.639867540757, 1403.875393737008, 3856.4430408506523, 6275.073229565182, 1839.7658037822484, 4486.190095386663, 11712.262523416035, 5128.401536704831, 4352.1607869209965, 3807.7512023641093, 18484.82672607008, 16254.293567514213, 17458.067185452073, 14216.693373840762, 19289.54066354477, 34421.77290596926, 77493.31835447827, 20123.126839427387, 27427.721945805093, 14590.327545034097, 12574.545119080178, 13048.131681197134, 27057.764904001844, 5896.621773561311, 1340.594562123913, 1223.2366873635883, 1026.505920014512, 842.6515724269145, 765.5442561611884, 737.0194523540146, 666.6931934530709, 624.0084033489356, 601.181977573471, 526.4175429737211, 442.80670069900157, 428.4677101690549, 404.9533640381458, 401.3335770148425, 393.9856088942898, 381.9509033033893, 378.2247115054983, 349.8204784537865, 345.8497830776552, 345.12399593629056, 306.2942390949543, 297.306758466757, 294.9374008016004, 289.4220141213837, 277.98094807686317, 276.7294159341123, 271.22047484503605, 265.2978007126264, 262.08748221886185, 656.1567305574151, 1181.4441864901405, 382.7800605357045, 2129.637132508368, 348.4048057511167, 1228.9054364639026, 1344.5649865391808, 464.4286269599651, 1058.5311796825704, 1362.0609004694547, 646.5199168406939, 802.7120976568104, 772.1019572717566, 2264.867237849535, 1662.0762009595078, 1046.7937963936765, 730.4962896258946, 2644.006775775728, 1183.1609715575623, 6466.628673030155, 1931.19100133811, 3833.3755639509013, 959.4754134175504, 11124.172824581265, 1176.2867783241163, 13048.131681197134, 2234.455231183863, 2963.1711435075085, 5710.301354791885, 3789.8056806155255, 39707.30688437456, 10809.280604513951, 35130.68311283235, 77493.31835447827, 16254.293567514213, 6035.050851576647, 27057.764904001844, 23101.588643468956, 27622.028612549464, 19825.80367675944, 14216.693373840762, 1353.6540307445903, 971.9403249770451, 720.0643330408798, 652.8757075012929, 593.4000972846468, 483.3016460958318, 480.2629538041745, 463.47094050344845, 449.0492686198803, 426.59612100958395, 399.7137918483613, 399.17909443440095, 349.06523379388824, 321.63044313016803, 302.24995681688125, 301.8462902307726, 288.99064030633394, 288.8312400515207, 286.7223718466855, 282.97040885663927, 274.91255522659475, 274.56458264212205, 269.9308429512087, 269.68436158587764, 264.75666407201396, 263.1823706436168, 260.0807941126598, 250.09775445548618, 247.48060380934075, 246.26851130984124, 367.37420055581896, 513.5627707168417, 528.9900490811164, 413.38099300049777, 947.8076019875023, 1127.354200335313, 921.5775566255567, 621.5699401200992, 1166.6425372306346, 478.1479956580238, 1903.4589694842327, 398.05025735254526, 1638.8639494852025, 1282.3136177320553, 2701.328187711239, 9375.361681333305, 2529.8246773281835, 2367.7663751399864, 6719.06905260666, 2190.9329142916345, 8264.807185515157, 3102.2809073758185, 3916.342403121075, 11243.625810606685, 1543.8366790396353, 3260.0924326280233, 6410.369330618357, 5458.482715286184, 6558.429210025222, 5377.555066187546, 1512.4562654845413, 117745.79365324546, 6845.703744009922, 18912.370093593814, 6821.369569776687, 3752.195972160871, 3239.625115264947, 7875.652216217256, 77493.31835447827, 20778.212312996868, 5461.083199273226, 4330.444448707467, 12027.80008677036, 5093.149303077431, 27161.997717498532, 8903.476097649924, 5174.8407734921875, 2302.849908215307, 1251.025623086788, 796.3558597597594, 590.1287090077626, 503.73742438546356, 454.9631195036385, 355.22046501546004, 329.09040013307197, 327.6890887827684, 325.30935123199396, 316.1396571328715, 309.4693505715608, 307.2232331379131, 303.32576565707507, 293.1716061468557, 288.4056475090248, 286.78089725750226, 278.9597942177103, 264.55071078146074, 250.68212203603022, 243.1707881646835, 238.59396147691402, 229.2741766878109, 228.68086612614013, 225.09177695609375, 221.33573903793086, 219.23454723078322, 209.78015664970584, 203.38639083916806, 200.93498800738627, 2723.9340471377254, 1568.6094823218145, 428.74898233482566, 765.8660130506813, 1658.3588123157347, 1926.8738790013272, 1579.2712324510933, 2253.074998456088, 1285.64689314087, 4124.455419242819, 1163.1072168285011, 885.1343397037174, 1400.8174261640356, 1289.2014972616096, 903.4715013332425, 1049.385172013087, 712.6840534044995, 9054.069329192098, 719.2756071417589, 1891.1572143994517, 2635.0020108449644, 708.2757540167959, 652.8586266351035, 1083.376541404964, 6558.429210025222, 1084.0042764650848, 2044.4889893757572, 2756.902296602774, 2266.4242888110875, 5377.555066187546, 4693.160565711175, 18912.370093593814, 9235.981301007087], \"Freq\": [117745.0, 26861.0, 10140.0, 9054.0, 17458.0, 16975.0, 27161.0, 43136.0, 34579.0, 20778.0, 13013.0, 18484.0, 5896.0, 27622.0, 22357.0, 34421.0, 39707.0, 8292.0, 10310.0, 19711.0, 8074.0, 58689.0, 20974.0, 4124.0, 18912.0, 9375.0, 10683.0, 11243.0, 33666.0, 4898.0, 7626.235152530962, 4914.678846966521, 4298.179646450061, 3707.252304960102, 2726.7146279432377, 1178.7252965350447, 822.0770943421903, 768.069568079173, 565.4915723271906, 522.1346848076137, 470.91979677193484, 456.02500125030167, 365.25091735015354, 346.4556709960799, 342.00475401658235, 334.20885578751637, 312.7559079696868, 306.14246382814804, 303.88786957085205, 262.1496135723978, 260.59132311777233, 252.8645065424972, 244.71589114445823, 232.81531666370267, 215.56590874794807, 210.68443515281982, 209.3172906399351, 204.56281820813186, 201.49567523365343, 195.4264313686715, 4679.831319617408, 4337.465842653148, 24351.841124143284, 3815.823984864708, 875.3143723092074, 739.3504810546079, 1275.4308580309525, 1924.5228815282985, 3411.5480004424553, 709.851501496734, 480.5500086881293, 1372.341617444443, 567.5525276671498, 459.5196081957658, 3544.301838034624, 2307.649750399298, 4313.584774992158, 1126.5460969100466, 2528.3980448954767, 1636.4248632216504, 1868.6081637197926, 6190.3906805303795, 3691.864914174253, 8979.21353808753, 2297.258464658048, 4987.396967421363, 8268.485084219468, 16251.734905685747, 29287.171255727968, 4780.562900419199, 19486.837251368626, 6064.770251690968, 10341.796666330496, 19441.946059806898, 13910.40869715283, 9848.383927869218, 6593.399651401112, 13686.47436871202, 11888.348113810538, 9406.471271902585, 6527.841430759853, 14814.625092373612, 8226.85275970222, 9626.204734770761, 9921.833231934492, 22161.830158647314, 8038.573694657132, 12813.121344047375, 9911.426879421584, 12379.573935373863, 13734.166634580044, 9386.857027802465, 8137.169178951734, 8110.368519305825, 15779.787207100248, 6953.969436377013, 8052.416503266977, 7439.381958116249, 454.95157431708833, 411.5682059229712, 295.7883547433916, 291.8286243942686, 285.5286698705207, 271.19745654692167, 232.0577152336844, 218.29925020040375, 217.94466591823382, 209.24343242303496, 206.6064349449183, 202.9468960980649, 201.14839499293893, 193.73414070203742, 180.2800732010883, 178.19315527860604, 177.0237932051709, 176.7788339953773, 170.9902648291439, 156.57316857688056, 156.4771408357021, 153.2176640055957, 152.2647243593283, 152.02055584566875, 151.85060929993477, 150.95377340163262, 144.8351810925646, 137.51422211131916, 136.2620837392604, 135.09427136929497, 692.2987350764176, 270.83253227522374, 673.8640934676666, 553.4235815785195, 589.070457103309, 586.0703820216008, 1260.6113115682729, 255.26079794264166, 66039.60281901479, 1098.7331191753913, 1203.6612909100488, 923.9938011182401, 483.8830616113389, 18557.640260038843, 3875.470070788993, 685.0483487299148, 2469.880879893504, 1388.6419668760016, 5692.340366682428, 1958.5004602088927, 1660.7688633074067, 387.6949027482869, 10700.7747928981, 2746.3185189241844, 10181.778375507643, 2540.2630394365688, 866.6318756984527, 1078.8746944535324, 5811.1218347153745, 1893.1545852468557, 2726.4420812673084, 3834.0041547080214, 4078.5337159879873, 4655.126579791872, 10383.704654752704, 3190.3040549193174, 4912.46686022234, 7532.740688819471, 1997.8182917208414, 5873.830731338113, 4972.158975404178, 9057.984585546299, 6738.906342759781, 6240.555819088957, 2590.1793597697974, 4240.018719541026, 3042.825677692222, 3566.5532130264905, 12704.394381429824, 8372.299144211502, 4414.538708386387, 6552.020897699347, 7283.5038549445135, 9351.941229137696, 5266.47573825113, 7580.777710682805, 6261.683876712637, 5725.668921069297, 6863.615337278703, 5857.617178074035, 6269.34307856786, 6204.928430847864, 5041.985068954227, 4847.170900114566, 4957.142226815735, 2014.0212757065235, 1738.2519746364176, 498.73791392615317, 443.6976191379895, 422.6529172636488, 339.5207243507945, 332.0744809001683, 312.07453914304887, 298.2302838812618, 290.0824106771993, 289.8586113351144, 274.4390641258567, 208.32837413626828, 204.15866719331774, 198.78204228392377, 197.83812838817687, 188.87998601915032, 185.88610983295695, 182.14493345560564, 179.4195486143928, 177.48283761651146, 177.38998018759713, 161.9472784664064, 160.9250713804183, 160.81905745098982, 156.15335831760964, 155.8122650746027, 153.60293735176248, 151.5513152154987, 142.35732439198821, 3920.0120694132884, 13383.522093236035, 1175.828359985038, 3143.632587346449, 956.1888400041682, 6200.872053374932, 6743.1362869338855, 8840.613615132774, 2612.477182523004, 16410.895985136292, 4981.95718459128, 3020.525644369028, 434.2096597635544, 2224.954031731229, 2500.461349904368, 294.4466130854573, 2755.9126846409827, 1592.1029223885105, 5199.069571849922, 3631.5274194946037, 4068.417332387588, 1793.8077525077108, 8193.370783495831, 2667.916158137522, 15427.058893233509, 7241.7864983423315, 11353.55354039109, 7537.522848738064, 11293.574459693164, 8081.647426353778, 4135.924159517332, 2924.7691618284407, 16821.597201659326, 8605.999288277955, 13613.856133111714, 7926.87663513544, 6948.48862580408, 6177.00747187744, 8595.110792502972, 5404.917871926538, 6976.563735048355, 8239.733704409642, 4817.264682131064, 6141.683419855997, 5296.907450251967, 4821.206682298434, 5121.2632206963945, 6663.262872608145, 5262.14992773787, 4954.199322088639, 1491.0174903372686, 615.0462587773974, 406.74011937812304, 390.51413927384675, 384.94517543397245, 326.7948164670395, 316.1858732945498, 306.03452357406553, 294.5159058352879, 266.5726525219031, 260.87851724458824, 260.155578449449, 231.5713891377707, 223.2303358483253, 215.75748899227028, 207.87115726321417, 205.99270237450514, 204.15808494850424, 202.2882399450929, 202.16987762093675, 202.00210062103483, 201.7111256927545, 196.20379155306176, 187.63085266634576, 186.04347623909572, 185.74158305347555, 185.30334879065887, 184.2026910546077, 184.08362115770473, 182.3929287017969, 3036.7409005947907, 3146.7168527096082, 2029.4060518175481, 555.9901333478078, 347.1276114295688, 292.0743244453356, 251.7984804069742, 2052.754036863952, 810.7997240473867, 355.2382420380966, 314.4146449320492, 980.800340720916, 262.769008849189, 728.3708416262092, 2470.865660151865, 632.3868544371609, 1285.8342506004456, 979.0233101157228, 695.5826471403951, 571.1552841727671, 1387.2749206761068, 647.5721038218962, 1253.4467323071, 1173.7879070145286, 1737.6151002861766, 2041.1931121065873, 795.3883658900887, 771.157544334144, 1642.1080980894656, 807.1207744055131, 1466.1002612295028, 1275.901545889272, 3062.8359036027346, 1703.9814016855407, 2393.8871458197787, 1348.1471482164823, 2137.082919377454, 1426.566572420095, 996.0875095216469, 3206.8764838133625, 3097.226367853284, 1945.8580712507446, 3136.5683592480586, 1941.1310068544392, 1373.0681019844553, 2614.35809081694, 2911.1426937949273, 3493.752362617283, 1881.4147538089812, 1692.9814656324709, 1474.564949907986, 1809.0809941567427, 1629.8112460050165, 1571.0153218585901, 1528.1085443215695, 1485.052231872915, 1471.529977768669, 382.39618351188494, 352.75781904557635, 326.4118353547969, 281.19034248453784, 258.8382134047459, 255.8378003437354, 252.5780289720722, 248.47589898211146, 241.18532473624327, 212.96919303436204, 210.62414401262856, 208.6358031769891, 198.60224313734588, 180.41293751246812, 178.92311690804686, 178.80023085924594, 173.9049315128075, 173.8269461951959, 172.5709566302667, 170.86209191855437, 170.260452176695, 169.56798142659545, 164.8906065351762, 151.76300834102338, 142.95585712273873, 142.60208697382615, 140.35016094202877, 140.09396491712056, 137.82896873191567, 134.87621510183274, 1672.3875709093709, 1811.5886572404665, 279.5851363801872, 342.6936662800105, 2150.225833263634, 270.1142756318578, 300.1298182150533, 885.4299854130377, 1585.0524555309669, 1101.1748904349017, 392.5293600157039, 430.0955002521798, 8513.279766779906, 786.0989157627316, 1124.0293075632815, 652.5253752996542, 346.28158295154594, 2491.288263349185, 1193.4820372167828, 2281.984629038991, 663.0332171994446, 612.6539710974323, 1607.6542174463707, 4402.038447201492, 2546.9675506027784, 1524.7550613791736, 769.7962748961841, 720.435071823575, 2269.8813183527163, 637.5755056848599, 3679.597388676595, 1110.382979044869, 2206.6133121117414, 1042.402285001805, 5771.464624071696, 1128.8034425609064, 4134.707766981791, 863.6923207113164, 3493.59825434143, 2263.5472459898474, 3617.350708356532, 2748.0983159095426, 1320.2462684305722, 1689.651662596804, 5395.4485776029205, 4236.448305023766, 1891.5505774676358, 1998.9902397656788, 2363.163927533167, 1834.985432330226, 2226.1300103483804, 1603.3253584463853, 1799.2562649577367, 1743.5614587187808, 1656.0209521998177, 1631.0809912938143, 2630.5811920156198, 1375.7595069962647, 1272.550946066488, 1218.876425718969, 971.3013205244337, 938.3679177161509, 812.0953048592842, 741.1466004444515, 563.208697009835, 539.1915363949065, 513.9498048155248, 509.7404564374432, 507.9540051644388, 506.5546148859086, 495.9042257658039, 488.332802437576, 493.1286794491273, 485.6264341951284, 477.6477747501001, 472.4349126622666, 469.4776333614449, 464.0673036267761, 425.432251235912, 363.06650722377987, 359.2876561978812, 346.81022472823565, 330.34547393918604, 307.9011946528999, 305.2103550671917, 279.2883716654269, 10092.626979543475, 765.1190837973314, 478.2179079803498, 988.2524480643399, 484.1327098668275, 1870.8400741859232, 901.922813504463, 2788.793341358564, 886.6723167345397, 5938.426666777296, 1231.6030829717783, 3619.973448146674, 767.6832307812945, 1235.9397491460068, 1823.615883830129, 2028.2845281860016, 1049.7621852452023, 2039.7126319332024, 1126.3214815292092, 1519.3013089129379, 2047.1735945259304, 2458.927420617596, 1761.2402360501967, 2079.5070921868887, 1683.7999046689365, 4941.324834650766, 1877.0596492523991, 2321.273571279571, 11379.734670701795, 3582.576275878757, 7289.497841542369, 2753.7486221221448, 2579.167233753978, 4861.236437865311, 3746.3259787258085, 2033.3073880251093, 2763.111467715755, 2438.0490106862585, 2857.146773778199, 2146.0916007988335, 2203.629678448847, 2134.9477630597203, 2101.1089208883222, 2062.525887256016, 1975.1043855872774, 2018.4978164056065, 750.2684020052641, 597.6005522318511, 422.09447329703545, 427.682645760416, 408.0159751466474, 338.468490141975, 313.53930494169845, 311.29894655241293, 280.5167432573125, 271.387205389675, 231.61442734696806, 229.70195122100668, 220.56148253553368, 213.63211665745501, 205.94901487894498, 199.92265551287022, 196.54220347247596, 187.06860536365323, 173.51371411474463, 172.3386273426572, 169.59145862738436, 168.4377759879383, 158.93898852489804, 158.64774844293183, 154.14678691051202, 151.8108363562627, 163.37515142094085, 149.1585268032599, 146.15417441190104, 145.2281646347296, 4706.743109715751, 4814.064426564728, 3485.5358521936737, 2308.020297765283, 1021.2227633872923, 3159.7872360885717, 1566.5891173835066, 1875.6309454028249, 563.3035259623266, 633.4125018159658, 2554.9778144861425, 1101.0925525019738, 3348.937674711287, 3274.3668973409985, 5710.383829642795, 5424.529404937177, 2472.8515861750525, 473.03871115420435, 1351.5004262336147, 5938.152304204839, 3248.2916761384686, 1871.8529243091662, 1104.012351209083, 4001.3334358337675, 1004.8877014891001, 2155.70170210818, 3020.5526842264912, 1206.457312344601, 2186.4449753604936, 4190.396137419141, 2363.7708886986934, 2116.0758694609485, 1858.266853536618, 4648.64703952665, 4257.249591714655, 3938.7142723006373, 3258.183468551762, 3682.930329640579, 3918.4571900048322, 3814.5483456470697, 2670.7994496662272, 2654.5691126086467, 2352.2828684833507, 2274.7025054965447, 2264.431440517161, 2237.27836607727, 5895.721268672983, 1339.6940515490503, 1222.335794912748, 1025.6054235241627, 841.7511231253842, 764.6436777752946, 736.1189597298893, 665.79268285407, 623.1078298033184, 600.2815275705394, 525.5170367133393, 441.9062019506335, 427.56719601084654, 404.05287828225755, 400.43312823103315, 393.08508705347356, 381.05041808092193, 377.3242334135696, 348.92000748354127, 344.94920247431924, 344.22347833171796, 305.3937195777194, 296.4062079431651, 294.03674204133785, 288.5215980614883, 277.0804930595619, 275.82890637817616, 270.3200737677154, 264.397378048137, 261.187049560914, 641.3524807724823, 1130.3958646034216, 376.550816029715, 1888.0875822811054, 342.8486402005983, 1076.7341750543399, 1164.476403445519, 443.14866806008945, 900.6112383806186, 1110.5074367452296, 583.4329931516976, 694.7744407272999, 671.5833291602102, 1537.3499164663067, 1198.4797825624767, 834.9860296123437, 627.5348862436593, 1656.1113366369034, 891.2856076347518, 3098.8858821951017, 1248.1023232985676, 2006.672790510746, 752.2399640305806, 2940.3025236760514, 796.2626665007807, 2208.9384644823, 1041.6388988101803, 1167.86728567596, 1514.1940024578341, 1197.3479180966406, 2574.2836466821805, 1618.5998123684326, 2201.0870539659945, 2708.5608225857563, 1729.6195176639, 1251.2713987782531, 1621.1924077327421, 1490.219309763158, 1318.226009937685, 1218.0395575573195, 1117.6951149610866, 1352.7581987701083, 971.044538750743, 719.1685023512465, 651.9798764500823, 592.504214250176, 482.40577387327164, 479.3671060168617, 462.575142576498, 448.15340470167297, 425.7002805116744, 398.8179850620376, 398.28320817340654, 348.1693777619851, 320.7344671934233, 301.35414035844525, 300.9504517645467, 288.09480297667324, 287.9354383697683, 285.82650689972735, 282.0744921749219, 274.0166615440636, 273.66876289413, 269.0350383386492, 268.78831732918064, 263.86083045514135, 262.286447865227, 259.1850279909076, 249.2016567720553, 246.5847591671668, 245.37270701437865, 360.4047663089728, 493.3124130631849, 506.39445293613846, 398.0458977010864, 847.4576797330988, 936.2027866589943, 783.0341721661945, 539.4234302459458, 904.6177079682436, 431.80500284877434, 1282.5696428052436, 370.5426242862591, 1127.0455436781706, 917.3192911970599, 1624.789466926569, 4175.257100459488, 1525.147118143117, 1414.7169268500888, 3012.87378166995, 1313.8655580303137, 3403.3835185791754, 1636.214383751752, 1877.3786838739215, 3708.8107680921607, 946.4516741562188, 1502.4725594406686, 2260.141722761674, 2038.7146147254402, 2245.511722468779, 1947.6188786788375, 932.0216586462177, 10635.375512539447, 2069.853412733321, 3385.2776413876536, 1923.59507982022, 1381.2372292004468, 1298.348884325522, 1806.9558032383002, 3914.9283863554415, 2391.481449562769, 1511.7833312789078, 1345.3094842852706, 1667.6887817583217, 1386.9787165913424, 1796.2602817416766, 1448.1681313210672, 1355.4183927046747, 2301.9561926230735, 1250.131874024718, 795.4621516336997, 589.235028091762, 502.8436737429519, 454.0694257955067, 354.3267529811414, 328.19666942221374, 326.79516924420733, 324.4156000733216, 315.2459801642277, 308.5756035346139, 306.3295222023675, 302.43204387590305, 292.2778866819268, 287.51195805494956, 285.8871397230928, 278.0660334833533, 263.656944326722, 249.78840893919826, 242.2770640171234, 237.70019704355417, 228.38024428748258, 227.78698472278646, 224.19810535444023, 220.44201520965413, 218.3407156272867, 208.8863422410227, 202.49269815531773, 200.0412607179492, 2704.3667301936352, 1561.0980979877959, 422.4065985239257, 737.431857346371, 1552.8166146634526, 1790.5742808009034, 1453.410922910921, 2035.6272148948403, 1186.6319592888656, 3395.7364571037547, 1046.2941171935347, 808.6307468777176, 1195.688661646569, 1024.262050235089, 746.359371935943, 801.3433252519461, 589.1655325496799, 4073.2735137218824, 580.2801307397666, 1115.2798434986141, 1319.316434356403, 531.3924094916781, 501.4069085048103, 614.9725471975054, 999.3399939336975, 566.9152416631969, 651.7230587654182, 682.473873700304, 649.4672637953254, 727.547237155781, 668.9636584054784, 648.2364726841073, 621.6763079018962], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5065, 1.5064, 1.5064, 1.5064, 1.5063, 1.5059, 1.5055, 1.5055, 1.505, 1.5049, 1.5047, 1.5047, 1.5042, 1.504, 1.504, 1.5039, 1.5038, 1.5037, 1.5037, 1.5032, 1.5032, 1.5031, 1.503, 1.5028, 1.5025, 1.5024, 1.5023, 1.5022, 1.5022, 1.502, 1.49, 1.469, 1.4085, 1.439, 1.4715, 1.4755, 1.4551, 1.4399, 1.3931, 1.4553, 1.4753, 1.4168, 1.4644, 1.4756, 1.3133, 1.3506, 1.2746, 1.3964, 1.3162, 1.359, 1.335, 1.1869, 1.2431, 1.1202, 1.306, 1.2, 1.0856, 0.9488, 0.8115, 1.1567, 0.8716, 1.0924, 0.9497, 0.7925, 0.8277, 0.9119, 1.0225, 0.8044, 0.8177, 0.8909, 1.0166, 0.659, 0.885, 0.7899, 0.758, 0.2548, 0.8732, 0.5406, 0.6931, 0.4636, 0.3622, 0.606, 0.6523, 0.6533, -0.5032, 0.8707, 0.4401, 0.672, 1.7382, 1.7379, 1.7371, 1.7371, 1.737, 1.7368, 1.7363, 1.736, 1.736, 1.7358, 1.7358, 1.7357, 1.7357, 1.7355, 1.7352, 1.7351, 1.7351, 1.7351, 1.7349, 1.7344, 1.7344, 1.7343, 1.7342, 1.7342, 1.7342, 1.7342, 1.7339, 1.7336, 1.7336, 1.7335, 1.7294, 1.718, 1.6647, 1.669, 1.6582, 1.6405, 1.5769, 1.6942, 1.1619, 1.5539, 1.54, 1.5512, 1.6222, 1.1223, 1.3351, 1.5643, 1.3689, 1.4534, 1.2325, 1.3808, 1.3995, 1.6338, 1.0403, 1.2802, 1.0268, 1.2904, 1.4865, 1.4244, 1.0127, 1.283, 1.1876, 1.0719, 1.0423, 0.9978, 0.7354, 1.0788, 0.9295, 0.7724, 1.2305, 0.7914, 0.8498, 0.5899, 0.631, 0.6248, 1.087, 0.7882, 0.9659, 0.8524, -0.0681, 0.1835, 0.6604, 0.3219, 0.1667, -0.0966, 0.4419, 0.0014, 0.2285, 0.3326, 0.0616, 0.2609, 0.0325, 0.049, 0.1935, 0.1786, 0.0391, 2.006, 2.006, 2.0047, 2.0044, 2.004, 2.0038, 2.0038, 2.0036, 2.0035, 2.0034, 2.0034, 2.0032, 2.0022, 2.0021, 2.002, 2.0019, 2.0017, 2.0017, 2.0016, 2.0015, 2.0014, 2.0014, 2.0009, 2.0009, 2.0009, 2.0007, 2.0007, 2.0006, 2.0006, 2.0002, 1.9428, 1.7687, 1.8674, 1.774, 1.8633, 1.6924, 1.6756, 1.6199, 1.7191, 1.5026, 1.6295, 1.6611, 1.898, 1.6887, 1.6123, 1.9383, 1.5426, 1.6288, 1.3893, 1.3847, 1.356, 1.5594, 1.17, 1.4463, 0.9782, 1.0694, 0.9195, 1.0451, 0.8874, 0.9889, 1.2196, 1.3429, 0.4789, 0.805, 0.5453, 0.8297, 0.9017, 0.8766, 0.5986, 0.934, 0.6511, 0.4339, 1.01, 0.4766, 0.6866, 0.7224, 0.5, -0.8655, 0.1283, 0.2951, 2.4622, 2.4613, 2.4606, 2.4605, 2.4604, 2.46, 2.4599, 2.4598, 2.4597, 2.4594, 2.4593, 2.4593, 2.4589, 2.4588, 2.4586, 2.4585, 2.4584, 2.4584, 2.4583, 2.4583, 2.4583, 2.4583, 2.4582, 2.458, 2.4579, 2.4579, 2.4579, 2.4579, 2.4579, 2.4579, 2.426, 2.4089, 2.405, 2.4222, 2.4319, 2.4331, 2.44, 2.2916, 2.3571, 2.4154, 2.4233, 2.2952, 2.4309, 2.3057, 2.1411, 2.3154, 2.1854, 2.1889, 2.2513, 2.2788, 2.0555, 2.242, 2.0521, 2.0691, 1.901, 1.761, 2.1088, 2.1178, 1.815, 2.0907, 1.7779, 1.8385, 1.3589, 1.6467, 1.4156, 1.7207, 1.3856, 1.6429, 1.9203, 0.8397, 0.8041, 1.1984, 0.5889, 1.135, 1.4695, 0.4219, 0.2198, -0.6364, 0.6984, 0.7726, 1.0713, -0.2424, 0.2968, 0.5459, 0.3555, 0.7945, 0.1033, 2.4827, 2.4825, 2.4823, 2.4819, 2.4816, 2.4816, 2.4815, 2.4815, 2.4813, 2.4809, 2.4808, 2.4808, 2.4805, 2.4801, 2.4801, 2.48, 2.4799, 2.4799, 2.4799, 2.4798, 2.4798, 2.4798, 2.4796, 2.4792, 2.4788, 2.4788, 2.4787, 2.4787, 2.4786, 2.4784, 2.4526, 2.4038, 2.4529, 2.4414, 2.3171, 2.4376, 2.4282, 2.3281, 2.2295, 2.2619, 2.3859, 2.3713, 1.7669, 2.2131, 2.1157, 2.2269, 2.3518, 1.9345, 2.0504, 1.8736, 2.1731, 2.1915, 1.9146, 1.5867, 1.7402, 1.902, 2.1058, 2.1007, 1.6535, 2.1384, 1.3787, 1.8791, 1.5342, 1.862, 0.9194, 1.7915, 0.9875, 1.9296, 0.9476, 1.1607, 0.7538, 0.8533, 1.5397, 1.1719, -0.5979, -0.4214, 0.8308, 0.7133, 0.1925, 0.7234, 0.2152, 0.9957, 0.4764, 0.054, -0.3085, 0.0182, 2.5734, 2.5731, 2.573, 2.573, 2.5728, 2.5728, 2.5726, 2.5725, 2.5722, 2.5721, 2.572, 2.572, 2.572, 2.572, 2.5719, 2.5719, 2.5719, 2.5719, 2.5719, 2.5719, 2.5718, 2.5718, 2.5716, 2.5713, 2.5713, 2.5712, 2.571, 2.5708, 2.5708, 2.5705, 2.5691, 2.5376, 2.5533, 2.5184, 2.5522, 2.4632, 2.507, 2.4152, 2.4739, 2.0221, 2.3221, 1.9669, 2.4127, 2.2495, 2.0957, 1.9736, 2.2535, 1.9309, 2.1688, 2.0082, 1.8525, 1.7495, 1.8937, 1.7957, 1.8894, 1.1375, 1.7882, 1.6074, 0.2371, 1.1306, 0.21, 1.2377, 1.2981, 0.3907, 0.5966, 1.2297, 0.2362, 0.3578, -0.4487, 0.391, 0.062, -0.0128, 0.0692, -0.2456, 0.4946, -0.3286, 2.587, 2.5867, 2.5861, 2.5861, 2.586, 2.5855, 2.5853, 2.5853, 2.585, 2.5849, 2.5843, 2.5843, 2.5841, 2.584, 2.5838, 2.5837, 2.5836, 2.5834, 2.583, 2.583, 2.5829, 2.5829, 2.5826, 2.5825, 2.5824, 2.5823, 2.5823, 2.5822, 2.5821, 2.582, 2.5482, 2.4633, 2.4644, 2.4805, 2.5081, 2.4393, 2.4535, 2.438, 2.5147, 2.5043, 2.385, 2.4154, 2.2977, 2.2986, 2.2151, 2.1904, 2.2489, 2.47, 2.2718, 2.0009, 2.0766, 2.1427, 2.2664, 1.9312, 2.2538, 2.0066, 1.857, 2.1662, 1.8695, 1.5603, 1.8137, 1.8671, 1.8708, 1.2078, 1.2485, 1.0992, 1.1149, 0.9323, 0.4152, -0.4232, 0.5687, 0.2529, 0.7632, 0.8784, 0.8369, 0.0955, 2.7411, 2.7406, 2.7405, 2.7404, 2.7402, 2.7401, 2.74, 2.7399, 2.7398, 2.7397, 2.7395, 2.7392, 2.7391, 2.739, 2.739, 2.7389, 2.7389, 2.7389, 2.7387, 2.7386, 2.7386, 2.7383, 2.7382, 2.7382, 2.7381, 2.738, 2.738, 2.7379, 2.7378, 2.7378, 2.7184, 2.6971, 2.7248, 2.6208, 2.7252, 2.609, 2.5974, 2.6943, 2.5797, 2.5371, 2.6386, 2.5968, 2.6018, 2.3538, 2.4142, 2.5152, 2.5893, 2.2734, 2.458, 2.0056, 2.3047, 2.094, 2.4979, 1.4106, 2.351, 0.9651, 1.978, 1.8102, 1.4138, 1.589, 0.0053, 0.8424, -0.0289, -0.6125, 0.5008, 1.1678, -0.0736, 0.0003, -0.3011, -0.0485, 0.1981, 2.8599, 2.8597, 2.8594, 2.8592, 2.8591, 2.8587, 2.8587, 2.8587, 2.8586, 2.8585, 2.8584, 2.8584, 2.858, 2.8578, 2.8576, 2.8576, 2.8575, 2.8575, 2.8575, 2.8574, 2.8573, 2.8573, 2.8573, 2.8573, 2.8572, 2.8572, 2.8572, 2.857, 2.857, 2.857, 2.8414, 2.8204, 2.8169, 2.8228, 2.7487, 2.6748, 2.6977, 2.7189, 2.6062, 2.7587, 2.4658, 2.789, 2.4862, 2.5256, 2.3522, 2.0517, 2.3545, 2.3456, 2.0585, 2.3492, 1.9734, 2.2208, 2.1253, 1.7515, 2.3713, 2.086, 1.8181, 1.8757, 1.7888, 1.845, 2.3765, 0.4563, 1.6645, 1.1402, 1.5947, 1.8612, 1.9462, 1.3885, -0.1248, 0.6986, 1.5762, 1.6916, 0.8848, 1.5598, 0.1445, 1.0445, 1.5209, 3.6215, 3.6211, 3.6207, 3.6203, 3.6201, 3.6199, 3.6193, 3.6191, 3.6191, 3.6191, 3.619, 3.619, 3.6189, 3.6189, 3.6188, 3.6187, 3.6187, 3.6186, 3.6185, 3.6183, 3.6182, 3.6181, 3.6179, 3.6179, 3.6179, 3.6178, 3.6178, 3.6176, 3.6174, 3.6174, 3.6146, 3.6171, 3.6069, 3.584, 3.5561, 3.5485, 3.5388, 3.5204, 3.5417, 3.4274, 3.516, 3.5315, 3.4635, 3.3918, 3.4308, 3.3522, 3.4315, 2.8231, 3.4071, 3.0938, 2.9301, 3.3345, 3.3579, 3.0556, 1.7404, 2.9736, 2.4786, 2.2257, 2.372, 1.6215, 1.6737, 0.2485, 0.9234]}, \"R\": 30, \"token.table\": {\"Term\": [\"1/10\", \"13th\", \"2/10\", \"3000\", \"abc\", \"abstract\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"affecting\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"aids\", \"aired\", \"akira\", \"aladdin\", \"album\", \"alex\", \"alex\", \"alex\", \"ali\", \"alien\", \"alien\", \"alien\", \"allan\", \"allan\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"amelie\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"americans\", \"americans\", \"americans\", \"americans\", \"americans\", \"amitabh\", \"andre\", \"animated\", \"animation\", \"animation\", \"annoying\", \"annoying\", \"annoying\", \"annoying\", \"annoying\", \"annoying\", \"ants\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apes\", \"argento\", \"army\", \"army\", \"army\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"arthur\", \"artistic\", \"artistic\", \"arts\", \"arts\", \"arts\", \"asoka\", \"attorney\", \"attorney\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"aunt\", \"aunt\", \"avery\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"awful\", \"awful\", \"awful\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"bachchan\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"ballet\", \"bambi\", \"band\", \"band\", \"band\", \"band\", \"barrymore\", \"batman\", \"battle\", \"battle\", \"battle\", \"battle\", \"battle\", \"battle\", \"beowulf\", \"bernie\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blah\", \"blood\", \"blood\", \"blood\", \"blood\", \"bogart\", \"bond\", \"bond\", \"bond\", \"bond\", \"bond\", \"bond\", \"book\", \"book\", \"book\", \"book\", \"book\", \"boring\", \"boring\", \"boring\", \"boring\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"bother\", \"bother\", \"bother\", \"bother\", \"boxing\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"bronson\", \"bronson\", \"brooks\", \"brooks\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"budget\", \"budget\", \"budget\", \"bugs\", \"bugs\", \"bunny\", \"bunuel\", \"bunuel\", \"burton\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"cabaret\", \"cagney\", \"cagney\", \"caine\", \"caine\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"canceled\", \"candyman\", \"cannibal\", \"captain\", \"captain\", \"captain\", \"captain\", \"car\", \"car\", \"car\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"carol\", \"carrey\", \"cartoon\", \"cartoon\", \"cartoons\", \"cary\", \"cary\", \"cassavetes\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"chainsaw\", \"chan\", \"chaney\", \"chaplin\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charles\", \"charlie\", \"charlie\", \"charlie\", \"charlie\", \"chase\", \"chase\", \"chase\", \"chase\", \"chase\", \"che\", \"child\", \"child\", \"child\", \"child\", \"child\", \"children\", \"children\", \"children\", \"children\", \"children\", \"children\", \"cinderella\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"civil\", \"colours\", \"columbo\", \"comedies\", \"comedies\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"commander\", \"commitment\", \"communist\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"compassion\", \"concert\", \"connery\", \"connie\", \"contestants\", \"conveys\", \"cop\", \"cop\", \"coppola\", \"cops\", \"cops\", \"costello\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"crap\", \"crappy\", \"crappy\", \"crawford\", \"creature\", \"creature\", \"creature\", \"creepy\", \"creepy\", \"creepy\", \"creepy\", \"crime\", \"crime\", \"crime\", \"crime\", \"criminal\", \"criminal\", \"criminal\", \"crumb\", \"cult\", \"cult\", \"cult\", \"cult\", \"cult\", \"culture\", \"culture\", \"culture\", \"culture\", \"culture\", \"culture\", \"dad\", \"dad\", \"dad\", \"daffy\", \"damme\", \"dana\", \"dance\", \"dance\", \"dancing\", \"dancing\", \"daughter\", \"daughter\", \"daughters\", \"david\", \"david\", \"david\", \"david\", \"david\", \"dead\", \"dead\", \"dead\", \"dead\", \"dealer\", \"dealer\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"deeply\", \"deeply\", \"deeply\", \"deeply\", \"deeply\", \"demon\", \"demon\", \"deniro\", \"dern\", \"desires\", \"desires\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"detective\", \"detectives\", \"dev\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dinosaur\", \"dinosaurs\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"disney\", \"disney\", \"disney\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"donnie\", \"dorothy\", \"douglas\", \"douglas\", \"douglas\", \"downside\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"dracula\", \"dumber\", \"duvall\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"effects\", \"effects\", \"effects\", \"effects\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"emotional\", \"emotional\", \"emotional\", \"emotional\", \"emotions\", \"emotions\", \"emotions\", \"emotions\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"enemy\", \"enemy\", \"enemy\", \"enemy\", \"enemy\", \"englund\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enterprise\", \"episode\", \"episode\", \"episode\", \"episodes\", \"errol\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"everett\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"fairbanks\", \"family\", \"family\", \"family\", \"family\", \"fan\", \"fan\", \"fan\", \"fan\", \"fan\", \"fans\", \"fans\", \"fans\", \"fans\", \"fans\", \"farm\", \"farm\", \"farm\", \"farm\", \"farrow\", \"father\", \"father\", \"father\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feelings\", \"feelings\", \"feelings\", \"feelings\", \"fellini\", \"festival\", \"festival\", \"festival\", \"fianc\\u00e9e\", \"fight\", \"fight\", \"fight\", \"fight\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finds\", \"finds\", \"finds\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fishing\", \"flick\", \"flick\", \"flick\", \"flick\", \"flynn\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"focus\", \"francis\", \"francis\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fu\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funniest\", \"funniest\", \"funny\", \"funny\", \"gail\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"games\", \"gang\", \"gang\", \"gang\", \"gang\", \"gang\", \"garbage\", \"gay\", \"gay\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"george\", \"george\", \"george\", \"george\", \"george\", \"german\", \"german\", \"german\", \"german\", \"germans\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"giallo\", \"ginger\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goldie\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gore\", \"gory\", \"government\", \"grandfather\", \"grant\", \"grant\", \"grant\", \"grant\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"gregory\", \"gregory\", \"gremlins\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"ha\", \"halloween\", \"hand-held\", \"hank\", \"harding\", \"hardy\", \"harold\", \"harriet\", \"harvard\", \"hawn\", \"heist\", \"highlander\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"hindi\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hitler\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"holmes\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"homosexuality\", \"hong\", \"hood\", \"hood\", \"hood\", \"hood\", \"horrible\", \"horror\", \"horror\", \"house\", \"house\", \"house\", \"house\", \"housekeeper\", \"howard\", \"howard\", \"howard\", \"howard\", \"howard\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hughes\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humphrey\", \"husband\", \"husband\", \"husband\", \"husband\", \"idiot\", \"idiot\", \"idiot\", \"idiotic\", \"idiotic\", \"idiotic\", \"imagery\", \"imagery\", \"images\", \"images\", \"images\", \"images\", \"immigrants\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"iraq\", \"issues\", \"issues\", \"issues\", \"issues\", \"jack\", \"jack\", \"jack\", \"jack\", \"jane\", \"jane\", \"jane\", \"jane\", \"jane\", \"jane\", \"jews\", \"joan\", \"joe\", \"joe\", \"joe\", \"joe\", \"joe\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"jokes\", \"jokes\", \"judy\", \"juliet\", \"junk\", \"junk\", \"karen\", \"karloff\", \"keane\", \"kelly\", \"kelly\", \"kelly\", \"kelly\", \"khan\", \"khan\", \"kidding\", \"kidman\", \"killed\", \"killed\", \"killed\", \"killed\", \"killed\", \"killer\", \"killer\", \"killer\", \"kirk\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kong\", \"kubrick\", \"kung\", \"kurosawa\", \"ladd\", \"lame\", \"lame\", \"lame\", \"lana\", \"lancaster\", \"las\", \"laughton\", \"leatherface\", \"lenny\", \"leonardo\", \"leopard\", \"li\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"liked\", \"liked\", \"liked\", \"liked\", \"liked\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"locusts\", \"lombard\", \"looney\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"loved\", \"loved\", \"loved\", \"loved\", \"loved\", \"loy\", \"lugosi\", \"lynch\", \"lynch\", \"lyrics\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"mae\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"makes\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"manga\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"marple\", \"married\", \"married\", \"married\", \"married\", \"married\", \"marries\", \"marry\", \"marrying\", \"martial\", \"massacre\", \"massacre\", \"match\", \"match\", \"match\", \"match\", \"match\", \"match\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"meets\", \"meets\", \"meets\", \"meets\", \"melanie\", \"melody\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"meryl\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"midler\", \"miike\", \"military\", \"mimi\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"mob\", \"mob\", \"mob\", \"mom\", \"mom\", \"mom\", \"money\", \"money\", \"money\", \"money\", \"money\", \"money\", \"money\", \"monster\", \"monster\", \"monsters\", \"monsters\", \"monsters\", \"moral\", \"moral\", \"moral\", \"moral\", \"moral\", \"moral\", \"morality\", \"morality\", \"morality\", \"motel\", \"mother\", \"mother\", \"mothers\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"ms\", \"ms\", \"ms\", \"ms\", \"ms\", \"mst3k\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"murder\", \"murder\", \"murdered\", \"murdered\", \"murdered\", \"murderer\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"musical\", \"musical\", \"musicals\", \"musicians\", \"narrative\", \"narrative\", \"narrative\", \"navy\", \"navy\", \"nazi\", \"nazi\", \"nazis\", \"nbc\", \"neil\", \"neil\", \"nero\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nikita\", \"nikki\", \"ninja\", \"niro\", \"nope\", \"norma\", \"norwegian\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"novel\", \"novel\", \"nuanced\", \"nudity\", \"nudity\", \"nudity\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numbers\", \"numbers\", \"o'hara\", \"o'neill\", \"oh\", \"oh\", \"oh\", \"ok\", \"ok\", \"ok\", \"ok\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"orphan\", \"orphanage\", \"orson\", \"oscar\", \"oscar\", \"oscar\", \"oscar\", \"oscar\", \"palma\", \"paltrow\", \"pammy\", \"panther\", \"parents\", \"parents\", \"parents\", \"parents\", \"passion\", \"passion\", \"passion\", \"passion\", \"passion\", \"passion\", \"passionate\", \"pathetic\", \"pathetic\", \"peanuts\", \"penn\", \"penn\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performances\", \"performances\", \"performances\", \"performances\", \"performances\", \"performances\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"pierce\", \"pirates\", \"pitt\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"poirot\", \"police\", \"police\", \"political\", \"political\", \"political\", \"politicians\", \"porky\", \"portrait\", \"portrait\", \"portrait\", \"portrait\", \"portrayal\", \"portrayal\", \"portrayal\", \"portrayal\", \"portrayal\", \"powell\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"powerful\", \"powerful\", \"powerful\", \"powerful\", \"powerful\", \"powerful\", \"predator\", \"predator\", \"president\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"prince\", \"prince\", \"prince\", \"prison\", \"prison\", \"prison\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"propaganda\", \"prostitution\", \"psyche\", \"quaid\", \"quaid\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"rangers\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"rebellious\", \"redeeming\", \"redeeming\", \"reilly\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationships\", \"relationships\", \"relationships\", \"relationships\", \"remember\", \"remember\", \"remember\", \"remember\", \"remember\", \"remember\", \"revolution\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ringu\", \"ripoff\", \"robbery\", \"robert\", \"robert\", \"robert\", \"robert\", \"robert\", \"robin\", \"robin\", \"robin\", \"robinson\", \"robot\", \"robot\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"rogers\", \"rogers\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"roswell\", \"rubbish\", \"rubbish\", \"rubbish\", \"russian\", \"russian\", \"sadako\", \"saif\", \"salman\", \"sandler\", \"sanjay\", \"sarandon\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scares\", \"scary\", \"scary\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"school\", \"school\", \"school\", \"school\", \"scifi\", \"scorsese\", \"scorsese\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"script\", \"script\", \"script\", \"script\", \"script\", \"script\", \"script\", \"seagal\", \"season\", \"seasons\", \"sebastian\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"self-indulgent\", \"sensitivity\", \"sequel\", \"sequel\", \"sequel\", \"sequel\", \"sequel\", \"serial\", \"serial\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"seth\", \"sex\", \"sex\", \"sex\", \"sex\", \"sex\", \"sex\", \"sexual\", \"sexual\", \"sexual\", \"sexual\", \"sexual\", \"sexual\", \"sexual\", \"sexuality\", \"sexuality\", \"sharpe\", \"shelter\", \"sheriff\", \"sheriff\", \"sheriff\", \"sherlock\", \"ship\", \"ship\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shyamalan\", \"silent\", \"silent\", \"silent\", \"silent\", \"silent\", \"silent\", \"sinbad\", \"sing\", \"sing\", \"singapore\", \"singer\", \"singer\", \"singing\", \"singing\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sitcom\", \"sitcom\", \"sitcoms\", \"slasher\", \"snipes\", \"snoopy\", \"social\", \"social\", \"social\", \"society\", \"society\", \"society\", \"soldier\", \"soldier\", \"soldiers\", \"soldiers\", \"solondz\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"son\", \"son\", \"son\", \"son\", \"song\", \"song\", \"song\", \"songs\", \"sons\", \"sopranos\", \"soundtrack\", \"soundtrack\", \"soundtrack\", \"soundtrack\", \"soundtrack\", \"soviet\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"squid\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stan\", \"stanwyck\", \"stanwyck\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stiller\", \"stinker\", \"stinks\", \"stories\", \"stories\", \"stories\", \"stories\", \"stories\", \"stories\", \"stories\", \"stories\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"storytelling\", \"storytelling\", \"streep\", \"struggles\", \"struggles\", \"struggles\", \"stupid\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"subtitles\", \"subtitles\", \"suck\", \"sucks\", \"sung\", \"superman\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"supposed\", \"supposed\", \"supposed\", \"supposed\", \"supposed\", \"supposed\", \"swanson\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"tarzan\", \"television\", \"television\", \"television\", \"television\", \"television\", \"terrible\", \"terrible\", \"terrorist\", \"terrorist\", \"terrorists\", \"terrorists\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"thriller\", \"thriller\", \"thriller\", \"thurman\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tina\", \"tokyo\", \"town\", \"town\", \"town\", \"town\", \"tragedy\", \"tragedy\", \"tragedy\", \"tragedy\", \"translated\", \"translation\", \"translation\", \"travolta\", \"tripe\", \"troy\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"tucker\", \"tunes\", \"turmoil\", \"turns\", \"turns\", \"turns\", \"turns\", \"turns\", \"turns\", \"turns\", \"turns\", \"turns\", \"turtles\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"u.s\", \"u.s\", \"u.s\", \"u.s\", \"uma\", \"underlying\", \"union\", \"united\", \"unless\", \"unless\", \"unless\", \"unless\", \"unless\", \"unsatisfying\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"valet\", \"vampire\", \"vampires\", \"vegas\", \"vegas\", \"version\", \"version\", \"version\", \"version\", \"version\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"vincent\", \"vincent\", \"violence\", \"violence\", \"violence\", \"violence\", \"violence\", \"violin\", \"violin\", \"visitor\", \"visual\", \"visual\", \"visual\", \"visual\", \"visually\", \"visually\", \"visually\", \"visuals\", \"visuals\", \"visuals\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voiced\", \"wakes\", \"wally\", \"walsh\", \"walter\", \"walter\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"war\", \"waste\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watching\", \"watching\", \"watching\", \"watching\", \"watson\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"welles\", \"werewolf\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"west\", \"western\", \"western\", \"western\", \"westerns\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wilbur\", \"william\", \"william\", \"william\", \"william\", \"william\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"women\", \"women\", \"women\", \"women\", \"women\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worse\", \"worst\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"woven\", \"wwii\", \"yeah\", \"yeah\", \"yeah\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"zohan\", \"zombie\", \"zombies\", \"zombies\"], \"Freq\": [0.9968602338588636, 0.9968751103322047, 0.9981260511946324, 0.9932497672855043, 0.9963009492777657, 0.9949723530152285, 0.5516680552601666, 0.18782930435492876, 0.16189283912488642, 0.012380040638961903, 0.06963072630873142, 0.016581411896528158, 0.07982303346443534, 0.18787025008579739, 0.07429458521720227, 0.003685632164822045, 0.575928520913508, 0.07691332386062846, 0.0013578644817765428, 0.21168827098907594, 0.02995147238754796, 0.2879709272261121, 0.05054310965398718, 0.053975049198393715, 0.009827826877164174, 0.0035879367964250157, 0.3525537895617624, 0.10927190258279275, 0.04661033132768843, 0.21885248285649733, 0.020372727600181696, 0.12285372098291387, 0.03765867829124496, 0.04352355441856999, 0.4006636428035734, 0.9927738751456269, 0.07412331905434133, 0.005294522789595809, 0.7015242696214448, 0.20560396832930392, 0.012353886509056888, 0.9961446580536097, 0.9983792401495244, 0.9975705554829769, 0.9927629433107947, 0.9960036847965801, 0.0815716298350952, 0.8317392970685599, 0.085941538576261, 0.9975191618890876, 0.31090227984981234, 0.02191120829417725, 0.6668113659255023, 0.9694855611378811, 0.029490055091646573, 0.0367362198439749, 0.24214860404197536, 0.2578557402931719, 0.06685696347862231, 0.06120239442819157, 0.13844454681642854, 0.08267497363276845, 0.05990886556044598, 0.039877647094214194, 0.014302733480501291, 0.054926921109249065, 0.1587266938668079, 0.4552660234626335, 0.050303813520895434, 0.08497712043354765, 0.0484325556875142, 0.059660102687801586, 0.04369937410896168, 0.033462493020464355, 0.010457029068895111, 0.9915455576935042, 0.07638416817943885, 0.006622904755442675, 0.08742234277184331, 0.1483530665219159, 0.0022076349184808916, 0.6786269739410261, 0.06324779428047, 0.1824752989020895, 0.11304190126900629, 0.10221709540193319, 0.04809306606656766, 0.4792296197437079, 0.011598006286149756, 0.0075576422816369185, 0.013225873992864607, 0.059516432967890734, 0.06707407524952765, 0.8511794619693579, 0.9953468133317235, 0.9971962874543677, 0.9991801741963866, 0.07058064437018938, 0.929484809316244, 0.818104002082668, 0.04060246244554817, 0.10471161367536107, 0.00427394341532086, 0.02101355512532756, 0.011041020489578888, 0.9959750581191864, 0.7080669794015906, 0.00918374811156408, 0.27183894410229675, 0.010102122922720488, 0.9945693730680292, 0.9967026176567487, 0.00162746452302699, 0.12205983922702424, 0.8763896456500341, 0.360781819374887, 0.1155175830587988, 0.06712002192314323, 0.14706492948571548, 0.05607377005852551, 0.06843059417826736, 0.15240083081014946, 0.026398669710357585, 0.001872246078748765, 0.00430616598112216, 0.09664825913968289, 0.6330628185170233, 0.1993161330354706, 0.023744035982413446, 0.02641942031846003, 0.020399805562355214, 0.9986586918593671, 0.8386921195434398, 0.16039221556232208, 0.016330633163400995, 0.07757050752615473, 0.9053294759960426, 0.9976881862325542, 0.9530921300784834, 0.042956265017621786, 0.16874706017615615, 0.5161103758024335, 0.07167300830109029, 0.02248882201772359, 0.21776945637306439, 0.0006471603458337723, 0.0024268512968766464, 0.9286441169313164, 0.06927718101441437, 0.9943688289716326, 0.8424393531926933, 0.048229540284439903, 0.01757899131862763, 0.06175184129876885, 0.01757899131862763, 0.00991635407717456, 0.002704460202865789, 0.9835674413773506, 0.006304919496008658, 0.009877707210413565, 0.0943682331291871, 0.03019783460133987, 0.050149975320082286, 0.09652522131499708, 0.7290620068037769, 0.9975105762390744, 0.32356328430520764, 0.019906175969211685, 0.1937862713610537, 0.11465394639510618, 0.029753754187196266, 0.22916721310138402, 0.07863994605504829, 0.0016881562659402138, 0.008792480551771946, 0.9065621499790814, 0.00014890968297948115, 0.03477041097570885, 0.0008190032563871464, 0.05766527473380408, 0.9962810750922998, 0.9991683388190458, 0.05119447979248432, 0.07368902394372744, 0.08067008936997529, 0.7942901107197567, 0.9982142426333087, 0.9980873511311449, 0.005178151717293144, 0.07352975438556264, 0.19521631974195153, 0.0031068910303758863, 0.6462333343181843, 0.07663664541593852, 0.9951471008079771, 0.9971684014598567, 0.047482150336312395, 0.13848079257327636, 0.4332085275115896, 0.010627964607571037, 0.04594876240785687, 0.08333170259468634, 0.008777324004262647, 0.01882365870793676, 0.17898338406282568, 0.034263288884109615, 0.4340312194117205, 0.18197921405365727, 0.25928683143778547, 0.00029172685805331397, 0.0010502166889919304, 0.07596567383708296, 0.0004667629728853024, 0.042592121275783844, 0.0043175574991890465, 0.17414931967398883, 0.04490434408339769, 0.04589853620332531, 0.0946139500797789, 0.21159722285792934, 0.15293988778219952, 0.06843355758835146, 0.20728905700490963, 0.9975453144896852, 0.08874181835951112, 0.029682608209905443, 0.8534514876023328, 0.02815257685887939, 0.9983839907547338, 0.02490765081485327, 0.059962863072794914, 0.0581178519013243, 0.05627284072985369, 0.2776741813063272, 0.5230606671119187, 0.08295017161618533, 0.6668711929724442, 0.15953279893818215, 0.009465268545415339, 0.08088502211536744, 0.7929794245292825, 0.12389154662325833, 0.02316073423520853, 0.059923804449825244, 0.07329128913658305, 0.8458677016822114, 0.011209255985595053, 0.0612197826905576, 0.008622504604303888, 0.895988968901382, 0.09063242453838292, 0.012720340286088832, 0.000795021267880552, 0.9979281348251038, 0.16794552585817363, 0.051366317749175446, 0.002009986346706865, 0.0015633227141053396, 0.7479382527912546, 0.029256467935399925, 0.9692801233247893, 0.027933144764403152, 0.09620452332273219, 0.903485958161311, 0.07356576367204339, 0.04208646014726203, 0.2378056081491635, 0.005132495139910004, 0.6405353934607685, 0.0010264990279820008, 0.43708285809455155, 0.10358355757773859, 0.45938976495031186, 0.1918596969364515, 0.8063668421966802, 0.9966866243055675, 0.02770772553120799, 0.9697703935922797, 0.9979437164229665, 0.6161760873476066, 0.08084215312538749, 0.2096288433066406, 0.015327298396027223, 0.015514216669149506, 0.0018691827312228321, 0.055701645390440396, 0.00467295682805708, 0.0001869182731222832, 0.9972789362460711, 0.8703513748035692, 0.1295165736314835, 0.9704947996445056, 0.026588898620397414, 0.3155658213200763, 0.575406306968915, 0.019841596792031554, 0.0004221616338730118, 0.05108155769863443, 0.03736130459776155, 0.9933125422700528, 0.9988706364589858, 0.9991041784584311, 0.12931461033453837, 0.7530674366540764, 0.056627966617085426, 0.06000874074347859, 0.25524680355526413, 0.7249595995230549, 0.01965693774506057, 0.07897415073454535, 0.0989594460224711, 0.12410223686857126, 0.07961883767931714, 0.01643951709168087, 0.009347960699191082, 0.0499632382198144, 0.5273539208233313, 0.01515014320213727, 0.9973067625439584, 0.9966344791079603, 0.004462551118611615, 0.9951488994503902, 0.9982974197487937, 0.9189670929655114, 0.07984706179005233, 0.9966805618395909, 0.05514235447208875, 0.08733837434127605, 0.3135109669582465, 0.09676593817037508, 0.002846057005011032, 0.1117966767280896, 0.0026681784421978424, 0.3298757947370599, 0.9989180312779987, 0.997071230466285, 0.9988551347912119, 0.9985218917365549, 0.13465184108436845, 0.32985051147493966, 0.1924576659919072, 0.15354519283230236, 0.17703951624942227, 0.0018110207634029925, 0.00024473253559499896, 0.010376659509227957, 4.8946507118999797e-05, 0.1758233409132418, 0.4966962964922388, 0.2092427721322318, 0.004641587669304166, 0.10332174151871074, 0.010211492872469165, 0.01748889656954887, 0.26557213309314953, 0.001295473819966583, 0.0382164776890142, 0.06412595408834586, 0.6127591168441938, 0.22366657425639197, 0.048437347291193165, 0.7158755004066049, 0.012109336822798291, 0.09786770556020319, 0.05514767535535259, 0.7604165376463407, 0.027185473766723107, 0.05903131446488446, 0.9985418727643616, 0.06452029274672061, 0.05674329317457125, 0.07258532934006068, 0.09390006890817373, 0.7123155534046431, 0.04846802145334653, 0.06607197264159032, 0.10333747970241808, 0.03269305220673846, 0.7485108596144177, 0.0006858682281133943, 0.9928667629556801, 0.09189600885958618, 0.6377482582058713, 0.01832898537363331, 0.14838945692900393, 0.08386138513415789, 0.01958439533073148, 0.017109976952892427, 0.6983009343899222, 0.01639706124652191, 0.04740889447363944, 0.09553070465364939, 0.12547316432121114, 0.9966771357015235, 0.9939954915901553, 0.9963950832894322, 0.8702557840680628, 0.12950234881965222, 0.11898309801217119, 0.7182659175434829, 0.016936716726889185, 0.009799861250778648, 0.13591981473906037, 0.995774523547101, 0.995248160254103, 0.9958506899694136, 0.08611134241398015, 0.004415966277640008, 0.001103991569410002, 0.7319464105188314, 0.11923108949628021, 0.0563035700399101, 0.9955285238693842, 0.9985360937072262, 0.9973702868518785, 0.9921166163032706, 0.9955726986381387, 0.9932362713157536, 0.9436596263990472, 0.055810327830402004, 0.9978971262505929, 0.09987153690312525, 0.8999535158714953, 0.9973438263509323, 0.5724573244129069, 0.22057148446182767, 0.15184983543834862, 0.00021134284681746502, 0.02560770827271618, 0.00014089523121164335, 0.006340285404523951, 0.006445956827932683, 0.01634384682055063, 3.522380780291084e-05, 0.05673207851594531, 0.04009066881793469, 0.02458390069024297, 0.00907713256255125, 0.12065022031057702, 0.1221630757376689, 0.6263221468160363, 0.9997751346158695, 0.9501596886180637, 0.04951536405474416, 0.9990325280751498, 0.006222969842159522, 0.9355197996046482, 0.05704389021979562, 0.10227261580953048, 0.10732311535568013, 0.7777769301070466, 0.012626248865374135, 0.029555354994335706, 0.8427381083801556, 0.07676182477695524, 0.05131138019849949, 0.8099123936437529, 0.02560067910942897, 0.16524074697904154, 0.995411811205547, 0.14522858767239125, 0.006224082328816768, 0.06639021150737885, 0.05601674095935091, 0.7261429383619562, 0.016602785271112454, 0.186320145820262, 0.00368950783802499, 0.6911678016566815, 0.04611884797531237, 0.05595753554337901, 0.05043070503514812, 0.10697422280182933, 0.8412758521772435, 0.9974009675083673, 0.9957588627618991, 0.9956822933340401, 0.06331561012021147, 0.9364680239684611, 0.09973285207214398, 0.8993152005815742, 0.13798907335830227, 0.8617499442929549, 0.9977667539127337, 0.08269229012435063, 0.13814862754901436, 0.5703142866512754, 0.022313792573237474, 0.1863857967882189, 0.2136782694162191, 0.20814903206070756, 0.4385933760388003, 0.13947947135516137, 0.9783028832978055, 0.019410771494004078, 0.09715091513276071, 0.02395039921675698, 0.24878055524448273, 0.14522037834949128, 0.2609244196360778, 0.11739068911875253, 0.10558415429359064, 0.0008433239160829923, 0.1728340131386301, 0.008286562273769935, 0.7730178806816811, 0.03788142753723399, 0.008286562273769935, 0.9783843912921172, 0.020214553539093332, 0.9969633032571363, 0.9955791627042259, 0.04236667571975025, 0.9532502036943806, 0.032233300198299196, 0.7762853131090389, 0.03402403909820471, 0.012535172299338575, 0.008953694499527555, 0.0017907388999055108, 0.10654896454437789, 0.026861083498582662, 0.9993847711866173, 0.9941502416622757, 0.9970237199746277, 0.1632666409387684, 0.7509184247150639, 0.0416275872592224, 0.01405606842519198, 0.027030900817676885, 0.0027030900817676885, 0.07179981946489568, 0.5126266440007635, 0.26781198954969465, 0.08075808371843014, 0.028345552563422503, 0.03783863856343664, 0.0006685271830995873, 0.996014141992951, 0.9973644430675366, 0.15896506312098174, 0.48313074361716785, 0.05861420998969254, 0.05520544033071751, 0.1048404521943295, 0.0005819850637274436, 0.13867872661391087, 0.07535121108696899, 0.0037992207270740666, 0.9200446194064364, 0.044350931506470376, 0.581230628690059, 0.07352917591862193, 0.19666136733790152, 0.07236204614213587, 0.03151250396512369, 0.9951013109160417, 0.9965720678521496, 0.10445157835029203, 0.0010550664479827478, 0.8936412814413874, 0.9912405704504988, 0.021105542264268147, 0.6653042527395436, 0.14869813868007103, 0.13670635330264597, 0.027820942075626198, 0.99865012454006, 0.9942152371343408, 0.9965741091250245, 0.18497786691034793, 0.09699878552612969, 0.5370420564495473, 0.1546657464334324, 0.00635815210003594, 0.019813776311739906, 0.4256070950663619, 0.0572846563285955, 0.022824355255924772, 0.027747255409163448, 0.46633290542497274, 0.00044753637756715235, 0.30114416407640293, 0.1448503429207498, 0.008883752840253886, 0.5450709369782892, 0.7114453523971177, 0.012421381829931615, 0.0025699410682617136, 0.008138146716162092, 0.26513225354233344, 0.30924583454468624, 0.0412327779392915, 0.6472461063365099, 0.0010850731036655657, 0.2800495779464718, 0.030227573492635048, 0.6845656349802643, 0.005334277675170891, 0.36947240611816384, 0.27227323620280325, 0.11138742391225269, 0.09450275965350777, 0.03293472534120209, 0.01656366303709579, 0.09315455452258135, 0.008667032984526866, 0.0010272039092772583, 0.10130099365446092, 0.0013689323466819043, 0.016427188160182852, 0.8596895137162359, 0.02190291754691047, 0.993535224083473, 0.010786020572770984, 0.26344855248993126, 0.6741262857981865, 0.051233597720662175, 0.9955074094031249, 0.012925374061359332, 0.9382864133431219, 0.04858983211955453, 0.9995442871247471, 0.9989084120974475, 0.5299426849358496, 0.1866642679427142, 0.11530543357766729, 0.008620712840594465, 0.04095518466225635, 0.05487885966031429, 0.008511934129672138, 0.01552816098416227, 0.030702791157826974, 0.008865464940169702, 0.5402353897675505, 0.07139194019573306, 0.2768950472112865, 0.04784351261709223, 0.024180214660019003, 0.00011487037843239431, 0.006834787516727461, 0.02326125163255985, 0.009189630274591545, 0.9965706348569793, 0.3987570285575963, 0.12605757600732886, 0.2629176568411967, 0.005018486494218896, 0.17003312715158597, 0.01531063676202375, 0.005954136518564791, 0.007910495660378937, 0.0016161227693247291, 0.00654955017042127, 0.14126035091289021, 0.00077332308893918, 0.00077332308893918, 0.06212362147811413, 0.5258597004786424, 0.08970547831694489, 0.13172269948264034, 0.04743048278826971, 0.9958443909080359, 0.0006192544469478838, 0.18961571165544203, 0.13784603989059893, 0.671891074938454, 0.19634370294956502, 0.07413515600158464, 0.51507204585424, 0.18542593650752642, 0.028879253169263375, 0.08145226393352614, 0.2631169342225859, 0.48610141187151024, 0.06981622622873669, 0.09926252572657122, 0.06762267201383235, 0.007513630223759149, 0.03381133600691617, 0.8884867739595194, 0.9949548573414831, 0.05940234879743458, 0.05775228355306139, 0.8826015651569445, 0.0034459447630062414, 0.7500673100810252, 0.08241551224856594, 0.061452681606944635, 0.060591195416193074, 0.04163849921965875, 0.26150745141068465, 0.41164919646156756, 0.19745774464416654, 0.12729157939363864, 0.002077287787022208, 0.12853726419507713, 0.05765274349926253, 0.6804913986798201, 0.13231777196552058, 0.9959758440343408, 0.8188706634316322, 0.07753426547442364, 0.10337902063256486, 0.9958538268941726, 0.20369918726033273, 0.5044550460976476, 0.12581420389608786, 0.1659549260915064, 0.1340175263200585, 0.5608692926601181, 0.05658800873704371, 0.0055543385433028045, 0.04581904654605295, 0.09664888780242495, 0.004908880241634588, 0.005257088009639811, 0.09032169787159836, 1.6985744780742522e-05, 0.05881160427048064, 0.4900325324730228, 0.09230822994335669, 0.002069475436686307, 0.23779716587597777, 0.00389831419468816, 0.11507245974690607, 0.2644230839773886, 0.1977009885015904, 0.21591238285672484, 0.07896905936527726, 0.02346009316491117, 0.029583575109650696, 0.18092105745821327, 0.00906593430779618, 0.47608881129575165, 0.010242266711852223, 0.5131728804248717, 0.044240759606274355, 0.2505199640355295, 0.14631431942075074, 0.04877143980691691, 0.05410165180767286, 0.03811101580540502, 0.024518975203477355, 0.024252464603439557, 0.36805113865219813, 0.0010660424001511892, 0.24789446364799111, 0.22782644619547096, 0.3082929735390062, 0.013767593368589402, 0.0033057780687290937, 0.08171105555764502, 0.06109855701145184, 0.034107851367946064, 0.019212404305319675, 0.0028390799884379274, 0.9959173847770227, 0.37908086774968497, 0.10613113823947933, 0.008053311302724719, 0.5064957572892225, 0.9986167904378122, 0.7293257638585303, 0.040555649519473415, 0.007435202411903459, 0.2203523623891389, 0.0006759274919912236, 0.0013518549839824472, 0.06783063068361901, 0.9320431105045427, 0.222188482306537, 0.12200835998425144, 0.14475568133724748, 0.01125877521511925, 0.4861952725549455, 0.009420607833058963, 0.0041358766096356425, 0.9983494386063512, 0.13774651504875512, 0.521743783257296, 0.005386735225370312, 0.2607436360280439, 0.0002565112012081101, 0.07400348154853977, 0.13231847237786526, 0.866414106803008, 0.32059894363898805, 0.6793900433155066, 0.9954360994988711, 0.09067863802214648, 0.004121756273733931, 0.04024773773175486, 0.0016971937597727953, 0.02424562513961136, 0.012365268821201795, 0.0029094750167533634, 0.0004849125027922272, 0.8233814297412019, 0.05050576414980927, 0.07242335991293404, 0.01905877892445633, 0.02668229049423886, 0.040976374687581106, 0.027635229440461675, 0.7633040959244759, 0.7578848653048502, 0.05598682908550604, 0.014733376075133168, 0.10549097269795349, 0.06600552481659659, 0.9994709941563953, 0.1541183232343744, 0.8452918238620025, 0.36440310815804905, 0.07574823332164345, 0.009739058569925586, 0.0013526470236007759, 0.5486336327724747, 0.07982206436596853, 0.06588487852429149, 0.11360918155791289, 0.1427505701359649, 0.5976096353325158, 0.018354539060172256, 0.1571148543550745, 0.008810178748882682, 0.815675715834055, 0.9951081361807738, 0.5071511235050803, 0.0745960602941333, 0.18062017727132768, 0.10613349536472241, 0.003791784100972563, 0.020891272017858446, 0.0967998729623284, 0.005869973464005602, 0.004156378726066078, 0.26295893180246527, 0.006495486170501406, 0.09050377397565293, 0.3315945690040968, 0.06625395893911434, 0.22452730529366527, 0.0012990972341002812, 0.005304647039242815, 0.01115058459269408, 0.9979575332219334, 0.9984833697725026, 0.21982806778392644, 0.010852729621806572, 0.03593459363664842, 0.0019293741549878348, 0.04292857494847933, 0.6885454015612835, 0.4741497391780338, 0.07806541672792433, 0.17977663571320943, 0.07271940926104277, 0.017820024889605202, 0.006579701497700383, 0.1612026866936594, 0.009389782345676587, 0.0002741542290708493, 0.26394761928855603, 0.10063600383635066, 0.05175175773945305, 0.2650400046761962, 0.008192890407301274, 0.049430438790717686, 0.2564374697485299, 0.0030040598160104674, 0.001638578081460255, 0.9943083243559927, 0.31838764798700164, 0.17574608703869662, 0.35763552100593227, 0.009759807761943183, 0.0005100137072749407, 0.11268984686652213, 0.007997942227720661, 0.0065838133120946895, 0.010246639027978354, 0.00039410150107609056, 0.9998171511603596, 0.9987757570351868, 0.9995564937075598, 0.997655492021871, 0.17662327245380302, 0.0072225129999696405, 0.7248776683605894, 0.09060970854507368, 0.0025771300302740254, 0.18249762228669061, 0.6041897275261005, 0.0019880717376399622, 0.007878654663980592, 0.08114277981034217, 0.007105515654898384, 0.03810470830476594, 0.06612179334817356, 0.008394080670035397, 0.0369964512292809, 0.9599605503176569, 0.993965100061396, 0.660893848408251, 0.11092991552837585, 0.1844836414435563, 0.020268137807738614, 0.02342822381002044, 0.9982649738524325, 0.9978944125353414, 0.9948224404860522, 0.9970243694495506, 0.9971340995070532, 0.9986026103374471, 0.9976633552413855, 0.9961178959979196, 0.9947641151521514, 0.9965998785909529, 0.9989409139345843, 0.9974190802342823, 0.1390223713866096, 0.727823003141662, 0.05495472563047156, 0.0552818370925577, 0.010467566786756487, 0.012430235559273329, 0.9967545051268997, 0.060260205697643114, 0.2574754243444751, 0.08347734122617229, 0.062086272536965637, 0.5235594495028992, 0.013304201257921207, 0.00026086669133178837, 0.9973641546864466, 0.13700904646802084, 0.1963345433166561, 0.1661886882404766, 0.11981044966173895, 0.0003864853214894805, 0.11807126571503629, 0.26184380530912305, 0.0003864853214894805, 0.999226759376906, 0.18070386924523563, 0.01402808382291921, 0.05537401509047057, 0.08250728248480114, 0.008490682313872153, 0.5995160033794947, 0.055743175191073704, 0.0035070209557298026, 0.9960006171997582, 0.9979789463907202, 0.0765862591993382, 0.08271315993528526, 0.07199108364737791, 0.7673943171773687, 0.9996895895343942, 0.004635096713252124, 0.9953623644011422, 0.13356988431918598, 0.19791741983061867, 0.20747205389140713, 0.4609623453000813, 0.9947685021186589, 0.06209228650514599, 0.0966865604151559, 0.0035481306574369135, 0.007096261314873827, 0.8302625738402378, 0.09437697009211825, 0.47597485999871125, 0.12852854973542, 0.0640086492715775, 0.022188301744300823, 0.09580847343046024, 0.05204537137257659, 0.026073810805514793, 0.040082093473575685, 0.0009202521460769927, 0.9958644929843992, 0.07290653904242754, 0.131463219606663, 0.0009257973211736831, 0.5765402817609112, 0.11549321581641697, 0.030319862268438122, 0.07221219105154729, 0.09445251318811457, 0.010078059719823639, 0.7080422886880747, 0.002343734818563637, 0.10312433201680003, 0.0820307186497273, 0.9956046795791118, 0.10189928178029611, 0.011499605467368841, 0.816152554697983, 0.0702753667450318, 0.9593834158526311, 0.023646774334395836, 0.016890553095997025, 0.9704468586350552, 0.016877336671914003, 0.012658002503935502, 0.8998712064441976, 0.09972799452860952, 0.8303190981306378, 0.07026358155245616, 0.0717746263170251, 0.02719880576224109, 0.9929703505187202, 0.15163308672561165, 0.601879727783948, 0.12308292395300693, 0.03320278189110325, 0.0037009470260783873, 0.06894335602866024, 0.013323409293882194, 0.004229653744089586, 0.9975104043604422, 0.007264871619139338, 0.17363043169743017, 0.01888866620976228, 0.7998623652672411, 0.2582374950528448, 0.5040960955359128, 0.010315745474814042, 0.2272902586284027, 0.06428704389438286, 0.030857781069303775, 0.10285927023101257, 0.0008571605852584382, 0.7757303296588866, 0.025714817557753143, 0.9964711679571941, 0.9995168405443815, 0.10423696040278481, 0.6629893978918242, 0.04180060848639594, 0.14339196075713037, 0.04709182475049669, 0.13126818508552587, 0.1580576106131842, 0.10686004182699271, 0.00014883014182032412, 0.15522983791859804, 0.4484252173046366, 0.3604101656011126, 0.6395408873277904, 0.9969483245801637, 0.9959479753422104, 0.9700799848294515, 0.02823517627362229, 0.9978783466747106, 0.9983811772654093, 0.9979702728471763, 0.09600780432530162, 0.13412855016034786, 0.019766312655209155, 0.7497080014225759, 0.9275786555757758, 0.07156393188418449, 0.9979128843798277, 0.9958984042019046, 0.2530096786469629, 0.37034359145232415, 0.18206986469797434, 0.06689381695570021, 0.12758377185502498, 0.04832954177626797, 0.49568138073046697, 0.45585200961836675, 0.9993257546022024, 0.5729807820369367, 0.12161021239325819, 0.21795652644878255, 0.004376859580440727, 0.01296436888383709, 0.057120787689042905, 0.012908965598008727, 0.9985895142913526, 0.994540196915925, 0.9971798228369717, 0.9985121384796867, 0.9948609074525067, 0.9356885836664728, 0.0009721439830301015, 0.06367543088847165, 0.9986113123296528, 0.9971420395604919, 0.9965599276775482, 0.9948375925862677, 0.9963037063396888, 0.9966257693500659, 0.992963384672106, 0.9959089932807705, 0.9974054558112875, 0.141310030130699, 0.05034921624843309, 0.09325201826217186, 0.48762557215348223, 0.22562635131123768, 0.0018329635039247539, 5.728010949764856e-05, 0.4990161005898103, 0.15934710187850942, 0.23196657880389515, 0.007820821189289546, 0.014057031549376634, 0.048679926226144296, 0.023854356568639136, 0.0076845105803258935, 0.0006645142186978045, 0.006917763404905349, 0.1832365338971935, 0.15516916942726225, 0.6288915160904922, 0.030121074065292082, 0.002510089505441007, 0.1853335992990429, 0.2729976878066461, 0.18300072881835563, 0.06941585719200516, 0.0006220987948499342, 0.08771593007384072, 0.19093248845269228, 0.0009331481922749013, 0.0010368313247498902, 0.007879918068099167, 0.05064278181218412, 0.044223274258526984, 0.011174698334143916, 0.00023775953902433863, 0.5425672680535407, 0.3243040112291979, 0.02639130883170159, 0.9911686376319944, 0.9975326559977293, 0.995149636424494, 0.06426892811088696, 0.0636197470188578, 0.3917807890395988, 0.2236969846283818, 0.00037868897035034404, 0.2515035747369642, 0.0039491849765107305, 0.000865574789372215, 0.023195444623299095, 0.08118405618154684, 0.7926791075614387, 0.024960315409854463, 0.07790643900651545, 0.9968216957257618, 0.9974801656659382, 0.9050320931575667, 0.09420982539694807, 0.9960184224174088, 0.40633569166481526, 0.20981241051447316, 0.22167306668962597, 0.005973615153909077, 0.045321558450310176, 0.00043287066332674474, 0.06449772883568497, 0.04471553952165273, 0.0011687507909822108, 0.9980580142364974, 0.2879090205841004, 0.4611090261249566, 0.13274121159561683, 0.010455643379106804, 0.06697673005166967, 0.04015573181830874, 0.000606124253861264, 0.5021098417348959, 0.2129574210992047, 0.1404792508084004, 0.01904874988412164, 0.046206945395186425, 0.02078045441904179, 0.03585895488163919, 0.011995221656520057, 0.009629966681994976, 0.0008869706154469057, 0.23965713812785627, 0.21422991843227476, 0.17277087400194244, 0.07844409126026906, 0.13414536138519398, 0.08545335416755546, 0.04302494359047077, 0.006636429773920102, 0.023041087642037206, 0.002609831933564085, 0.09499028632569023, 0.004983298699728568, 0.022332560839524323, 0.1973017151855496, 0.2149585883561928, 0.06804356002345427, 0.2619000316635125, 0.1064334166732151, 0.029100003518168056, 0.9939098660845382, 0.14420600781755086, 0.3799593763167578, 0.26717706310233186, 0.00025219658589987906, 0.08796616916187781, 0.045849339316598015, 0.007465018942636421, 0.06143508832521054, 0.0055987642069773155, 0.00010087863435995163, 0.9940408395522263, 0.0011158265452386996, 0.017295311451199842, 0.05244384762621888, 0.874250098194521, 0.055233413989315625, 0.9974019163156843, 0.9984483294640297, 0.9953636159318542, 0.9987643203213661, 0.9643070677354196, 0.03403436709654423, 0.09825989388059998, 0.027294414966833328, 0.07408484062426189, 0.005458882993366666, 0.7151136721310332, 0.07798404276238094, 0.1335502527292185, 0.35404465182265193, 0.1509390820020615, 0.0338677779442, 0.1912771220012728, 0.07784534034935522, 0.015973459448309253, 0.016883340049795224, 0.01991627538808179, 0.005762577142744477, 0.7683936064136914, 0.04433040037002066, 0.12862059825667965, 0.021228642430714116, 0.025807369229495593, 0.004370602853382318, 0.007076214143571373, 0.288080145261103, 0.017937065648332827, 0.03804832107222115, 0.6555182173299815, 0.9969530663703554, 0.991825297621796, 0.09439081521462786, 0.00017512210614958787, 0.08721080886249476, 0.38649448827214045, 0.018562943251856315, 0.14797817969640176, 0.26513486871047603, 0.9977997509104876, 0.1492067642889573, 0.2452835234898586, 0.04648875445204901, 0.499864797870127, 0.010626001017611202, 0.04825975462165088, 0.011115293731118263, 0.18185855576746268, 0.44059789317516, 0.06545672974991866, 0.0015437907959886476, 0.07132313477467551, 0.2071767248216765, 0.020995554825445607, 0.9975353396472173, 0.9981374153722342, 0.9989890040281135, 0.994917849734904, 0.7263004173951364, 0.1756497132214086, 0.018186844054320864, 0.043648425730370075, 0.030624298697920938, 0.005514720455181165, 0.0017270598245875796, 0.9602452624706943, 0.036268256316339176, 0.06779193385024306, 0.00903892451336574, 0.9228741928146422, 0.7358322551471218, 0.00029510016248130013, 0.09885855443123553, 0.03216591771046171, 0.1057934082495461, 0.01947661072376581, 0.0075250541432731535, 0.10432568271373378, 0.8953823502632839, 0.08316343731001909, 0.8515935980545954, 0.06542190401721501, 0.0862553912966361, 0.09788533169618255, 0.01356826379947085, 0.761761096170292, 0.026167365898979493, 0.01356826379947085, 0.02518236444374542, 0.0283301599992136, 0.9443386666404533, 0.9946797910868779, 0.03898845208841619, 0.960830596754843, 0.9985291491898267, 0.44328872943352576, 0.0851153720221975, 0.36148315117362073, 0.001028719682874694, 0.10904428638471757, 0.07149528084593876, 0.005362146063445408, 0.14835270775532294, 0.1450332840017615, 0.06894187795858381, 0.0464719325498602, 0.005872826640916398, 0.47927372195652523, 0.029108792915846497, 0.009765862824344758, 0.016276438040574596, 0.06185046455418347, 0.06185046455418347, 0.8496300657179939, 0.9961009286629995, 0.2585044904522391, 0.3661588635733257, 0.21657817219446904, 0.02344911828546433, 0.02538851904591627, 0.07528401133754338, 0.014809969443451157, 0.010190669450374725, 0.00881545800205426, 0.0008462839681972089, 0.9640121266216972, 0.03586874228128146, 0.8023914946013865, 0.10649022922178894, 0.0903928689905883, 0.9984630408231071, 0.08846850746077442, 0.31345021744529067, 0.10304758734195073, 0.00033134272457218886, 0.0447312678172455, 0.44985297239417504, 0.09631281699397425, 0.9036538958512975, 0.998593482781879, 0.9979183167573658, 0.8278764253819626, 0.02956701519221295, 0.14245925501702603, 0.9844869942609872, 0.014351122365320513, 0.021336365761434446, 0.9769007466485343, 0.996743211281959, 0.9922650795206519, 0.03628613858398162, 0.9627922104283124, 0.9961510508735517, 0.3547838269947874, 0.18290614893162355, 0.22306642964129741, 0.024137785297006064, 0.0848463961625364, 0.009571880376398957, 0.06299753878162574, 0.037663268437569807, 0.019351845108806585, 0.0006242530680260189, 0.05481375212768896, 0.14558659853943512, 0.24821321718198774, 0.12156083200451194, 0.0891021805268674, 0.0539386414260858, 0.1208448323395639, 0.05027908758301803, 0.08178307284073186, 0.033811095289213076, 0.9948151440961239, 0.9986601867017126, 0.9980241719028828, 0.9985908951420636, 0.9977533905879487, 0.9975429127926779, 0.9924105262258455, 0.6563062483581004, 0.20606809636400927, 0.04905627255506846, 0.0279414367951199, 0.01698712350612403, 0.028973364858576035, 0.012303757679669274, 0.0016669607178906758, 0.0005556535726302253, 7.93790818043179e-05, 0.39832257512985314, 0.601555933630122, 0.9956750508975662, 0.24102727236841848, 0.03568607673415904, 0.7230818171052554, 0.11026868829359954, 0.20240107917048863, 0.08777967949687858, 0.11389594777694162, 0.013058134140031524, 0.036998046730089315, 0.09322056872189172, 0.09467147251522855, 0.24737909676393055, 0.08586267258079673, 0.9139855541824284, 0.9974805877824149, 0.997268689359998, 0.8928073701422374, 0.07954672934444319, 0.027736688521417693, 0.8264540752983078, 0.014711405612509434, 0.13469109138564195, 0.023865169104737526, 0.20090055147719799, 0.002219895596433127, 0.22361179104070614, 0.02774869495541409, 0.00042690299931406287, 0.08666130886075477, 0.3577447134251847, 0.0352621877433416, 0.04508095672756504, 0.020320582767349393, 0.2859859465383092, 0.16393671441308008, 0.21707677974313863, 0.04508775819893748, 0.0546627772554949, 0.09405972224157279, 0.049230050809658414, 0.03495785259328038, 0.05052048464477397, 0.00449070974620213, 0.07665671647936317, 0.24339590204181977, 0.30781786010004164, 0.27923399971790624, 0.02555223882645439, 0.06734530741548572, 0.99229799902482, 0.9956106984731782, 0.9965515502377141, 0.008542502874871133, 0.23735954416606217, 0.06589930789186302, 0.6876714814271261, 0.0006101787767765094, 0.9949513355610901, 0.9926985475060364, 0.9981458767522357, 0.9948490722460036, 0.021785513667793004, 0.05563086525882857, 0.02489772990604915, 0.8978743847368974, 0.013387338173346152, 0.11805198207405243, 0.07180481383885663, 0.74603984547829, 0.006085153715157342, 0.04381310674913286, 0.9944938097650254, 0.9138485441755867, 0.08525700703678943, 0.9932668839283243, 0.1303151822051582, 0.8671590519577811, 0.4954741084361221, 0.11085355253773241, 0.13203230114471265, 0.20892744993313317, 0.0001448119562870443, 0.004814997546544223, 0.0477155395965811, 0.09461805731784374, 0.1923819835490685, 0.10526561364005634, 0.18621124068051345, 0.009558601698349944, 0.41174584277828935, 0.00012099495820696131, 0.2411609477356561, 0.21973662663841104, 0.04852517171598244, 0.1571116880464639, 0.056399067503858834, 0.2768681495643979, 0.10025524803818503, 0.5203629106591167, 0.05143355410375825, 0.005022808017945142, 0.20673877801862203, 0.020091232071780568, 0.010447440677325896, 0.08398135006004277, 0.001808210886460251, 0.02675405856825563, 0.19036541673566507, 0.1239947714413386, 0.049392108126010394, 0.5361101736177378, 0.008232018021001732, 0.0030870067578756496, 0.041160090105008665, 0.02109454617881694, 0.07574280281967365, 0.2189151740032031, 0.04433725043102847, 0.08197772866153702, 0.07389541738504746, 0.07158618559176472, 0.029789090133347254, 0.09052188629668313, 0.3105916761965276, 0.002309231793282733, 0.9970226362280586, 0.9965593815395446, 0.9984285022124115, 0.15247553460993357, 0.07196845233588864, 0.15659237404440177, 0.02698816962595824, 0.07639024283957671, 0.0009148532076596014, 0.01661983327248276, 0.0033544617614185385, 0.3424600507339108, 0.15232305907532362, 0.018030252746254943, 0.004190129159340937, 0.2848018092242946, 0.18855581217034217, 0.03821905687762491, 0.023363144403597953, 0.12176769284266542, 0.04456773742208088, 0.2294413148766386, 0.04723418325075238, 0.14531784933649583, 0.019176842287807364, 0.18921151057303268, 0.12166641051486674, 0.04304135713485653, 0.006818432813442619, 0.059448211092202836, 0.008309964991383192, 0.26464042357174167, 0.14254786100603475, 0.14806410789923888, 0.350955915159186, 0.039288297937619815, 0.04339304548334129, 0.11610571629326454, 0.011288055750734053, 0.28205479564171837, 0.008942485724607496, 0.425993321267725, 0.3278205077721173, 0.05672907360901886, 0.07621656093211701, 0.11272277033792082, 0.0004727422800751572, 0.9959055248370157, 0.9476218073352518, 0.0520936042799487, 0.0024066285274350367, 0.27616062352317045, 0.7207852439667934, 0.9961904093422664, 0.9970821564438428, 0.0027933110476590613, 0.958105689347058, 0.013966555238295306, 0.02234648838127249, 0.06843037858089852, 0.1255664228329109, 0.5740179329504498, 0.12224455979500319, 0.10962148025095395, 0.9992890598331734, 0.10394269688905654, 0.0965182185398382, 0.0043871917518108275, 0.2520947875848222, 0.06850768658596908, 0.03982220205489828, 0.39417230508577283, 0.04083463092070078, 0.2604108680649347, 0.008844142688997782, 0.5453887991548632, 0.017196944117495686, 0.021619015461994576, 0.14592835436846338, 0.018438000174480107, 0.979262675933499, 0.9995071435978617, 0.3955985218807726, 0.055273056025329954, 0.1990402794694524, 0.06453296351143877, 0.26290500223447094, 0.020524537211478306, 0.002004722239260672, 0.3627547625226799, 0.06922800811501525, 0.567669666543125, 0.6747033779312823, 0.07758514141458528, 0.24769774777545375, 0.18083114104734857, 0.3300511922916319, 0.01767079554208618, 0.14097367999130975, 0.023953745068161267, 0.272326593520817, 0.03416353804803328, 0.9980339105003749, 0.9971365105950352, 0.9965210164608997, 0.8967756329189135, 0.10124886178116765, 0.05095113792066293, 0.4445328409703309, 0.1883291616570152, 0.04696916621815996, 0.0020814852081265495, 0.17104378449387733, 0.03574724596565161, 0.0023529832787517516, 0.05638109933316697, 0.0016289884237512128, 0.995473493179625, 0.12446338143791429, 0.49773150282867884, 0.04844310042240389, 0.08077917501166594, 0.07199352455722494, 0.11018669944944764, 0.008541604608484315, 0.021354011521210785, 0.03575271643265577, 0.0008541604608484314, 0.2512459280615087, 0.20121026590081734, 0.19195758299770227, 0.029110363902877356, 0.19558748167507817, 0.02007120445137265, 0.054163782225158115, 0.056512540192871934, 0.00014234896774023158, 0.14130424025015373, 0.1435006792177727, 0.12849167960570973, 0.5582615709364882, 0.00439287793523794, 0.02342868232126901, 0.42843087435318483, 0.18129147157071318, 0.32660805230812484, 0.0010699927337879068, 2.891872253480829e-05, 0.059659324589309504, 0.0029207909760156376, 0.9940685890113542, 0.9651233329622108, 0.03308994284441866, 0.9947280725132152, 0.0812185536771557, 0.04500422021937632, 0.0003515954704638775, 0.565365516505915, 0.30799763212635667, 0.1168057727443715, 0.02702943501522646, 0.8543232138741221, 0.0019306739296590329, 0.31189984966149636, 0.02633059815261774, 0.5711385030177573, 0.06379283129658608, 0.025902458345258107, 0.0008562796147192762, 0.9976457436267747, 0.07074611869168715, 0.2902872999220195, 0.02966772719328816, 0.5997445158766253, 0.009584958016293098, 0.8556435303116029, 0.022614495385185344, 0.05412649715142722, 0.06784348615555603, 0.9962401415167847, 0.9931098010921311, 0.9978084436984449, 0.05193644517790232, 0.5231879937553106, 0.002549027984191525, 0.07551495403167394, 0.3466678058500474, 0.14134604289025615, 0.004283213420916853, 0.853787208569426, 0.9970462019408316, 0.17258699617653525, 0.8264531768128395, 0.12161865668742822, 0.07561507785348798, 0.07085608693963209, 0.08301795260837491, 0.021151070728248386, 0.03754315054264089, 0.5895860965499238, 0.013994202312331979, 0.9842588959673492, 0.25332356027700925, 0.15487402506198628, 0.11050240355662382, 0.020905860132334236, 0.00021332510339116566, 0.013972794272121352, 0.44531615332905833, 0.0007466378618690799, 0.3460024595347738, 0.05459967889821785, 0.12208242809826239, 0.006134795381822231, 0.010429152149097792, 0.46072313317484953, 0.9969365682771103, 0.968907864584033, 0.013111067179756873, 0.017044387333683936, 0.21470065529480292, 0.7837616154451058, 0.9963728075330796, 0.9948353118409595, 0.9965834037501052, 0.9987405852203409, 0.9912648126213945, 0.9971818488641221, 0.3343100529153261, 0.11039909879016356, 0.5394407092199816, 0.001971412478395778, 0.013696128797275931, 0.00020751710298902925, 0.5370644074229265, 0.220648802368967, 0.23501055873617788, 0.0005875263968404447, 0.00665863249752504, 0.9987457481550204, 0.3797508649811807, 0.620112424105348, 0.3294137122966802, 0.19793098592933575, 0.053724124752248276, 0.19037026814383629, 0.028521732133916705, 0.07714390911221006, 0.07210343058854374, 0.0004917540023089087, 0.04733132272223246, 0.0030734625144306794, 0.2082489894578674, 0.38722841534520835, 0.0827326627950692, 0.04443172487958298, 0.007712925535476571, 0.23620010421891072, 0.0027028200594405075, 0.00013184488094831746, 0.030653934820483807, 0.38317591618948993, 0.11591127191305106, 0.013597283820569451, 0.4872731546190954, 0.9978531837003684, 0.10583262515451561, 0.8923142905184649, 0.16492095513012658, 0.30968328155524216, 0.10999599575995157, 0.037395717017991506, 0.028923249881102803, 0.03418202258675786, 0.001460770196015293, 0.3023794305751657, 0.011101853489716228, 0.47633081208803535, 0.24642060875292374, 0.03953512045625759, 0.04638637712623405, 0.000786209781800577, 0.02796660509547767, 0.16263310914960508, 0.9983217520166703, 0.9993393918210305, 0.9987294339191598, 0.992779382212316, 0.3805810275293268, 0.18430541448680815, 0.33724475037602253, 0.0002673245335022197, 0.036356136556301884, 0.010811792243867554, 0.0422966817452401, 0.0077524114715643725, 0.00038613543728098406, 0.19755939802544129, 0.3860146763262079, 0.035779190517971626, 0.17124849202113138, 0.05772011905443769, 0.04169686833901019, 0.07219566941667048, 0.0010924943669609658, 0.03678064368768585, 0.42557987437714945, 0.20000214324913598, 0.32306831559882665, 0.008525195959625828, 0.04262597979812914, 0.00020920726281290375, 0.9948215555911161, 0.9974964464754374, 0.17909760583714843, 0.2093834222730754, 0.567952532915717, 0.0007477979366895549, 0.04262448239130463, 0.6892272087049158, 0.3100241347706127, 0.05642808686979594, 0.7305022268885274, 0.0023560787837075547, 0.13900864823874573, 0.06526338230869927, 0.00047121575674151096, 0.006008000898454264, 0.9959547266131988, 0.3128445461795158, 0.0019181149367229664, 0.05869431706372277, 0.43541209063611336, 0.15613455584924946, 0.03490969184835799, 0.04446387311754308, 0.015147033699382808, 0.019055945621804175, 0.03273713735027897, 0.7744531746297338, 0.06596288869086062, 0.04837278503996445, 0.09228200404244602, 0.9066706897170321, 0.9971220562866662, 0.9977742673507517, 0.8542330250850234, 0.10677912813562793, 0.03754870439934169, 0.9976545728328626, 0.13312855963974926, 0.8657075051433974, 0.14821463134242463, 5.890883598665526e-05, 0.788435860845394, 5.890883598665526e-05, 0.03935110243908571, 0.00011781767197331052, 0.01331339693298409, 0.01042686396963798, 0.08763047857773666, 0.046839405082482036, 0.4591105651327969, 0.03755591939045857, 0.225476175216873, 0.012940616419184228, 0.04641742846011734, 0.07651842752213282, 0.0007032943706078384, 0.006892284831956816, 0.9943928374192098, 0.2671151617535015, 0.03967056857725269, 0.0033058807147710576, 0.020496460431580557, 0.6162161652333251, 0.05355526757929113, 0.9982294988565301, 0.0365599197808339, 0.9623093170883781, 0.9970103213925059, 0.17266731686588077, 0.8257039639868401, 0.07622621772964687, 0.9232706167866412, 0.022482053796507143, 0.026611410616273763, 0.06239916972091779, 0.02752904546511079, 0.8607414882091307, 0.9327135804638371, 0.06344990343291408, 0.997058201096886, 0.9993634977145209, 0.9969918576467043, 0.9987725030474104, 0.008104265713737574, 0.9678808766692303, 0.02315504489639307, 0.004580097807468926, 0.9221263585704105, 0.07328156491950281, 0.09589805106542007, 0.9017510285667726, 0.042321085136100306, 0.9564565240758669, 0.9973015921521079, 0.7047918953709003, 0.07827745167160595, 0.0860904553224442, 0.027271805196322222, 0.06633682345051352, 0.03729603629551093, 0.5308025761653739, 0.280554813549779, 0.12637842154254578, 0.023242008559548652, 0.010630577778657195, 0.0016507108351952167, 0.02674151553016251, 0.0628644059423493, 0.025855521798869466, 0.8836504803025389, 0.027629920353693844, 0.00036711608383132, 0.00660808950896376, 0.9926818906798893, 0.999630931997663, 0.9986159467344131, 0.9962312446520041, 0.048093377986687044, 0.3300352911379992, 0.21090490529941658, 0.1244250696536307, 0.2863541496638522, 0.995500063755388, 0.2860173942814899, 0.12276620533916101, 0.19095144088133856, 0.3804277204168127, 0.004425484037593254, 0.015243333907265653, 0.9976435415614368, 0.07385708643317504, 0.005380317554734606, 0.10516075220617639, 0.49694569414639633, 0.3189060950624512, 0.9980398524342645, 0.016332121283754542, 0.9799272770252724, 0.1663465925168537, 0.22167332262708475, 0.021068125704227062, 0.0023816142100430595, 0.07859326893142096, 0.04396826233925648, 0.37354702879059987, 0.09251655200551885, 0.052541388943535244, 0.0802450303864902, 0.0028658939423746497, 0.041077813174036644, 0.024837747500580296, 0.7976738139609442, 0.176749040540643, 0.1855485092495074, 0.3421663911612376, 0.006140636437121192, 0.01607960469101838, 0.12502842230220984, 0.07102880497371111, 0.05412622838905794, 0.008989385299703188, 0.014180438782630382, 0.9958079473473498, 0.9966027725257712, 0.9966948744547074, 0.0021049053470267344, 0.6008001261942022, 0.21981225838236326, 0.06314716041080203, 0.012930132846021368, 0.07878360013157205, 0.010524526735133672, 0.012328731318299443, 0.11725136909784493, 0.5391355073631829, 0.15286836080100596, 0.007495256002785926, 0.031172130585229836, 0.030707308507537685, 0.11382330627486532, 0.0067108687466804215, 0.0008134386359612632, 0.8890115698198648, 0.11020804584543778, 0.9959758175473374, 0.03791472217801137, 0.8745662582394622, 0.083412388791625, 0.9998824509974168, 0.6312359710794355, 0.07884702770584355, 0.04252682252355993, 0.010344362235460524, 0.14757956789257015, 0.0027584965961228065, 0.01632110486039327, 0.06988191376844442, 0.9889159880090097, 0.010003485427836804, 0.9979783726740098, 0.9988142816158359, 0.9951853256161237, 0.9978830822491959, 0.2885575457232763, 0.054944518980185486, 0.033994450592057204, 0.01739250960523857, 0.0023717058552598052, 0.6028085715452005, 0.8241414428352994, 0.1209236880006647, 0.00023254555384743213, 0.04999729407719791, 0.00046509110769486425, 0.004185819969253778, 0.9974558463108026, 0.07735860027985766, 0.17851984679967153, 0.020319332156060174, 0.2824387169692364, 0.07808429071400266, 0.0930335136573898, 0.2481861284775921, 0.019448503635086167, 0.0026124855629220223, 0.00014513808682900124, 0.9956292349441428, 0.07921530830297956, 0.6853846240127361, 0.016790201216392405, 0.18081755156114898, 0.03745506425195229, 0.9629852435904384, 0.03663651491639897, 0.043063667566995284, 0.9538602366089456, 0.013062331389473242, 0.9848997867662824, 0.6794931571386945, 0.11979481765792999, 0.15687596778577945, 0.006129741143583278, 0.0006810823492870309, 0.02065949792837327, 0.010291911055892911, 0.0004540548995246873, 7.567581658744787e-05, 0.005448658794296247, 0.4730420212090351, 0.18889261117014686, 0.33125337264265026, 0.0066269744958734, 0.00014302822652964174, 0.17913621783161865, 0.3397278357932115, 0.22176571296852446, 0.02393099817252294, 0.02470048364430824, 0.11449943820165316, 0.03485769187187425, 0.020237467907953482, 0.03462684623033866, 0.00654062651017508, 0.306465028989565, 0.5356840780694315, 0.15785048068503624, 0.9941429619766465, 0.3523984990624307, 0.20734011851136877, 0.24465792402597672, 0.026557980583622603, 0.015912016233917508, 0.01631052826839845, 0.047764513847072594, 0.06265178484946769, 0.02268672082009348, 0.0037573991822488573, 0.9955515648060865, 0.9958567929802746, 0.3176502250970081, 0.021003810802332778, 0.5590643961707342, 0.10216668464344586, 0.10534903162672447, 0.7075019176615812, 0.10756690597676079, 0.07873453942628882, 0.9934998517842564, 0.954119670057031, 0.04115810341422487, 0.9968955478161272, 0.9974972223843142, 0.9974623608805002, 0.060330108410062894, 0.2981485304803372, 0.22333282875809563, 0.26901816151188995, 0.018305969570335708, 0.05841992028098438, 0.07258714890498333, 0.9944425634573497, 0.9965642041051691, 0.994303113603382, 0.07369500318240883, 0.13026370280834237, 0.016088345765173758, 0.442170019094453, 0.02101864527385604, 0.10146037409972485, 0.19695249089946587, 0.008563151778237646, 0.00960110956953918, 0.9931834630948126, 0.2496126989896881, 0.6859186245817022, 0.013630257694417607, 0.0016521524478081949, 0.03249233147356116, 0.01665920384873263, 0.16891013146825248, 0.2102555946578924, 0.11956392359888658, 0.12990028939629655, 0.07846693074091517, 0.058638998273768145, 0.13273285117731756, 0.03756871414827857, 0.05675062375308748, 0.0072056396183867635, 0.03986485328103427, 0.0597972799215514, 0.03363596995587267, 0.865814782197463, 0.9948518309864123, 0.9981167969151911, 0.996761947413157, 0.9989602511921855, 0.8624879316868778, 0.10491142934333049, 0.006853510459614555, 0.014234214031507151, 0.011071055357838895, 0.9969810729660699, 0.1827133619951767, 0.1654134132898106, 0.08825749232577143, 0.0008326178521299192, 0.40724264278621153, 9.251309468110212e-05, 0.0056432987755472295, 0.14977870028870435, 0.9928480640038525, 0.9996459043700526, 0.9980344620099761, 0.9600484724192803, 0.03668975053831644, 0.16141164326846671, 0.19116494156680158, 0.15006819829222653, 0.36224640678222714, 0.13537750725742367, 0.022066187256917356, 0.6899175003112136, 0.08379564781107858, 0.18909884522700066, 0.015083216605994144, 0.8624614677444682, 0.13646542211146648, 0.015302762817230748, 0.10564792021895843, 0.06179961906958571, 0.3884547484373959, 0.4284773588824609, 0.9787318318367162, 0.01805778287521617, 0.9961680790729867, 0.8496835262766248, 0.1057893050162015, 0.02223596857028439, 0.021562151340881835, 0.9306412240744282, 0.055535552250372745, 0.011780268659169976, 0.9211991985627705, 0.0688162389418708, 0.009384032582982382, 0.12827314689282296, 0.16318773125418307, 0.097153626049002, 0.007969633386832195, 0.05464891465256363, 0.04819730667274709, 0.5005688779634127, 0.9972770248472963, 0.9943975453440823, 0.9967629189489084, 0.9952820969649215, 0.04158868401818742, 0.9565397324183106, 0.6126327100188085, 0.07198058955031036, 0.2127509732862498, 9.38469224906263e-05, 0.04429574741557561, 0.05677738810682891, 0.0015015507598500207, 0.152058254131866, 0.005777688412609761, 0.025474353455597582, 0.2342590029112685, 0.09191777020060983, 0.4879520486649516, 0.0023635998051585384, 0.9998945542744324, 0.9997491617039528, 0.48833289250871664, 0.11911548219514509, 0.3824073700115007, 0.003906257295155951, 0.006138404320959351, 0.0001014612284456091, 0.5294411504841804, 0.16254772163987993, 0.3074321779774404, 0.0005329433496389506, 0.9974983632091134, 0.3441813547906766, 0.24475688493932116, 0.15144492548725372, 0.05514082807749289, 0.1010060284861362, 0.024578276081053032, 0.0650576281658482, 0.01162659320703726, 0.0022227310542865353, 0.14146941354994347, 0.3165587816045919, 0.30076229570425234, 0.03323552675049314, 0.024393688403621678, 0.09656126226247377, 0.029740728984931297, 0.032711307085658864, 0.023135561208019413, 0.0013979191062247378, 0.9989838834276494, 0.997919758878534, 0.022103453408736616, 0.09011407928177236, 0.03400531293651787, 0.03315518011310492, 0.008501328234129468, 0.6767057274367057, 0.13517111892265854, 0.09485183974139139, 0.018312978959971606, 0.8865360070878562, 0.9992068216964002, 0.21399514073984938, 0.026650442928144003, 0.18074805352256082, 0.0414269261358278, 0.15620853676694307, 0.3158473285642413, 0.06464711403361664, 0.01243013382417608, 0.012111412444069, 0.3405537946444139, 0.055138798758524656, 0.48142864465174273, 0.00302785311101725, 0.09529769265201661, 0.9929433081127227, 0.005253593673579227, 0.2248538092291909, 0.008405749877726763, 0.08720965498141517, 0.6740360683202148, 0.06516692171573762, 0.17412394192038047, 0.20690173753485686, 0.034721143180552055, 0.518355574124585, 0.000647782522025225, 0.14876853375986898, 0.05257233899784844, 0.4748288915869503, 0.05350447266802306, 0.2703187643506392, 0.1742899932386916, 0.4105020589212576, 0.11145973039897379, 0.07744387193647216, 0.08066381970355363, 0.012632102778550362, 0.019815063182039783, 0.0022291946079794757, 0.1105515400031303, 0.0003302510530339964, 0.10670456300149049, 0.16873164680185143, 0.06975799569431898, 0.33081111360192506, 0.050610504608120596, 0.26428931358414665, 0.0011686262165285395, 0.008000594867003078, 0.9346118786038093, 0.01518499383475791, 0.007592496917378955, 0.007102658406580312, 0.01959354043194569, 0.0009796770215972845, 0.009796770215972845, 0.005143304363385744, 0.9998513304017436, 0.4896328037712054, 0.2108428059444775, 0.20751848076713983, 0.00017628997152548286, 0.010375924038356991, 0.0002014742531719804, 0.011383295304216893, 0.06482434095808469, 0.004709460667895042, 0.0003022113797579706, 0.9927062388684983, 0.9981782102715968, 0.9494267833000833, 0.04170031361945464, 0.008191133032392876, 0.09196718958080771, 0.07158112922372867, 0.36917162684229227, 0.00038319662325336546, 0.014254914385025194, 0.04468072627134241, 0.17351143100912386, 0.16929626815333684, 0.0632274428368053, 0.0018393437916161541, 0.14992979678502769, 0.2756546263774882, 0.14652230140354977, 0.05440242626290581, 0.2660196394367576, 0.0652124116110426, 0.02537996559997334, 0.004934993311105927, 0.011984983755542966, 0.014227637895219154, 0.057191360223545415, 0.05709775734265582, 0.17176128643241545, 0.04558460299323505, 0.555813906722443, 0.011513154349420763, 0.08667626770376932, 0.9972430058047649, 0.9995209577894667, 0.05265482421678405, 0.9458721150215026], \"Topic\": [1, 6, 1, 1, 3, 2, 1, 2, 3, 5, 6, 9, 1, 2, 3, 4, 6, 8, 10, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 5, 1, 3, 4, 8, 10, 5, 3, 2, 2, 10, 2, 4, 7, 2, 1, 2, 6, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 5, 7, 8, 1, 2, 3, 5, 6, 8, 9, 1, 2, 3, 5, 8, 10, 9, 10, 2, 10, 1, 2, 3, 4, 6, 10, 10, 4, 5, 7, 9, 2, 6, 1, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 9, 10, 9, 2, 5, 2, 5, 6, 2, 4, 8, 1, 2, 3, 4, 5, 9, 10, 7, 9, 10, 1, 2, 4, 5, 6, 8, 9, 1, 3, 6, 1, 3, 4, 6, 7, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 10, 7, 1, 6, 8, 10, 9, 10, 1, 2, 6, 7, 8, 10, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 1, 4, 6, 8, 8, 3, 5, 6, 7, 8, 10, 1, 2, 3, 7, 9, 1, 2, 3, 6, 3, 4, 7, 8, 10, 1, 2, 3, 6, 7, 1, 3, 4, 5, 7, 10, 4, 6, 3, 9, 1, 3, 4, 5, 7, 9, 1, 2, 6, 1, 10, 10, 2, 5, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 10, 8, 10, 4, 8, 1, 2, 4, 5, 6, 9, 3, 2, 6, 4, 8, 9, 10, 1, 4, 7, 1, 3, 4, 5, 6, 7, 8, 9, 10, 9, 3, 3, 10, 10, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 6, 6, 6, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 3, 4, 6, 7, 8, 9, 3, 4, 7, 10, 1, 3, 4, 6, 9, 8, 1, 2, 3, 5, 7, 1, 2, 3, 5, 7, 10, 7, 1, 2, 3, 5, 6, 9, 1, 2, 4, 5, 6, 9, 8, 2, 10, 3, 9, 1, 3, 4, 5, 9, 8, 5, 8, 1, 2, 4, 5, 7, 8, 5, 10, 9, 7, 3, 5, 4, 6, 10, 1, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 1, 6, 9, 1, 6, 7, 1, 4, 6, 7, 2, 4, 5, 8, 4, 5, 8, 5, 1, 2, 3, 4, 6, 1, 2, 3, 5, 8, 10, 1, 3, 7, 3, 6, 4, 7, 10, 7, 10, 4, 7, 7, 2, 3, 4, 5, 9, 1, 4, 6, 7, 4, 5, 1, 2, 4, 5, 6, 7, 8, 9, 2, 3, 5, 7, 8, 6, 7, 4, 5, 4, 5, 1, 2, 3, 5, 6, 7, 8, 9, 4, 4, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 7, 8, 1, 1, 1, 2, 4, 5, 6, 8, 9, 3, 7, 10, 1, 4, 6, 7, 8, 9, 2, 9, 4, 8, 9, 3, 3, 4, 6, 8, 9, 6, 1, 4, 1, 2, 3, 6, 9, 10, 1, 2, 3, 7, 8, 9, 1, 2, 3, 6, 2, 3, 4, 5, 6, 2, 3, 5, 7, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 6, 8, 10, 4, 1, 2, 3, 6, 9, 1, 3, 4, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 10, 9, 1, 3, 5, 7, 1, 2, 3, 6, 10, 2, 3, 6, 9, 10, 1, 4, 6, 7, 4, 4, 5, 7, 2, 3, 6, 7, 9, 10, 1, 2, 3, 5, 7, 2, 3, 5, 7, 2, 2, 3, 5, 5, 1, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 3, 4, 6, 8, 2, 3, 4, 5, 6, 7, 4, 9, 1, 3, 4, 5, 7, 8, 9, 6, 1, 3, 4, 6, 7, 10, 1, 3, 1, 3, 5, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 10, 4, 6, 7, 8, 10, 1, 1, 5, 2, 3, 4, 5, 6, 3, 4, 6, 8, 9, 1, 2, 6, 8, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 6, 10, 1, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 6, 8, 7, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 9, 10, 1, 3, 4, 6, 7, 1, 6, 2, 4, 7, 9, 9, 5, 5, 3, 4, 1, 1, 3, 4, 6, 9, 10, 2, 1, 2, 3, 5, 8, 9, 10, 8, 1, 2, 3, 5, 6, 8, 9, 10, 8, 1, 2, 3, 4, 5, 7, 8, 9, 5, 6, 1, 7, 8, 10, 1, 1, 6, 1, 4, 6, 7, 7, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 4, 5, 7, 9, 1, 3, 4, 1, 3, 4, 2, 6, 2, 5, 6, 7, 5, 1, 2, 3, 4, 5, 6, 8, 9, 8, 1, 2, 3, 5, 3, 4, 8, 9, 3, 6, 7, 8, 9, 10, 8, 9, 3, 4, 5, 9, 10, 3, 4, 6, 7, 8, 9, 1, 3, 9, 5, 1, 4, 4, 6, 4, 3, 7, 8, 10, 2, 5, 1, 2, 1, 4, 6, 7, 8, 1, 4, 6, 9, 1, 2, 3, 4, 5, 7, 8, 6, 2, 6, 2, 7, 1, 3, 6, 7, 9, 4, 4, 4, 5, 7, 7, 6, 1, 2, 3, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 5, 7, 10, 1, 2, 3, 5, 6, 7, 9, 10, 1, 2, 3, 5, 7, 8, 6, 2, 4, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 3, 4, 5, 7, 9, 7, 7, 7, 6, 6, 8, 1, 2, 3, 6, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 4, 7, 1, 3, 4, 5, 6, 7, 8, 7, 1, 2, 3, 5, 7, 8, 1, 3, 4, 6, 7, 8, 9, 10, 5, 2, 8, 3, 1, 2, 3, 4, 6, 9, 3, 4, 8, 1, 3, 7, 1, 3, 4, 5, 7, 8, 9, 1, 6, 1, 6, 10, 1, 2, 3, 5, 7, 8, 2, 4, 5, 4, 5, 7, 7, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 7, 8, 9, 10, 3, 4, 5, 7, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 4, 6, 8, 4, 1, 2, 3, 5, 6, 10, 9, 10, 10, 10, 2, 4, 5, 8, 10, 6, 8, 8, 3, 3, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 3, 1, 4, 1, 8, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 9, 5, 1, 2, 6, 1, 2, 3, 4, 5, 6, 8, 9, 10, 8, 10, 9, 5, 1, 3, 10, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 9, 10, 7, 9, 9, 1, 3, 5, 9, 10, 4, 3, 5, 9, 1, 3, 5, 7, 1, 2, 3, 5, 7, 9, 5, 1, 5, 7, 3, 9, 1, 2, 3, 5, 6, 7, 8, 2, 3, 4, 5, 6, 9, 10, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 10, 3, 4, 8, 2, 5, 8, 2, 3, 2, 5, 8, 9, 2, 3, 5, 8, 9, 8, 1, 2, 3, 5, 6, 7, 8, 10, 2, 4, 5, 6, 7, 8, 1, 6, 8, 1, 2, 3, 4, 6, 7, 10, 7, 9, 10, 4, 6, 8, 1, 2, 3, 6, 8, 9, 10, 8, 5, 5, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 5, 1, 6, 4, 2, 3, 4, 5, 7, 2, 3, 5, 7, 1, 2, 3, 7, 8, 10, 8, 3, 4, 8, 9, 10, 1, 2, 4, 6, 6, 1, 4, 3, 4, 5, 8, 9, 3, 8, 10, 9, 1, 10, 1, 3, 4, 7, 8, 9, 10, 4, 10, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 6, 8, 9, 1, 1, 2, 6, 2, 8, 6, 2, 7, 1, 2, 4, 1, 2, 3, 6, 7, 8, 1, 2, 3, 5, 6, 6, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 5, 7, 1, 4, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 6, 3, 3, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 9, 2, 5, 1, 3, 6, 7, 10, 4, 6, 2, 3, 4, 6, 8, 9, 10, 4, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 2, 5, 9, 7, 4, 6, 8, 8, 1, 8, 1, 2, 3, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 3, 6, 8, 9, 10, 4, 7, 10, 2, 7, 10, 7, 10, 1, 3, 4, 6, 7, 3, 5, 3, 6, 1, 3, 2, 5, 8, 2, 5, 8, 6, 8, 6, 8, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 4, 5, 7, 9, 3, 7, 10, 10, 7, 3, 1, 2, 3, 6, 10, 8, 1, 2, 3, 6, 9, 10, 7, 2, 4, 5, 9, 10, 9, 7, 9, 1, 3, 4, 5, 6, 8, 9, 10, 1, 2, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 1, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 5, 7, 4, 5, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 6, 1, 1, 10, 10, 3, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 8, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 3, 5, 8, 9, 1, 6, 1, 8, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 2, 4, 6, 7, 8, 2, 5, 7, 8, 2, 2, 10, 3, 1, 9, 1, 2, 3, 5, 6, 7, 8, 10, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 8, 3, 5, 8, 8, 1, 2, 3, 4, 6, 4, 1, 2, 3, 4, 5, 6, 7, 8, 7, 6, 6, 4, 10, 2, 3, 6, 9, 10, 1, 2, 4, 5, 6, 4, 9, 1, 2, 4, 5, 6, 2, 9, 7, 2, 6, 9, 10, 2, 6, 9, 2, 6, 10, 1, 2, 3, 5, 7, 9, 10, 10, 4, 3, 7, 8, 9, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 7, 8, 8, 1, 1, 2, 3, 5, 6, 7, 1, 2, 3, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 6, 2, 3, 4, 6, 7, 8, 9, 2, 6, 8, 8, 1, 2, 5, 6, 7, 8, 10, 1, 3, 4, 5, 7, 8, 9, 4, 3, 4, 6, 8, 9, 1, 4, 5, 6, 7, 9, 1, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 8, 1, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 1, 6]}, \"mdsDat\": {\"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"y\": [0.10435846925763753, 0.12227876130720414, 0.1445317436485519, -0.144452018558395, -0.08546042972596556, 0.12382746413354467, -0.19890057978098713, -0.16997935978651002, 0.07499719938007703, 0.02879875012484264], \"x\": [0.1451765727140289, 0.12027739960778161, 0.08893383468584332, 0.008759059521978262, 0.11408912564050938, -0.004863781514416023, 0.0687474178087151, -0.024073571394351966, -0.14333225831784258, -0.3737137987522463], \"Freq\": [22.1656833227266, 17.549846489680288, 13.446281778368114, 8.519801723364477, 8.332095139104153, 7.624861248324784, 7.515619033538547, 6.449056905689894, 5.7234399034576295, 2.6733144557455137], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, \"topic.order\": [3, 7, 10, 1, 9, 2, 4, 6, 8, 5], \"lambda.step\": 0.01};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el233823139830939239192202009675\", ldavis_el233823139830939239192202009675_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el233823139830939239192202009675\", ldavis_el233823139830939239192202009675_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el233823139830939239192202009675\", ldavis_el233823139830939239192202009675_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "2      22.165683        1       1  0.145177  0.104358\n",
       "6      17.549846        1       2  0.120277  0.122279\n",
       "9      13.446282        1       3  0.088934  0.144532\n",
       "0       8.519802        1       4  0.008759 -0.144452\n",
       "8       8.332095        1       5  0.114089 -0.085460\n",
       "1       7.624861        1       6 -0.004864  0.123827\n",
       "3       7.515619        1       7  0.068747 -0.198901\n",
       "5       6.449057        1       8 -0.024074 -0.169979\n",
       "7       5.723440        1       9 -0.143332  0.074997\n",
       "4       2.673314        1      10 -0.373714  0.028799, topic_info=      Category           Freq        Term          Total  loglift  logprob\n",
       "term                                                                      \n",
       "3404   Default  117745.000000        film  117745.000000  30.0000  30.0000\n",
       "25626  Default   26861.000000         bad   26861.000000  29.0000  29.0000\n",
       "43733  Default   10140.000000      horror   10140.000000  28.0000  28.0000\n",
       "51266  Default    9054.000000       music    9054.000000  27.0000  27.0000\n",
       "10200  Default   17458.000000        life   17458.000000  26.0000  26.0000\n",
       "16411  Default   16975.000000        show   16975.000000  25.0000  25.0000\n",
       "50035  Default   27161.000000       great   27161.000000  24.0000  24.0000\n",
       "4516   Default   43136.000000        good   43136.000000  23.0000  23.0000\n",
       "26743  Default   34579.000000      really   34579.000000  22.0000  22.0000\n",
       "44470  Default   20778.000000       films   20778.000000  21.0000  21.0000\n",
       "38367  Default   13013.000000       funny   13013.000000  20.0000  20.0000\n",
       "51063  Default   18484.000000        love   18484.000000  19.0000  19.0000\n",
       "22894  Default    5896.000000         war    5896.000000  18.0000  18.0000\n",
       "386    Default   27622.000000      people   27622.000000  17.0000  17.0000\n",
       "35413  Default   22357.000000      movies   22357.000000  16.0000  16.0000\n",
       "42053  Default   34421.000000       story   34421.000000  15.0000  15.0000\n",
       "50651  Default   39707.000000       would   39707.000000  14.0000  14.0000\n",
       "53519  Default    8292.000000        girl    8292.000000  13.0000  13.0000\n",
       "41944  Default   10310.000000      action   10310.000000  12.0000  12.0000\n",
       "25796  Default   19711.000000       watch   19711.000000  11.0000  11.0000\n",
       "15758  Default    8074.000000      family    8074.000000  10.0000  10.0000\n",
       "22015  Default   58689.000000        like   58689.000000   9.0000   9.0000\n",
       "1874   Default   20974.000000       think   20974.000000   8.0000   8.0000\n",
       "11603  Default    4124.000000        game    4124.000000   7.0000   7.0000\n",
       "26557  Default   18912.000000        best   18912.000000   6.0000   6.0000\n",
       "7464   Default    9375.000000        role    9375.000000   5.0000   5.0000\n",
       "23511  Default   10683.000000       young   10683.000000   4.0000   4.0000\n",
       "43597  Default   11243.000000        cast   11243.000000   3.0000   3.0000\n",
       "16222  Default   33666.000000         see   33666.000000   2.0000   2.0000\n",
       "4938   Default    4898.000000      mother    4898.000000   1.0000   1.0000\n",
       "...        ...            ...         ...            ...      ...      ...\n",
       "47483  Topic10     737.431857        sing     765.866013   3.5840  -5.7356\n",
       "40982  Topic10    1552.816615       dance    1658.358812   3.5561  -4.9909\n",
       "43750  Topic10    1790.574281   animation    1926.873879   3.5485  -4.8484\n",
       "31092  Topic10    1453.410923      disney    1579.271232   3.5388  -5.0571\n",
       "51250  Topic10    2035.627215     musical    2253.074998   3.5204  -4.7202\n",
       "10845  Topic10    1186.631959     singing    1285.646893   3.5417  -5.2599\n",
       "11603  Topic10    3395.736457        game    4124.455419   3.4274  -4.2085\n",
       "21624  Topic10    1046.294117     dancing    1163.107217   3.5160  -5.3857\n",
       "16619  Topic10     808.630747     numbers     885.134340   3.5315  -5.6434\n",
       "1185   Topic10    1195.688662       robin    1400.817426   3.4635  -5.2523\n",
       "50873  Topic10    1024.262050        band    1289.201497   3.3918  -5.4070\n",
       "46251  Topic10     746.359372      singer     903.471501   3.4308  -5.7235\n",
       "2222   Topic10     801.343325       games    1049.385172   3.3522  -5.6524\n",
       "18710  Topic10     589.165533       robot     712.684053   3.4315  -5.9600\n",
       "51266  Topic10    4073.273514       music    9054.069329   2.8231  -4.0265\n",
       "53856  Topic10     580.280131        bugs     719.275607   3.4071  -5.9752\n",
       "38277  Topic10    1115.279843        rock    1891.157214   3.0938  -5.3219\n",
       "6863   Topic10    1319.316434       voice    2635.002011   2.9301  -5.1539\n",
       "48745  Topic10     531.392409       kelly     708.275754   3.3345  -6.0632\n",
       "23066  Topic10     501.406909        hood     652.858627   3.3579  -6.1213\n",
       "42783  Topic10     614.972547      prince    1083.376541   3.0556  -5.9172\n",
       "54154  Topic10     999.339994        play    6558.429210   1.7404  -5.4316\n",
       "37523  Topic10     566.915242        bond    1084.004276   2.9736  -5.9985\n",
       "26829  Topic10     651.723059       stage    2044.488989   2.4786  -5.8591\n",
       "19490  Topic10     682.473874      number    2756.902297   2.2257  -5.8130\n",
       "8356   Topic10     649.467264  soundtrack    2266.424289   2.3720  -5.8626\n",
       "26823  Topic10     727.547237     version    5377.555066   1.6215  -5.7491\n",
       "35440  Topic10     668.963658     playing    4693.160566   1.6737  -5.8330\n",
       "26557  Topic10     648.236473        best   18912.370094   0.2485  -5.8645\n",
       "6117   Topic10     621.676308    original    9235.981301   0.9234  -5.9063\n",
       "\n",
       "[829 rows x 6 columns], token_table=       Topic      Freq      Term\n",
       "term                            \n",
       "30838      1  0.996860      1/10\n",
       "50800      6  0.996875      13th\n",
       "39449      1  0.998126      2/10\n",
       "25929      1  0.993250      3000\n",
       "33958      3  0.996301       abc\n",
       "32088      2  0.994972  abstract\n",
       "30491      1  0.551668    acting\n",
       "30491      2  0.187829    acting\n",
       "30491      3  0.161893    acting\n",
       "30491      5  0.012380    acting\n",
       "30491      6  0.069631    acting\n",
       "30491      9  0.016581    acting\n",
       "41944      1  0.079823    action\n",
       "41944      2  0.187870    action\n",
       "41944      3  0.074295    action\n",
       "41944      4  0.003686    action\n",
       "41944      6  0.575929    action\n",
       "41944      8  0.076913    action\n",
       "41944     10  0.001358    action\n",
       "8357       1  0.211688     actor\n",
       "8357       2  0.029951     actor\n",
       "8357       3  0.287971     actor\n",
       "8357       4  0.050543     actor\n",
       "8357       5  0.053975     actor\n",
       "8357       6  0.009828     actor\n",
       "8357       8  0.003588     actor\n",
       "8357       9  0.352554     actor\n",
       "37880      1  0.109272   actress\n",
       "37880      2  0.046610   actress\n",
       "37880      3  0.218852   actress\n",
       "...      ...       ...       ...\n",
       "13112      2  0.071581     years\n",
       "13112      3  0.369172     years\n",
       "13112      4  0.000383     years\n",
       "13112      5  0.014255     years\n",
       "13112      6  0.044681     years\n",
       "13112      7  0.173511     years\n",
       "13112      8  0.169296     years\n",
       "13112      9  0.063227     years\n",
       "13112     10  0.001839     years\n",
       "15202      1  0.149930       yet\n",
       "15202      2  0.275655       yet\n",
       "15202      3  0.146522       yet\n",
       "15202      4  0.054402       yet\n",
       "15202      5  0.266020       yet\n",
       "15202      6  0.065212       yet\n",
       "15202      7  0.025380       yet\n",
       "15202      8  0.004935       yet\n",
       "15202      9  0.011985       yet\n",
       "23511      2  0.014228     young\n",
       "23511      3  0.057191     young\n",
       "23511      4  0.057098     young\n",
       "23511      5  0.171761     young\n",
       "23511      6  0.045585     young\n",
       "23511      7  0.555814     young\n",
       "23511      8  0.011513     young\n",
       "23511      9  0.086676     young\n",
       "8021       1  0.997243     zohan\n",
       "24629      6  0.999521    zombie\n",
       "15457      1  0.052655   zombies\n",
       "15457      6  0.945872   zombies\n",
       "\n",
       "[2420 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 7, 10, 1, 9, 2, 4, 6, 8, 5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_tf, corpus_tf, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with LDA vectors (we'll have to use the labeled set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For predictions we need only the labeled set:\n",
    "print(\"Parsing sentences from training set...\")\n",
    "sentences2 = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                         'review', \n",
    "                                                         remove_html=True,\n",
    "                                                         remove_stopwords=True,)\n",
    "\n",
    "print(\"Creating Dictionary\")\n",
    "additional_stopwords=set(['n\\'t', 'movie'])\n",
    "dictionary2 = prep_corpus(sentences2, additional_stopwords)\n",
    "dictionary2.compactify()\n",
    "print('dictionary done')\n",
    "\n",
    "print(\"Creating Corpora\")\n",
    "corpus_tf2 = [dictionary2.doc2bow(sentence) for sentence in sentences2]\n",
    "print('corpus tf done')\n",
    "\n",
    "lda_tf2 = models.LdaModel(corpus_tf2, id2word=dictionary2, num_topics=10, passes=10)\n",
    "corpus_lda_tf2 = lda_tf2[corpus_tf2]\n",
    "print('corpus lda tf done')\n",
    "\n",
    "print('Creating lda vectors')\n",
    "X = gensim.matutils.corpus2csc(corpus_lda_tf2)\n",
    "X = X.T\n",
    "print('lda vectors done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = train_test_split(X,\n",
    "                                                                        train[\"sentiment\"],\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf_lda = LR(penalty='l2',\n",
    "                   dual=False,\n",
    "                   tol=0.0001,\n",
    "                   C=1.0,\n",
    "                   fit_intercept=True,\n",
    "                   intercept_scaling=1,\n",
    "                   class_weight=None,\n",
    "                   random_state=0,\n",
    "                   solver='liblinear',\n",
    "                   max_iter=100,\n",
    "                   multi_class='ovr',\n",
    "                   verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_lda = clf_LR_tf_lda.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Word Vectors\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors  \n",
    "\n",
    "Introducing Distributed Word Vectors: This part of the tutorial will focus on using distributed word vectors created by the Word2Vec algorithm.\n",
    "\n",
    "Word2vec, published by Google in 2013, is a neural network implementation that learns distributed representations for words. Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. Word2vec learns quickly relative to other models.\n",
    "\n",
    "Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\"\n",
    "\n",
    "Distributed word vectors are powerful and can be used for many applications, particularly word prediction and translation. Here, we will try to apply them to sentiment analysis.\n",
    "\n",
    "Using word2vec in Python: In Python, we will use the excellent implementation of word2vec from the gensim package. If you don't already have gensim installed, you'll need to install it. There is an excellent tutorial that accompanies the Python Word2Vec implementation, here.\n",
    "\n",
    "Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is compute intensive. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). ln order to train your model in a reasonable amount of time, you will need to install cython (instructions here). Word2Vec will run without cython installed, but it will take days to run instead of minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to Train a Model\n",
    "\n",
    "First, we read in the data with pandas, as we did in Part 1. Unlike Part 1, we now use unlabeledTrain.tsv, which contains 50,000 additional reviews with no labels. When we built the Bag of Words model in Part 1, extra unlabeled training reviews were not useful. However, since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used.  \n",
    "\n",
    "The functions we write to clean the data are also similar to Part 1, although now there are a couple of differences. First, to train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. For this reason, we will make stop word removal optional in the functions below. It also might be better not to remove numbers, but we leave that as an exercise for the reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Sentences (with stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "sentences_w2v = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                         'review', \n",
    "                                                         remove_html=True,\n",
    "                                                         remove_stopwords=False,)\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "sentencesw2v += Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                          'review', \n",
    "                                                          remove_html=True,\n",
    "                                                          remove_stopwords=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "\n",
    "Training and Saving Your Model\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "\n",
    "Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n",
    "\n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 10   # Minimum word count\n",
    "num_workers = -1       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentencesw2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-532937fc90d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Initialize and train the model (this will take some time)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m model = Word2Vec(sentencesw2v,\n\u001b[0m\u001b[0;32m      3\u001b[0m                  \u001b[0mworkers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mmin_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_word_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentencesw2v' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "model = Word2Vec(sentencesw2v,\n",
    "                 workers = num_workers,\n",
    "                 size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)\n",
    "\n",
    "##Optionally converting the model for Bigrams (to capture more context):\n",
    "\n",
    "#bigram_transformer = gensim.models.Phrases(sentences_w2v)\n",
    "#model = Word2Vec(bigram_transformer[sentences_w2v],\n",
    "#                 workers = num_workers,\n",
    "#                 size = num_features,\n",
    "#                 min_count = min_word_count, \n",
    "#                 window = context,\n",
    "#                 sample = downsampling,\n",
    "#                 seed=1,)\n",
    "\n",
    "# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"{}features_{}minwords_{}context\".format(num_features, min_word_count, context)\n",
    "model.save(os.path.join(outputs,model_name))\n",
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Model Results\n",
    "\n",
    "Congratulations on making it successfully through everything so far! Let's take a look at the model we created out of our 75,000 training reviews.\n",
    "\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n",
      "berlin\n",
      "berlin\n",
      "[('strains', 0.24189811944961548), ('hysteria', 0.22662164270877838), ('submitting', 0.2188923954963684), ('sponsors', 0.21460050344467163), ('bumper', 0.21113869547843933), ('pepe', 0.20673879981040955), ('distracting', 0.20640014111995697), ('archibald', 0.20597897469997406), ('intangible', 0.2026800811290741), ('via', 0.20249362289905548)]\n",
      "[('punjab', 0.24595320224761963), ('baritone', 0.24127547442913055), ('abortions', 0.21941334009170532), ('murdering', 0.21634769439697266), ('bakersfield', 0.2108631432056427), ('inaugural', 0.20844942331314087), ('cooks', 0.20685292780399323), ('caved', 0.20354284346103668), ('forgery', 0.20292317867279053), ('hmm', 0.2020784616470337)]\n",
      "[('lightning', 0.25641635060310364), ('motherhood', 0.21935205161571503), ('melting', 0.21734225749969482), ('gradually', 0.2173215001821518), ('stove', 0.2124207615852356), ('laugh', 0.2099732905626297), ('underpaid', 0.20531228184700012), ('subverted', 0.20407818257808685), ('ten', 0.20405235886573792), ('slated', 0.202193483710289)]\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"man woman child kitchen\".split()))\n",
    "print(model.doesnt_match(\"france england germany berlin\".split()))\n",
    "print(model.doesnt_match(\"paris berlin london austria\".split()))\n",
    "print(model.most_similar(\"man\"))\n",
    "print(model.most_similar(\"queen\"))\n",
    "print(model.most_similar(\"awful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have a reasonably good model for semantic meaning - at least as good as Bag of Words. But how can we use these fancy distributed word vectors for supervised learning? The next section takes a stab at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: More Fun With Word Vectors\n",
    "--\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors  \n",
    "\n",
    "Numeric Representations of Words\n",
    "\n",
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36156, 300)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02072788,  0.08614165, -0.07159118,  0.08381763,  0.06069779,\n",
       "       -0.0762706 , -0.05979674, -0.08882733,  0.09815833, -0.00967654], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'film',\n",
       " \"n't\",\n",
       " 'one',\n",
       " 'like',\n",
       " 'good',\n",
       " 'would',\n",
       " 'even',\n",
       " 'time',\n",
       " 'really']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.index2word[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words To Paragraphs, \n",
    "--\n",
    "Attempt 1:  Vector Averaging  \n",
    "--\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from Part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 2000th review\n",
    "        if counter%2000. == 0.:\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        #Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the pickles from Part 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    (clean_train_reviews, \n",
    "     clean_test_reviews,\n",
    "     clean_train_reviews_sw,\n",
    "     clean_test_reviews_sw) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call these functions to create average vectors for each paragraph. The following operations will take a few minutes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, using the functions \n",
    "# we defined above. Notice that we now use stop word removal.\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "trainDataVecs = Imputer().fit_transform(trainDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the average paragraph vectors with the classifiers from Part 1.  \n",
    "Note that, as in Part 1, we can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvWV, X_testcvWV, y_traincvWV, y_testcvWV = train_test_split(trainDataVecs,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(20000, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-ee6352665937>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                                    class_weight=None).fit(X_traincvWV, y_traincvWV)\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0meval_RF_WV_tts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_RF_WV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_testcvWV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_testcvWV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \"\"\"\n\u001b[0;32m    211\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;31m# Pre-sort indices to avoid that each individual tree of the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    413\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 415\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(20000, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_WV = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_RF_WV_tts = clf_RF_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_RF_WV_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-93111c51f5a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                verbose=0).fit(X_traincvWV, y_traincvWV)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0meval_LR_WV_tts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_LR_WV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_testcvWV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_testcvWV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64, \n\u001b[1;32m-> 1142\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1143\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    508\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    509\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    396\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     52\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     53\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 54\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "clf_LR_WV = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=None,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_LR_WV_tts = clf_LR_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_LR_WV_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 543\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews[0], model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(testDataVecs).any()) #testando se no h valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "testDataVecs = Imputer().fit_transform(testDataVecs)\n",
    "\n",
    "print(np.isnan(testDataVecs).any()) #testando se no h valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_RF_WV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-e7c27bd341db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use the random forest to make sentiment label predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_RF_WV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataVecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mresult_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_RF_WV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataVecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# \"probs\":result_prob[:,1]})\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Use pandas to write the comma-separated output file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf_RF_WV' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = clf_RF_WV.predict(testDataVecs)\n",
    "result_prob = clf_RF_WV.predict_proba(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          0\n",
       "1    \"8348_2\"          1\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this produced results much better than chance, but underperformed Bag of Words by a few percentage points.\n",
    "\n",
    "Since the element-wise average of the vectors didn't produce spectacular results, perhaps we could do it in a more intelligent way? A standard way of weighting word vectors is to apply \"tf-idf\" weights, which measure how important a given word is within a given set of documents. One way to extract tf-idf weights in Python is by using scikit-learn's TfidfVectorizer, which has an interface similar to the CountVectorizer that we used in Part 1. However, when we tried weighting our word vectors in this way, we found no substantial improvement in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words to Paragraphs, Attempt 2: Clustering \n",
    "--\n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "In K-Means, the one parameter we need to set is \"K,\" or the number of clusters. How should we decide how many clusters to create? Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Clustering code is given below. We use scikit-learn to perform our K-Means.\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code took more than 40 minutes on my computer. Below, we set a timer around the K-Means function to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignment for each word is now stored in idx, and the vocabulary from our original Word2Vec model is still stored in model.index2word. For convenience, we zip these into one dictionary as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain. Your clusters may differ, as Word2Vec relies on a random number seed. Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n",
    "Run k-means on the word vectors and print a few clusters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the first ten clusters\n",
    "for cluster in range(0,10):\n",
    "    # Print the cluster number\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        #print(len(word_centroid_map.values()))\n",
    "        #print(cluster)\n",
    "        #print(word_centroid_map.keys())\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are of varying quality. Some make sense, some cointain mostly names, and some contain related adjectives. On the other hand, some are a little mystifying. Perhaps our algorithm works best on adjectives.\n",
    "\n",
    "At any rate, now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    # The number of clusters is equal to the highest cluster index in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function above will give us a numpy array for each review, each with a number of features equal to the number of clusters. Finally, we create bags of centroids for our training and test set, then train a random forest and extract results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ****** Create bags of centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvCT, X_testcvCT, y_traincvCT, y_testcvCT = train_test_split(train_centroids,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "clf_RF_CT = RandomForestClassifier(n_estimators=100, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=True, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvCT, y_traincvCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_RF_CT_tts = clf_RF_CT.score(X_testcvCT, y_testcvCT)\n",
    "print(eval_RF_CT_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = clf_RF_CT.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(os.path.join(outputs,\"BagOfCentroids.csv\"), index=False, quoting=3)\n",
    "print(\"Wrote BagOfCentroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the code above gives about the same (or slightly worse) results compared to the Bag of Words in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Comparing deep and non-deep learning methods\n",
    "--\n",
    "\n",
    "You may ask: Why is Bag of Words better?\n",
    "\n",
    "The biggest reason is, in our tutorial, averaging the vectors and using the centroids lose the order of words, making it very similar to the concept of Bag of Words. The fact that the performance is similar (within range of standard error) makes all three methods practically equivalent.  \n",
    "\n",
    "A few things to try:\n",
    "\n",
    "First, training Word2Vec on a lot more text should greatly improve performance. Google's results are based on word vectors that were learned out of more than a billion-word corpus; our labeled and unlabeled training sets together are only a measly 18 million words or so. Conveniently, Word2Vec provides functions to load any pre-trained model that is output by Google's original C tool, so it's also possible to train a model in C and then import it into Python.\n",
    "\n",
    "Second, in published literature, distributed word vector techniques have been shown to outperform Bag of Words models. In this paper, an algorithm called Paragraph Vector is used on the IMDB dataset to produce some of the most state-of-the-art results to date. In part, it does better than the approaches we try here because vector averaging and clustering lose the word order, whereas Paragraph Vectors preserves word order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Deep Learning?\n",
    "\n",
    "The term \"deep learning\" was coined in 2006, and refers to machine learning algorithms that have multiple non-linear layers and can learn feature hierarchies [1].\n",
    "\n",
    "Most modern machine learning relies on feature engineering or some level of domain knowledge to obtain good results. In deep learning systems, this is not the case -- instead, algorithms can automatically learn feature hierarchies, which represent objects in increasing levels of abstraction. Although the basic ingredients of many deep learning algorithms have been around for many years, they are currently increasing in popularity for many reasons, including advances in compute power, the falling cost of computing hardware, and advances in machine learning research.\n",
    "\n",
    "Deep learning algorithms can be categorized by their architecture (feed-forward, feed-back, or bi-directional) and training protocols (purely supervised, hybrid, or unsupervised) [2]. \n",
    "\n",
    "Some good background materials include:\n",
    "\n",
    "[1] \"Deep Learning for Signal and Information Processing\", by Li Deng and Dong Yu (out of Microsoft)\n",
    "\n",
    "[2] \"Deep Learning Tutorial\" (2013 Presentation by Yann LeCun and Marc'Aurelio Ranzato)\n",
    "\n",
    "Where Does Word2Vec Fit In?\n",
    "\n",
    "Word2Vec works in a way that is similar to deep approaches such as recurrent neural nets or deep neural nets, but it implements certain algorithms, such as hierarchical softmax, that make it computationally more efficient.  \n",
    "\n",
    "See Part 2 of this tutorial for more on Word2Vec, as well as this paper: Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "In this tutorial, we use a hybrid approach to training -- consisting of an unsupervised piece (Word2Vec) followed by supervised learning (the Random Forest). \n",
    "\n",
    "Libraries and Packages \n",
    "\n",
    "The lists below should in no way be considered exhaustive.\n",
    "\n",
    "In Python:\n",
    "\n",
    "Theano offers very low-level, nuts and bolts functionality for building deep learning systems. You can also find some good tutorials on their site.  \n",
    "Caffe is a deep learning framework out of the Berkeley Vision and Learning Center.  \n",
    "Pylearn2 wraps Theano and seems slightly more user friendly.  \n",
    "OverFeat was used to win the Kaggle Cats and Dogs competition.  \n",
    "\n",
    "\n",
    "More Tutorials  \n",
    "The O'Reilly Blog has a series of deep learning articles and tutorials:  \n",
    "\n",
    "http://radar.oreilly.com/2014/07/what-is-deep-learning-and-why-should-you-care.html  \n",
    "http://radar.oreilly.com/2014/07/how-to-build-and-run-your-first-deep-learning-network.html  \n",
    "Webcast: How to Get Started with Deep Learning in Computer Vision  \n",
    "There are several tutorials using Theano as well.  \n",
    "\n",
    "If you want to dive into the weeds of creating a neural network from scratch, check out Geoffrey Hinton's Coursera course.\n",
    "\n",
    "For NLP, check out this recent lecture at Stanford: http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/  \n",
    "\n",
    "This free, online book also introduces neural nets for deep learning: http://neuralnetworksanddeeplearning.com/  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#word2vec example\n",
    "# load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "sentences = word2vec.Text8Corpus('/tmp/text8')\n",
    "#train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=200)\n",
    "# ... and some hours later... just as advertised...\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "[('queen', 0.5359965)]\n",
    " \n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "model.save('/tmp/text8.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "model.save_word2vec_format('/tmp/text8.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "model = word2vec.Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "model.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
    "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)\n",
    "# which word doesn't go with the others?\n",
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "\n",
    "#http://rare-technologies.com/word2vec-tutorial/\n",
    "Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence\n",
    "\n",
    "For example, if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file, line by line:\n",
    ">>> class MySentences(object):\n",
    "...     def __init__(self, dirname):\n",
    "...         self.dirname = dirname\n",
    "... \n",
    "...     def __iter__(self):\n",
    "...         for fname in os.listdir(self.dirname):\n",
    "...             for line in open(os.path.join(self.dirname, fname)):\n",
    "...                 yield line.split()\n",
    ">>>\n",
    ">>> sentences = MySentences('/some/directory') # a memory-friendly iterator\n",
    ">>> model = gensim.models.Word2Vec(sentences)\n",
    "\n",
    "Say we want to further preprocess the words from the files  convert to unicode, lowercase, remove numbers, extract named entities All of this can be done inside the MySentences iterator and word2vec doesnt need to know. All that is required is that the input yields one sentence (list of utf8 words) after another.\n",
    "\n",
    "calling Word2Vec(sentences) will run two passes over the sentences iterator. The first pass collects words and their frequencies to build an internal dictionary tree structure.\n",
    "\n",
    "The second pass trains the neural model.\n",
    "\n",
    "These two passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and youre able to initialize the vocabulary some other way:\n",
    "\n",
    ">>> model = gensim.models.Word2Vec() # an empty model, no training\n",
    ">>> model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    ">>> model.train(other_sentences)  # can be a non-repeatable, 1-pass generator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
