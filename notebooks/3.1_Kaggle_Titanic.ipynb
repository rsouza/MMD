{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to predictive analysis: the sinking of Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/titanic-gettingStarted\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VARIABLE DESCRIPTIONS:\n",
    "survival        Survival\n",
    "                (0 = No; 1 = Yes)\n",
    "pclass          Passenger Class\n",
    "                (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "name            Name\n",
    "sex             Sex\n",
    "age             Age\n",
    "sibsp           Number of Siblings/Spouses Aboard\n",
    "parch           Number of Parents/Children Aboard\n",
    "ticket          Ticket Number\n",
    "fare            Passenger Fare\n",
    "cabin           Cabin\n",
    "embarked        Port of Embarkation\n",
    "                (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "SPECIAL NOTES:\n",
    "Pclass is a proxy for socio-economic status (SES)\n",
    " 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "\n",
    "Age is in Years; Fractional if Age less than One (1)\n",
    " If the Age is Estimated, it is in the form xx.5\n",
    "\n",
    "With respect to the family relation variables (i.e. sibsp and parch)\n",
    "some relations were ignored.  The following are the definitions used\n",
    "for sibsp and parch.\n",
    "\n",
    "Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
    "Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n",
    "Parent:   Mother or Father of Passenger Aboard Titanic\n",
    "Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
    "\n",
    "Other family relatives excluded from this study include cousins,\n",
    "nephews/nieces, aunts/uncles, and in-laws.  Some children travelled\n",
    "only with a nanny, therefore parch=0 for them.  As well, some\n",
    "travelled with very close friends or neighbors in a village, however,\n",
    "the definitions do not support such relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the usual packages...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "#import seaborn as sns\n",
    "import seaborn.apionly as sns\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#matplotlib.rcdefaults()\n",
    "#matplotlib.verbose.set_level('silent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = \"../datasets/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(datapath,'Kaggle/kaggle_titanic_train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(datapath,'Kaggle/kaggle_titanic_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived         int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    int64\n",
       "Survived       int64\n",
       "Pclass         int64\n",
       "SibSp          int64\n",
       "Parch          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes[df_train.dtypes.map(lambda x: x=='int64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age     float64\n",
       "Fare    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes[df_train.dtypes.map(lambda x: x=='float64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name        object\n",
       "Sex         object\n",
       "Ticket      object\n",
       "Cabin       object\n",
       "Embarked    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes[df_train.dtypes.map(lambda x: x=='object')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of levels in category 'PassengerId': \b 891.00 \n",
      "Number of levels in category 'Survived': \b 2.00 \n",
      "Number of levels in category 'Pclass': \b 3.00 \n",
      "Number of levels in category 'Name': \b 891.00 \n",
      "Number of levels in category 'Sex': \b 2.00 \n",
      "Number of levels in category 'Age': \b 89.00 \n",
      "Number of levels in category 'SibSp': \b 7.00 \n",
      "Number of levels in category 'Parch': \b 7.00 \n",
      "Number of levels in category 'Ticket': \b 681.00 \n",
      "Number of levels in category 'Fare': \b 248.00 \n",
      "Number of levels in category 'Cabin': \b 148.00 \n",
      "Number of levels in category 'Embarked': \b 4.00 \n"
     ]
    }
   ],
   "source": [
    "for cat in df_train.columns:\n",
    "    print(\"Number of levels in category '{0}': \\b {1:2.2f} \".format(cat, df_train[cat].unique().size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describing the numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for category 'Sex': \b ['male' 'female'] \n",
      "Unique values for category 'Survived': \b [0 1] \n",
      "Unique values for category 'Pclass': \b [3 1 2] \n",
      "Unique values for category 'SibSp': \b [1 0 3 4 2 5 8] \n",
      "Unique values for category 'Embarked': \b ['S' 'C' 'Q' nan] \n",
      "Unique values for category 'Cabin': \b [nan 'C85' 'C123' 'E46' 'G6' 'C103' 'D56' 'A6' 'C23 C25 C27' 'B78' 'D33'\n",
      " 'B30' 'C52' 'B28' 'C83' 'F33' 'F G73' 'E31' 'A5' 'D10 D12' 'D26' 'C110'\n",
      " 'B58 B60' 'E101' 'F E69' 'D47' 'B86' 'F2' 'C2' 'E33' 'B19' 'A7' 'C49' 'F4'\n",
      " 'A32' 'B4' 'B80' 'A31' 'D36' 'D15' 'C93' 'C78' 'D35' 'C87' 'B77' 'E67'\n",
      " 'B94' 'C125' 'C99' 'C118' 'D7' 'A19' 'B49' 'D' 'C22 C26' 'C106' 'C65'\n",
      " 'E36' 'C54' 'B57 B59 B63 B66' 'C7' 'E34' 'C32' 'B18' 'C124' 'C91' 'E40'\n",
      " 'T' 'C128' 'D37' 'B35' 'E50' 'C82' 'B96 B98' 'E10' 'E44' 'A34' 'C104'\n",
      " 'C111' 'C92' 'E38' 'D21' 'E12' 'E63' 'A14' 'B37' 'C30' 'D20' 'B79' 'E25'\n",
      " 'D46' 'B73' 'C95' 'B38' 'B39' 'B22' 'C86' 'C70' 'A16' 'C101' 'C68' 'A10'\n",
      " 'E68' 'B41' 'A20' 'D19' 'D50' 'D9' 'A23' 'B50' 'A26' 'D48' 'E58' 'C126'\n",
      " 'B71' 'B51 B53 B55' 'D49' 'B5' 'B20' 'F G63' 'C62 C64' 'E24' 'C90' 'C45'\n",
      " 'E8' 'B101' 'D45' 'C46' 'D30' 'E121' 'D11' 'E77' 'F38' 'B3' 'D6' 'B82 B84'\n",
      " 'D17' 'A36' 'B102' 'B69' 'E49' 'C47' 'D28' 'E17' 'A24' 'C50' 'B42' 'C148'] \n"
     ]
    }
   ],
   "source": [
    "# There are many values for name and ticket\n",
    "\n",
    "for cat in ['Sex', 'Survived', 'Pclass', 'SibSp', 'Embarked', 'Cabin']:\n",
    "    print(\"Unique values for category '{0}': \\b {1} \".format(cat, df_train[cat].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.Survived.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex     Survived\n",
       "female  1           233\n",
       "        0            81\n",
       "male    0           468\n",
       "        1           109\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('Sex').Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">female</th>\n",
       "      <th>count</th>\n",
       "      <td>261.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.915709</td>\n",
       "      <td>44.479818</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>431.028662</td>\n",
       "      <td>2.159236</td>\n",
       "      <td>0.694268</td>\n",
       "      <td>0.742038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.110146</td>\n",
       "      <td>57.997698</td>\n",
       "      <td>1.022846</td>\n",
       "      <td>256.846324</td>\n",
       "      <td>0.857290</td>\n",
       "      <td>1.156520</td>\n",
       "      <td>0.438211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>12.071875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>414.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>641.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>889.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">male</th>\n",
       "      <th>count</th>\n",
       "      <td>453.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.726645</td>\n",
       "      <td>25.523893</td>\n",
       "      <td>0.235702</td>\n",
       "      <td>454.147314</td>\n",
       "      <td>2.389948</td>\n",
       "      <td>0.429809</td>\n",
       "      <td>0.188908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.678201</td>\n",
       "      <td>43.138263</td>\n",
       "      <td>0.612294</td>\n",
       "      <td>257.486139</td>\n",
       "      <td>0.813580</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>0.391775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>26.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Age        Fare       Parch  PassengerId      Pclass  \\\n",
       "Sex                                                                         \n",
       "female count  261.000000  314.000000  314.000000   314.000000  314.000000   \n",
       "       mean    27.915709   44.479818    0.649682   431.028662    2.159236   \n",
       "       std     14.110146   57.997698    1.022846   256.846324    0.857290   \n",
       "       min      0.750000    6.750000    0.000000     2.000000    1.000000   \n",
       "       25%     18.000000   12.071875    0.000000   231.750000    1.000000   \n",
       "       50%     27.000000   23.000000    0.000000   414.500000    2.000000   \n",
       "       75%     37.000000   55.000000    1.000000   641.250000    3.000000   \n",
       "       max     63.000000  512.329200    6.000000   889.000000    3.000000   \n",
       "male   count  453.000000  577.000000  577.000000   577.000000  577.000000   \n",
       "       mean    30.726645   25.523893    0.235702   454.147314    2.389948   \n",
       "       std     14.678201   43.138263    0.612294   257.486139    0.813580   \n",
       "       min      0.420000    0.000000    0.000000     1.000000    1.000000   \n",
       "       25%     21.000000    7.895800    0.000000   222.000000    2.000000   \n",
       "       50%     29.000000   10.500000    0.000000   464.000000    3.000000   \n",
       "       75%     39.000000   26.550000    0.000000   680.000000    3.000000   \n",
       "       max     80.000000  512.329200    5.000000   891.000000    3.000000   \n",
       "\n",
       "                   SibSp    Survived  \n",
       "Sex                                   \n",
       "female count  314.000000  314.000000  \n",
       "       mean     0.694268    0.742038  \n",
       "       std      1.156520    0.438211  \n",
       "       min      0.000000    0.000000  \n",
       "       25%      0.000000    0.000000  \n",
       "       50%      0.000000    1.000000  \n",
       "       75%      1.000000    1.000000  \n",
       "       max      8.000000    1.000000  \n",
       "male   count  577.000000  577.000000  \n",
       "       mean     0.429809    0.188908  \n",
       "       std      1.061811    0.391775  \n",
       "       min      0.000000    0.000000  \n",
       "       25%      0.000000    0.000000  \n",
       "       50%      0.000000    0.000000  \n",
       "       75%      0.000000    0.000000  \n",
       "       max      8.000000    1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_sex = df_train.groupby('Sex')\n",
    "df_by_sex.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass  Survived\n",
       "1       1           136\n",
       "        0            80\n",
       "2       0            97\n",
       "        1            87\n",
       "3       0           372\n",
       "        1           119\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('Pclass').Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass  Survived\n",
       "1       1           136\n",
       "        0            80\n",
       "2       0            97\n",
       "        1            87\n",
       "3       0           372\n",
       "        1           119\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(['Pclass']).Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass  Sex   \n",
       "1       male      122\n",
       "        female     94\n",
       "2       male      108\n",
       "        female     76\n",
       "3       male      347\n",
       "        female    144\n",
       "Name: Sex, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(['Pclass']).Sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n",
      "1 122\n",
      "2 108\n",
      "3 347\n",
      "\n",
      "Female\n",
      "1 94\n",
      "2 76\n",
      "3 144\n"
     ]
    }
   ],
   "source": [
    "print('Male')\n",
    "for i in range(1,4):\n",
    "    print(i, len(df_train[ (df_train['Sex'] == 'male') & (df_train['Pclass'] == i) ]))\n",
    "print()\n",
    "print('Female')\n",
    "for i in range(1,4):\n",
    "    print(i, len(df_train[ (df_train['Sex'] == 'female') & (df_train['Pclass'] == i) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">1</th>\n",
       "      <th>count</th>\n",
       "      <td>186.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>216.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.233441</td>\n",
       "      <td>84.154687</td>\n",
       "      <td>0.356481</td>\n",
       "      <td>461.597222</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.802856</td>\n",
       "      <td>78.380373</td>\n",
       "      <td>0.693997</td>\n",
       "      <td>246.737616</td>\n",
       "      <td>0.611898</td>\n",
       "      <td>0.484026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.923950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>60.287500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>93.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>670.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">2</th>\n",
       "      <th>count</th>\n",
       "      <td>173.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.877630</td>\n",
       "      <td>20.662183</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>445.956522</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.472826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.001077</td>\n",
       "      <td>13.417399</td>\n",
       "      <td>0.690963</td>\n",
       "      <td>250.852161</td>\n",
       "      <td>0.601633</td>\n",
       "      <td>0.500623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>435.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>668.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">3</th>\n",
       "      <th>count</th>\n",
       "      <td>355.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>491.000000</td>\n",
       "      <td>491.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.140620</td>\n",
       "      <td>13.675550</td>\n",
       "      <td>0.393075</td>\n",
       "      <td>439.154786</td>\n",
       "      <td>0.615071</td>\n",
       "      <td>0.242363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.495398</td>\n",
       "      <td>11.778142</td>\n",
       "      <td>0.888861</td>\n",
       "      <td>264.441453</td>\n",
       "      <td>1.374883</td>\n",
       "      <td>0.428949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>666.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>69.550000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Age        Fare       Parch  PassengerId       SibSp  \\\n",
       "Pclass                                                                      \n",
       "1      count  186.000000  216.000000  216.000000   216.000000  216.000000   \n",
       "       mean    38.233441   84.154687    0.356481   461.597222    0.416667   \n",
       "       std     14.802856   78.380373    0.693997   246.737616    0.611898   \n",
       "       min      0.920000    0.000000    0.000000     2.000000    0.000000   \n",
       "       25%     27.000000   30.923950    0.000000   270.750000    0.000000   \n",
       "       50%     37.000000   60.287500    0.000000   472.000000    0.000000   \n",
       "       75%     49.000000   93.500000    0.000000   670.500000    1.000000   \n",
       "       max     80.000000  512.329200    4.000000   890.000000    3.000000   \n",
       "2      count  173.000000  184.000000  184.000000   184.000000  184.000000   \n",
       "       mean    29.877630   20.662183    0.380435   445.956522    0.402174   \n",
       "       std     14.001077   13.417399    0.690963   250.852161    0.601633   \n",
       "       min      0.670000    0.000000    0.000000    10.000000    0.000000   \n",
       "       25%     23.000000   13.000000    0.000000   234.500000    0.000000   \n",
       "       50%     29.000000   14.250000    0.000000   435.500000    0.000000   \n",
       "       75%     36.000000   26.000000    1.000000   668.000000    1.000000   \n",
       "       max     70.000000   73.500000    3.000000   887.000000    3.000000   \n",
       "3      count  355.000000  491.000000  491.000000   491.000000  491.000000   \n",
       "       mean    25.140620   13.675550    0.393075   439.154786    0.615071   \n",
       "       std     12.495398   11.778142    0.888861   264.441453    1.374883   \n",
       "       min      0.420000    0.000000    0.000000     1.000000    0.000000   \n",
       "       25%     18.000000    7.750000    0.000000   200.000000    0.000000   \n",
       "       50%     24.000000    8.050000    0.000000   432.000000    0.000000   \n",
       "       75%     32.000000   15.500000    0.000000   666.500000    1.000000   \n",
       "       max     74.000000   69.550000    6.000000   891.000000    8.000000   \n",
       "\n",
       "                Survived  \n",
       "Pclass                    \n",
       "1      count  216.000000  \n",
       "       mean     0.629630  \n",
       "       std      0.484026  \n",
       "       min      0.000000  \n",
       "       25%      0.000000  \n",
       "       50%      1.000000  \n",
       "       75%      1.000000  \n",
       "       max      1.000000  \n",
       "2      count  184.000000  \n",
       "       mean     0.472826  \n",
       "       std      0.500623  \n",
       "       min      0.000000  \n",
       "       25%      0.000000  \n",
       "       50%      0.000000  \n",
       "       75%      1.000000  \n",
       "       max      1.000000  \n",
       "3      count  491.000000  \n",
       "       mean     0.242363  \n",
       "       std      0.428949  \n",
       "       min      0.000000  \n",
       "       25%      0.000000  \n",
       "       50%      0.000000  \n",
       "       75%      0.000000  \n",
       "       max      1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_by_class = df_train.groupby('Pclass')\n",
    "df_by_class.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass  Sex     Survived\n",
       "1       female  1            91\n",
       "                0             3\n",
       "        male    0            77\n",
       "                1            45\n",
       "2       female  1            70\n",
       "                0             6\n",
       "        male    0            91\n",
       "                1            17\n",
       "3       female  0            72\n",
       "                1            72\n",
       "        male    0           300\n",
       "                1            47\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(['Pclass','Sex']).Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>female</th>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.968085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.631148</td>\n",
       "      <td>0.368852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>female</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.842593</td>\n",
       "      <td>0.157407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>female</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.864553</td>\n",
       "      <td>0.135447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Survived             No       Yes\n",
       "Pclass Sex                       \n",
       "1      female  0.031915  0.968085\n",
       "       male    0.631148  0.368852\n",
       "2      female  0.078947  0.921053\n",
       "       male    0.842593  0.157407\n",
       "3      female  0.500000  0.500000\n",
       "       male    0.864553  0.135447"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = pd.crosstab([df_train.Pclass, df_train.Sex], df_train.Survived.astype(float))\n",
    "id.columns = (['No', 'Yes'])\n",
    "id.columns.name = \"Survived\"\n",
    "id.div(id.sum(1).astype(float), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f767afc2390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAFpCAYAAACvaj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f767b000780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_train['Age'].hist()\n",
    "df_train['Age'].dropna().hist(bins=16, range=(0,80), alpha = .5, figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.42  19.    25.    31.8   41.    80.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age  Survived\n",
       "0.0  0           85\n",
       "     1           79\n",
       "1.0  0           92\n",
       "     1           45\n",
       "2.0  0           77\n",
       "     1           50\n",
       "3.0  0           81\n",
       "     1           63\n",
       "4.0  0           89\n",
       "     1           53\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser, bins = pd.qcut(df_train.Age.dropna(), 5, retbins=True, labels=False)\n",
    "print(bins)\n",
    "df_train.groupby(ser).Survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['Gender'] = df_train['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "df_test['Gender'] = df_test['Sex'].map( {'female': 0, 'male': 1} ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling the null values for Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train[df_train['Age'].isnull()]))\n",
    "print(len(df_test[df_test['Age'].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['AgeFill'] = df_train['Age']\n",
    "df_test['AgeFill'] = df_test['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeFill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender  Pclass  Age  AgeFill\n",
       "5        1       3  NaN      NaN\n",
       "17       1       2  NaN      NaN\n",
       "19       0       3  NaN      NaN\n",
       "26       1       3  NaN      NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['Age'].isnull()][['Gender','Pclass','Age','AgeFill']].head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 35. ,  28. ,  21.5],\n",
       "       [ 40. ,  30. ,  25. ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_ages = np.zeros((2,3))\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 3):\n",
    "        median_ages[i,j] = df_train[(df_train['Gender'] == i) & (df_train['Pclass'] == j+1)]['Age'].dropna().median()\n",
    "\n",
    "median_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 2):\n",
    "    for j in range(0, 3):\n",
    "        df_train.loc[(df_train.Age.isnull()) & (df_train.Gender == i) & (df_train.Pclass == j+1),'AgeFill'] = median_ages[i,j]\n",
    "        df_test.loc[(df_test.Age.isnull()) & (df_test.Gender == i) & (df_test.Pclass == j+1),'AgeFill'] = median_ages[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeFill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender  Pclass  Age  AgeFill\n",
       "5        1       3  NaN     25.0\n",
       "17       1       2  NaN     30.0\n",
       "19       0       3  NaN     21.5\n",
       "26       1       3  NaN     25.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['Age'].isnull()][['Gender','Pclass','Age','AgeFill']].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Enconding the field \"Embarked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Embarked'] = df_train['Embarked'].map({np.nan:0,'C':1, 'Q':2,'S':3} ).astype(int)\n",
    "df_test['Embarked'] = df_test['Embarked'].map({np.nan:0,'C':1, 'Q':2,'S':3} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked  Survived\n",
       "0         1             2\n",
       "1         1            93\n",
       "          0            75\n",
       "2         0            47\n",
       "          1            30\n",
       "3         0           427\n",
       "          1           217\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(['Embarked']).Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked = pd.DataFrame(lb.fit_transform(df_train['Embarked'].values), columns=['Emb1','Emb2','Emb3','Emb4'])\n",
    "df_train = pd.concat([df_train, embarked], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (3, 418), indices imply (4, 418)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4246\u001b[0m                 blocks = [make_block(values=blocks[0],\n\u001b[0;32m-> 4247\u001b[0;31m                                      placement=slice(0, len(axes[0])))]\n\u001b[0m\u001b[1;32m   4248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   2684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[1;32m    108\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[0;32m--> 109\u001b[0;31m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 3, placement implies 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4b524db169f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membarked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embarked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Emb1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Emb2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Emb3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Emb4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membarked\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 297\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_possibly_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4254\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4255\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4256\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4231\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4232\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4233\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (3, 418), indices imply (4, 418)"
     ]
    }
   ],
   "source": [
    "embarked = pd.DataFrame(lb.fit_transform(df_test['Embarked'].values), columns=['Emb1','Emb2','Emb3','Emb4'])\n",
    "df_test = pd.concat([df_test, embarked], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emb1</th>\n",
       "      <th>Emb2</th>\n",
       "      <th>Emb3</th>\n",
       "      <th>Emb4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emb1  Emb2  Emb3  Emb4\n",
       "0       0     0     0     1\n",
       "1       0     1     0     0\n",
       "2       0     0     0     1\n",
       "3       0     0     0     1\n",
       "4       0     0     0     1\n",
       "5       0     0     1     0\n",
       "6       0     0     0     1\n",
       "7       0     0     0     1\n",
       "8       0     0     0     1\n",
       "9       0     1     0     0\n",
       "10      0     0     0     1\n",
       "11      0     0     0     1\n",
       "12      0     0     0     1\n",
       "13      0     0     0     1\n",
       "14      0     0     0     1\n",
       "15      0     0     0     1\n",
       "16      0     0     1     0\n",
       "17      0     0     0     1\n",
       "18      0     0     0     1\n",
       "19      0     1     0     0\n",
       "20      0     0     0     1\n",
       "21      0     0     0     1\n",
       "22      0     0     1     0\n",
       "23      0     0     0     1\n",
       "24      0     0     0     1\n",
       "25      0     0     0     1\n",
       "26      0     1     0     0\n",
       "27      0     0     0     1\n",
       "28      0     0     1     0\n",
       "29      0     0     0     1\n",
       "..    ...   ...   ...   ...\n",
       "861     0     0     0     1\n",
       "862     0     0     0     1\n",
       "863     0     0     0     1\n",
       "864     0     0     0     1\n",
       "865     0     0     0     1\n",
       "866     0     1     0     0\n",
       "867     0     0     0     1\n",
       "868     0     0     0     1\n",
       "869     0     0     0     1\n",
       "870     0     0     0     1\n",
       "871     0     0     0     1\n",
       "872     0     0     0     1\n",
       "873     0     0     0     1\n",
       "874     0     1     0     0\n",
       "875     0     1     0     0\n",
       "876     0     0     0     1\n",
       "877     0     0     0     1\n",
       "878     0     0     0     1\n",
       "879     0     1     0     0\n",
       "880     0     0     0     1\n",
       "881     0     0     0     1\n",
       "882     0     0     0     1\n",
       "883     0     0     0     1\n",
       "884     0     0     0     1\n",
       "885     0     0     1     0\n",
       "886     0     0     0     1\n",
       "887     0     0     0     1\n",
       "888     0     0     0     1\n",
       "889     0     1     0     0\n",
       "890     0     0     1     0\n",
       "\n",
       "[891 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Enconding the field \"Cabin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Cabin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CabinTrans = pd.DataFrame(mlb.fit_transform([{str(val)} for val in df_train['Cabin'].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CabinTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Give up using the feature\n",
    "#df_train = pd.concat([df_train, CabinTrans], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a feature for number of relatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch']\n",
    "df_train['Age*Class'] = df_train.AgeFill * df_train.Pclass\n",
    "\n",
    "df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch']\n",
    "df_test['Age*Class'] = df_test.AgeFill * df_test.Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discarding unused columns for predictive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train2 = df_train.drop(['Age','Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "df_train2 = df_train2.dropna()\n",
    "\n",
    "df_test2 = df_test.drop(['Age','Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "df_test2 = df_test2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.rename(columns={'Survived': 'class'}, inplace=True)  #For the TPOT genetic algorithm we must rename our target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnng correlation hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train2.Survived\n",
    "X = df_train2.Gender\n",
    "model = sm.Logit(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train2.Survived\n",
    "X = df_train2[['Gender','Pclass','AgeFill']]\n",
    "model = sm.Logit(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Data for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train2.values\n",
    "print(train_data.shape)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_test2.values\n",
    "print(test_data.shape)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting features scales:   \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[0::,2::])\n",
    "X_test = scaler.fit_transform(test_data[0::,1::])\n",
    "\n",
    "y_train = train_data[0::,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stats.stackexchange.com/questions/95797/how-to-split-the-dataset-for-cross-validation-learning-curve-and-final-evaluat  \n",
    "http://scikit-learn.org/stable/modules/cross_validation.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing modules for cross validation and evaluation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating functions to help evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def clf_eval(clf, X, y_true, classes=['No Sobreviveu', 'Sobreviveu']):\n",
    "    y_pred = clf.predict(X)\n",
    "    clf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "    print('ROC Score: {}'.format(roc_auc_score(y_true, y_pred)))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(y_true, y_pred)))\n",
    "    print('Average Precision Score: {}'.format(average_precision_score(y_true, y_pred)))\n",
    "    print('f1 Score: {}'.format(f1_score(y_true, y_pred)))\n",
    "    plot_confusion_matrix(clf_matrix, classes=classes)\n",
    "    return roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating cross-validation with subsets (60% train / 40% test):  \n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html  \n",
    "http://www.analyticsvidhya.com/blog/2015/05/k-fold-cross-validation-simple/  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html  \n",
    "http://stackoverflow.com/questions/25375203/identical-learning-curves-on-subsequent-runs-using-shufflesplit  \n",
    "http://stackoverflow.com/questions/28064634/random-state-pseudo-random-numberin-scikit-learn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincv, X_testcv, y_traincv, y_testcv = model_selection.train_test_split(X_train,\n",
    "                                                                            y_train,\n",
    "                                                                            test_size=0.4,\n",
    "                                                                            random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_traincv[y_traincv == 0]))\n",
    "print(len(y_traincv[y_traincv == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling the minority class  \n",
    "http://contrib.scikit-learn.org/imbalanced-learn/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(kind='regular')\n",
    "X_traincv_res, y_traincv_res = smote.fit_sample(X_traincv, y_traincv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_traincv_res[y_traincv_res == 0]))\n",
    "print(len(y_traincv_res[y_traincv_res == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating another train/test set using k-fold or other cross-validation method:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = model_selection.StratifiedKFold(n_splits=10)\n",
    "\n",
    "def mean_scores_cv(clf, cv, X, y):\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              scoring=None, \n",
    "                                              cv=cv, \n",
    "                                              n_jobs=1,\n",
    "                                              verbose=0,\n",
    "                                              fit_params=None,\n",
    "                                              pre_dispatch='2*n_jobs')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the best parameters using GridSearchCV and RandomizedSearchCV  \n",
    "http://scikit-learn.org/stable/modules/grid_search.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "estimator = svm.SVC()\n",
    "cv = model_selection.StratifiedKFold(n_splits=10)\n",
    "\n",
    "kernels = ['linear', 'poly']\n",
    "Cs = np.linspace(0.1,2,10)\n",
    "degrees = [2,3,4]\n",
    "gammas = np.logspace(-5, 0, 10)\n",
    "\n",
    "param_grid=dict(kernel=kernels, C=Cs, gamma=gammas, degree=degrees)\n",
    "\n",
    "clf_svc = model_selection.RandomizedSearchCV(estimator=estimator,\n",
    "                                             cv=cv,\n",
    "                                             param_distributions=param_grid, \n",
    "                                             n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "with open(os.path.join(outputs,'best_parameters_svm.pickle'), 'wb') as f:\n",
    "    pickle.dump(clf_svc,f)\n",
    "\n",
    "with open(os.path.join(outputs,'best_parameters_svm.pickle'), 'rb') as f:\n",
    "    clf_svc = pickle.load(f)\n",
    "\n",
    "print(clf_svc.best_score_)\n",
    "print(clf_svc.best_estimator_.kernel)\n",
    "print(clf_svc.best_estimator_.C)\n",
    "print(clf_svc.best_estimator_.degree)\n",
    "print(clf_svc.best_estimator_.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing some classifiers:  \n",
    "Note that only the first classifier (svm) had its parameters optimized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "clf_svc2 = svm.SVC(kernel=clf_svc.best_estimator_.kernel,\n",
    "                   C=clf_svc.best_estimator_.C,\n",
    "                   degree=clf_svc.best_estimator_.degree, \n",
    "                   gamma=clf_svc.best_estimator_.gamma, \n",
    "                   coef0=0.0, \n",
    "                   shrinking=True, \n",
    "                   probability=False, \n",
    "                   tol=0.001, \n",
    "                   cache_size=200, \n",
    "                   class_weight=None, \n",
    "                   verbose=False, \n",
    "                   max_iter=-1, \n",
    "                   random_state=0).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_svc2 = clf_eval(clf_svc2, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first classifier, we'll also display the learning curve  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.learning_curve.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.learning_curve.html\n",
    "\n",
    "def plot_learning_curve(estimator, \n",
    "                        title, \n",
    "                        X, \n",
    "                        y, \n",
    "                        ylim=None, \n",
    "                        cv=None,\n",
    "                        n_jobs=-1, \n",
    "                        train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = model_selection.learning_curve(estimator, \n",
    "                                                            X, y, cv=cv, \n",
    "                                                            n_jobs=n_jobs, \n",
    "                                                            train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Learning Curves (SVM, kernel:{1}{0} , $\\gamma={2:.6f}$)'.format(clf_svc.best_estimator_.degree,\n",
    "                                                                         clf_svc.best_estimator_.kernel,\n",
    "                                                                         clf_svc.best_estimator_.gamma)\n",
    "graph = plot_learning_curve(clf_svc2, title, X_train, y_train, cv=cv)\n",
    "#matplotlib.rcdefaults()\n",
    "#matplotlib.verbose.set_level('silent')\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar outros estimadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/ensemble.html\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#http://blog.yhathq.com/posts/random-forests-in-python.html\n",
    "#http://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=300, \n",
    "                                criterion='gini', \n",
    "                                max_depth=None, \n",
    "                                min_samples_split=2, \n",
    "                                min_samples_leaf=1, \n",
    "                                min_weight_fraction_leaf=0.0, \n",
    "                                max_features='auto', \n",
    "                                max_leaf_nodes=None, \n",
    "                                bootstrap=True, \n",
    "                                oob_score=False, \n",
    "                                n_jobs=-1, \n",
    "                                random_state=0, \n",
    "                                verbose=0, \n",
    "                                warm_start=False, \n",
    "                                class_weight=None).fit(X_traincv, y_traincv)\n",
    "\n",
    "\n",
    "\n",
    "roc_rf = clf_eval(clf_rf, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the confidence intervals  \n",
    "https://github.com/scikit-learn-contrib/forest-confidence-interval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestci as fci\n",
    "\n",
    "y_predicted = clf_rf.predict_proba(X_testcv)\n",
    "\n",
    "# calculate inbag and unbiased variance\n",
    "inbag = fci.calc_inbag(X_traincv.shape[0], clf_rf)\n",
    "unbiased = fci.random_forest_error(clf_rf, inbag,X_traincv, X_testcv)\n",
    "\n",
    "# Plot forest prediction for survivors and standard deviation for estimates\n",
    "# Blue points are survivors; Green points are non-survivors\n",
    "idx = np.where(y_testcv == 1)[0]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.errorbar(y_predicted[idx, 1], \n",
    "             np.sqrt(unbiased[idx]),\n",
    "             fmt='.', \n",
    "             alpha=0.75, \n",
    "             label='Survived')\n",
    "\n",
    "idx = np.where(y_testcv == 0)[0]\n",
    "plt.errorbar(y_predicted[idx, 1],\n",
    "             np.sqrt(unbiased[idx]),\n",
    "             fmt='.', \n",
    "             alpha=0.75, \n",
    "             label='Not Survived')\n",
    "\n",
    "plt.xlabel('Prediction (Surviving probability)')\n",
    "plt.ylabel('Standard deviation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dtc = DecisionTreeClassifier(criterion='gini', \n",
    "                                 splitter='best', \n",
    "                                 max_depth=None, \n",
    "                                 min_samples_split=2, \n",
    "                                 min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, \n",
    "                                 max_features=None, \n",
    "                                 random_state=0, \n",
    "                                 max_leaf_nodes=None, \n",
    "                                 class_weight=None, \n",
    "                                 presort=False).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_dtc = clf_eval(clf_dtc, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf_etc = ExtraTreesClassifier(n_estimators=300,\n",
    "                               max_depth=None,\n",
    "                               min_samples_split=2,\n",
    "                               random_state=0).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_etc = clf_eval(clf_etc, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf_abc = AdaBoostClassifier(base_estimator=None,\n",
    "                             n_estimators=200,\n",
    "                             learning_rate=0.1,\n",
    "                             algorithm='SAMME.R',\n",
    "                             random_state=0).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_abc = clf_eval(clf_abc, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_gbc = GradientBoostingClassifier(loss='deviance',\n",
    "                                     learning_rate=0.1,\n",
    "                                     n_estimators=200,\n",
    "                                     subsample=1.0, \n",
    "                                     min_samples_split=2, \n",
    "                                     min_samples_leaf=1, \n",
    "                                     min_weight_fraction_leaf=0.0, \n",
    "                                     max_depth=3, \n",
    "                                     init=None, \n",
    "                                     random_state=0, \n",
    "                                     max_features=None, \n",
    "                                     verbose=0, \n",
    "                                     max_leaf_nodes=None, \n",
    "                                     warm_start=False, \n",
    "                                     presort='auto').fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_gbc = clf_eval(clf_gbc, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/dmlc/xgboost/tree/master/python-package  \n",
    "#https://xgboost.readthedocs.io/en/latest/build.html#building-on-ubuntu-debian\n",
    "#http://xgboost.readthedocs.io/en/latest/build.html#python-package-installation  \n",
    "#http://xgboost.readthedocs.io/en/latest/parameter.html  \n",
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/  \n",
    "#https://www.kaggle.com/cbrogan/titanic/xgboost-example-python/run/1620  \n",
    "#http://xgboost.readthedocs.io/en/latest//python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "import xgboost\n",
    "\n",
    "clf_xgb = xgboost.sklearn.XGBClassifier(base_score=0.5,\n",
    "                                        learning_rate=0.1,\n",
    "                                        n_estimators=250,\n",
    "                                        max_delta_step=0,\n",
    "                                        max_depth=3,\n",
    "                                        min_child_weight=1,\n",
    "                                        missing=None,\n",
    "                                        gamma=0,\n",
    "                                        subsample=1,\n",
    "                                        colsample_bylevel=1,\n",
    "                                        colsample_bytree=1,\n",
    "                                        objective= 'binary:logitraw',\n",
    "                                        #objective='multi:softprob',\n",
    "                                        reg_alpha=0, \n",
    "                                        reg_lambda=1,\n",
    "                                        nthread=-1,\n",
    "                                        scale_pos_weight=1,\n",
    "                                        seed=0,\n",
    "                                        silent=False,).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_xgb = clf_eval(clf_xgb, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf_sgdc = SGDClassifier(loss='hinge',\n",
    "                         penalty='l2', \n",
    "                         alpha=0.0001,\n",
    "                         l1_ratio=0.15, \n",
    "                         fit_intercept=True,\n",
    "                         n_iter=200, \n",
    "                         shuffle=True,\n",
    "                         verbose=0,\n",
    "                         epsilon=0.1,\n",
    "                         n_jobs=-1,\n",
    "                         random_state=0,\n",
    "                         learning_rate='optimal',\n",
    "                         eta0=0.0, \n",
    "                         power_t=0.5,\n",
    "                         class_weight=None,\n",
    "                         warm_start=False, \n",
    "                         average=False).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_sgdc = clf_eval(clf_sgdc, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "clf_rdg = RidgeClassifier(alpha=1.0, \n",
    "                          fit_intercept=True, \n",
    "                          normalize=False, \n",
    "                          copy_X=True, \n",
    "                          max_iter=None, \n",
    "                          tol=0.001, \n",
    "                          class_weight=None, \n",
    "                          solver='auto', \n",
    "                          random_state=0).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_rdg = clf_eval(clf_rdg, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_pcp = Perceptron(penalty=None,\n",
    "                     alpha=0.0001,\n",
    "                     fit_intercept=True,\n",
    "                     n_iter=200,\n",
    "                     shuffle=True,\n",
    "                     verbose=0,\n",
    "                     eta0=1.0,\n",
    "                     n_jobs=-1, \n",
    "                     random_state=0, \n",
    "                     class_weight=None, \n",
    "                     warm_start=False).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_pcp = clf_eval(clf_pcp, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "clf_pac = PassiveAggressiveClassifier(C=1.0, \n",
    "                                      fit_intercept=True, \n",
    "                                      n_iter=200, \n",
    "                                      shuffle=True, \n",
    "                                      verbose=0, \n",
    "                                      loss='hinge', \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=0, \n",
    "                                      warm_start=False, \n",
    "                                      class_weight=None).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_pac = clf_eval(clf_pac, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2',\n",
    "                            dual=False, \n",
    "                            tol=0.0001, \n",
    "                            C=1.0, \n",
    "                            fit_intercept=True, \n",
    "                            intercept_scaling=1, \n",
    "                            class_weight=None, \n",
    "                            random_state=0, \n",
    "                            solver='liblinear', \n",
    "                            max_iter=100, \n",
    "                            multi_class='ovr', \n",
    "                            verbose=0, \n",
    "                            warm_start=False, \n",
    "                            n_jobs=-1).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_lr = clf_eval(clf_lr, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB().fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_gnb = clf_eval(clf_gnb, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf_bnb = BernoulliNB(alpha=1.0, \n",
    "                     binarize=0.0, \n",
    "                     fit_prior=True, \n",
    "                     class_prior=None).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_bnb = clf_eval(clf_bnb, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5,\n",
    "                               weights='uniform', \n",
    "                               algorithm='auto', \n",
    "                               leaf_size=30, \n",
    "                               p=2, \n",
    "                               metric='minkowski', \n",
    "                               metric_params=None, \n",
    "                               n_jobs=1).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_knn = clf_eval(clf_knn, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clf_bgc = BaggingClassifier().fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_bgc = clf_eval(clf_bgc, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras (with TensorFlow / Theano backends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "# http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "# https://www.kaggle.com/cstahl12/titanic/titanic-with-keras\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "print('Keras using {} backend'.format(keras.backend.backend()))  #https://keras.io/backend/\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 20\n",
    "\n",
    "x_train_kr = X_traincv.astype('float32') \n",
    "x_test_kr = X_testcv.astype('float32')\n",
    "\n",
    "print(x_train_kr.shape[0], 'train samples')\n",
    "print(x_test_kr.shape[0], 'test samples')\n",
    "\n",
    "y_train_kr = y_traincv\n",
    "y_test_kr = y_testcv\n",
    "\n",
    "# convert class vectors to binary class matrices for categorical cross_entropy\n",
    "#y_train_kr = to_categorical(y_traincv)\n",
    "#y_test_kr = to_categorical(y_testcv)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=x_train_kr.shape[1], activation='relu', output_dim=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(input_dim=200, activation='relu', output_dim=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(input_dim=200, activation='relu', output_dim=24)) #activation='softmax'\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(input_dim=24,  activation='sigmoid', output_dim=1)) #kernel_initializer='uniform',\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #loss='categorical_crossentropy',\n",
    "              #loss='mean_squared_error',\n",
    "              optimizer=RMSprop(), \n",
    "              #optimizer=SGD(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_kr, y_train_kr,\n",
    "                    batch_size=batch_size,\n",
    "                    #epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_kr, y_test_kr))\n",
    "\n",
    "score = model.evaluate(x_test_kr, y_test_kr, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "y_pred = model.predict_classes(x_test_kr) #y_pred = np.around(model.predict(x_test_kr)[:,1])\n",
    "#y_test_kr = y_test_kr[:,1] #for categorical cross_entropy\n",
    "clf_matrix = confusion_matrix(y_test_kr, y_pred)\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test_kr, y_pred, target_names=['No Sobreviveu', 'Sobreviveu']))\n",
    "print('ROC Score: {}'.format(roc_auc_score(y_test_kr, y_pred)))\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test_kr, y_pred)))\n",
    "print('Average Precision Score: {}'.format(average_precision_score(y_test_kr, y_pred)))\n",
    "print('f1 Score: {}'.format(f1_score(y_test_kr, y_pred)))\n",
    "plot_confusion_matrix(clf_matrix, classes=['No Sobreviveu', 'Sobreviveu'])\n",
    "roc_keras = roc_auc_score(y_test_kr, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theano Neural Networks (with Lasagne and Nolearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sudo -H pip3 install --upgrade theano nolearn\n",
    "#sudo -H pip3 install git+https://github.com/Lasagne/Lasagne.git\n",
    "\n",
    "#http://stackoverflow.com/questions/30034492/does-nolearn-lasagne-support-python-3\n",
    "#https://gist.github.com/dnouri/fe855653e9757e1ce8c4\n",
    "#https://github.com/dnouri/nolearn/issues/62\n",
    "\n",
    "#https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/base.py\n",
    "#https://github.com/Lasagne/Lasagne/blob/master/lasagne/objectives.py\n",
    "\n",
    "#http://nbviewer.ipython.org/github/ottogroup/kaggle/blob/master/Otto_Group_Competition.ipynb\n",
    "#http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/\n",
    "\n",
    "#https://github.com/IssamLaradji/NeuralNetworks/tree/master/multilayer_perceptron\n",
    "#https://github.com/fchollet/keras\n",
    "\n",
    "import lasagne\n",
    "import nolearn\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator\n",
    "\n",
    "clf_lsgn_layers = [('input', lasagne.layers.InputLayer),\n",
    "                  #('input', lasagne.layers.InputLayer(shape=(None, X.shape[1])))\n",
    "                  #('dropout0', lasagne.layers.DropoutLayer),\n",
    "                  ('dense0', lasagne.layers.DenseLayer),\n",
    "                  ('dropout1', lasagne.layers.DropoutLayer),#\n",
    "                  ('dense1', lasagne.layers.DenseLayer),\n",
    "                  #('dropout2', lasagne.layers.DropoutLayer),#\n",
    "                  ('output', lasagne.layers.DenseLayer)]\n",
    "\n",
    "#l = InputLayer(shape=(None, X.shape[1]))\n",
    "#l = DenseLayer(l, num_units=len(np.unique(y)), nonlinearity=softmax)\n",
    "#net = NeuralNet(l, update_learning_rate=0.01)\n",
    "\n",
    "clf_lsgn = nolearn.lasagne.NeuralNet(layers=clf_lsgn_layers,\n",
    "                                     input_shape=(None, X_traincv.shape[1]), #num_features\n",
    "                                     #dropout0_p=0.5, #0.15                 \n",
    "                                     dense0_num_units=200,\n",
    "                                     dropout1_p=0.25,\n",
    "                                     dense1_num_units=200,\n",
    "                                     #dropout2_p=0.25,\n",
    "                                     output_num_units=len(np.unique(y_traincv)), #num_classes\n",
    "                                     output_nonlinearity=lasagne.nonlinearities.softmax,\n",
    "                                     #update=lasagne.updates.adagrad,\n",
    "                                     update=lasagne.updates.nesterov_momentum,\n",
    "                                     update_momentum=0.5, #0.9 #only used with nesterov_momentum\n",
    "                                     update_learning_rate=0.01,\n",
    "                                     train_split=nolearn.lasagne.TrainSplit(eval_size=0.2), #split\n",
    "                                     verbose=1,\n",
    "                                     max_epochs=200, #To tune,verify the behaviour of outputs\n",
    "                                     )\n",
    "\n",
    "clf_lsgn.fit(X_traincv.astype(np.float32), y_traincv.astype(np.int32))\n",
    "\n",
    "roc_lsgn = clf_eval(clf_lsgn, X_testcv.astype(np.float32), y_testcv.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in clf_lsgn.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in clf_lsgn.train_history_])\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(valid_loss, linewidth=3, label=\"test\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "#plt.ylim(1e-1, 1e1)\n",
    "plt.ylim(0.3, 0.8)\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theano Neural Networks (with  Lasagne layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://lasagne.readthedocs.org/en/latest/\n",
    "#http://lasagne.readthedocs.org/en/latest/user/tutorial.html\n",
    "#https://github.com/Lasagne\n",
    "#https://martin-thoma.com/lasagne-for-python-newbies/\n",
    "#http://deeplearning.net/tutorial/\n",
    "#http://deeplearning.net/software/theano/tutorial/\n",
    "#http://cs231n.github.io/\n",
    "#http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "#import lasagne\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(534,8), input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(l_in_drop, \n",
    "                                       num_units=800,\n",
    "                                       nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                       W=lasagne.init.GlorotUniform())\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.6)\n",
    "    l_hid2 = lasagne.layers.DenseLayer(l_hid1_drop,\n",
    "                                       num_units=800,\n",
    "                                       nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.6)\n",
    "    l_out = lasagne.layers.DenseLayer(l_hid2_drop,\n",
    "                                      num_units=10,\n",
    "                                      nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out\n",
    "\n",
    "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2, drop_hidden=.5):\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(534,8), input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    \n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.rectify\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(network, \n",
    "                                            width, \n",
    "                                            nonlinearity=nonlin)\n",
    "    if drop_hidden:\n",
    "        network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    \n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(534,8),\n",
    "                                        input_var=input_var)\n",
    "    network = lasagne.layers.Conv2DLayer(network,\n",
    "                                         num_filters=32,\n",
    "                                         filter_size=(5, 5),\n",
    "                                         nonlinearity=lasagne.nonlinearities.rectify,\n",
    "                                         W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.Conv2DLayer(network, \n",
    "                                         num_filters=32,\n",
    "                                         filter_size=(5, 5),\n",
    "                                         nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, p=.5),\n",
    "                                        num_units=256,\n",
    "                                        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, p=.5),\n",
    "                                        num_units=10,\n",
    "                                        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model\n",
    "network = build_mlp(input_var)\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss,\n",
    "                                            params,\n",
    "                                            learning_rate=0.01, \n",
    "                                            momentum=0.9)\n",
    "# Monitoring progress during training\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compilation\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    start_time = time.time()\n",
    "    inputs, targets = X_traincv, y_traincv\n",
    "    train_err += train_fn(inputs, targets)\n",
    "    \n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    inputs, targets = X_testcv, y_testcv\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    val_err += err\n",
    "    val_acc += acc\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err = 0\n",
    "test_acc = 0\n",
    "inputs, targets = X_testcv, y_testcv\n",
    "err, acc = val_fn(inputs, targets)\n",
    "test_err += err\n",
    "test_acc += acc\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier \n",
    "#http://scikit-learn.org/stable/modules/ensemble.html#votingclassifier\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[#('clf_sgdc', clf_sgdc),\n",
    "                                        ('clf_lgr', clf_lr),\n",
    "                                        #('clf_rdg', clf_rdg),\n",
    "                                        ('clf_bgc', clf_bgc),\n",
    "                                        ('clf_etc', clf_etc),\n",
    "                                        ('clf_abc', clf_abc),\n",
    "                                        #('clf_pct', clf_pcp),\n",
    "                                        ('clf_xgb', clf_xgb),\n",
    "                                        ('clf_rf', clf_rf),\n",
    "                                        ('clf_etc', clf_etc),\n",
    "                                        ('clf_knn', clf_knn),\n",
    "                                        ],\n",
    "                            voting='soft',\n",
    "                            weights=[1,2,1,6,1,3,1,1]).fit(X_traincv, y_traincv)\n",
    "\n",
    "roc_ens = clf_eval(ensemble, X_testcv, y_testcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_results = {'SVM': roc_svc2,\n",
    "               'RandomForest': roc_rf,\n",
    "               'DecisionTree': roc_dtc,\n",
    "               'ExtraTree': roc_etc,\n",
    "               'AdaBoost': roc_abc,\n",
    "               'GradBoost': roc_gbc,\n",
    "               'SGDC': roc_sgdc,\n",
    "               'Ridge': roc_rdg,\n",
    "               'Perceptron': roc_pcp,\n",
    "               'PassAgre': roc_pac,\n",
    "               'LogiReg': roc_lr,\n",
    "               'GaussianNB': roc_gnb,\n",
    "               'BernouNB': roc_bnb,\n",
    "               'NoLearn_Lasagne': roc_lsgn,\n",
    "               #'Theano': roc_thn,\n",
    "               'XGBoost':roc_xgb,\n",
    "               'Knn':roc_knn,\n",
    "               'Keras+Tflow': roc_keras,\n",
    "               'Bagging': roc_bgc,\n",
    "               'Voting': roc_ens, \n",
    "              }\n",
    "\n",
    "import operator\n",
    "tup_results = sorted(dic_results.items(), key=operator.itemgetter(1))\n",
    "\n",
    "N = len(dic_results)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.40       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects = ax.bar(ind+0.7, list(zip(*tup_results))[1], width,)\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., \n",
    "            1.005*height, \n",
    "            '{0:.4f}'.format(height), \n",
    "            ha='center', \n",
    "            va='bottom',)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(ymin=0.65,ymax = 0.85)\n",
    "ax.set_title(\"Classificators' performance\")\n",
    "ax.set_xticks(ind + width/2.)\n",
    "ax.set_xticklabels(list(zip(*tup_results))[0], rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best classifier and training with all training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = clf_abc.predict(X_test)\n",
    "print(output[10:20])\n",
    "print()\n",
    "output_prob = clf_abc.predict_proba(X_test)\n",
    "print(output_prob[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/rhiever/tpot\n",
    "# https://github.com/rhiever/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb\n",
    "\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(verbosity=2, max_time_mins=10, max_eval_time_mins=10, population_size=40)\n",
    "tpot.fit(X_traincv, y_traincv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/automl/auto-sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
